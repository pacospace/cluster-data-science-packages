{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+12\"><center>\n",
    "    Data Science Packages Latent Dirichlet Allocation (LDA)\n",
    "</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from gensim import corpora, models\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_PERCENTAGE_TEST_DATASET = 0.1 or os.getenv(\"LDA_PERCENTAGE_TEST_DATASET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 2949\n",
      "Training Dataset percentage is: 90\n",
      "Test Dataset percentage is: 10\n"
     ]
    }
   ],
   "source": [
    "# Retrieve clean dataset\n",
    "current_path = Path.cwd().parents[0]\n",
    "data_path = current_path.joinpath(\"data/processed\")\n",
    "\n",
    "with open(f\"{data_path}/clean_dataset.json\") as json_file:\n",
    "    clean_dataset = json.load(json_file)\n",
    "\n",
    "texts_names = []\n",
    "texts = []\n",
    "for file_name, file_vocabulary in clean_dataset.items():\n",
    "    texts.append(file_vocabulary)\n",
    "    texts_names.append(file_name)\n",
    "\n",
    "# Process data for LDA\n",
    "    \n",
    "# Assign a unique integer id to all words appearing in the corpus, creating a vocabulary corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(\"Number of unique tokens: %d\" % len(dictionary))\n",
    "# print(f\"Token ID map:\\n {dictionary.token2id}\")\n",
    "\n",
    "# Bag of Words (BoW) Representation\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in texts]\n",
    "\n",
    "lda_percentage_training_dataset = (1 - float(LDA_PERCENTAGE_TEST_DATASET)) * 100\n",
    "print(\"Training Dataset percentage is: %d\" % lda_percentage_training_dataset)\n",
    "\n",
    "lda_percentage_test_dataset = float(LDA_PERCENTAGE_TEST_DATASET) * 100\n",
    "print(\"Test Dataset percentage is: %d\" % lda_percentage_test_dataset)\n",
    "\n",
    "corpus_train, corpus_test = train_test_split(corpus, test_size=LDA_PERCENTAGE_TEST_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set inputs for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA hyperparameter Number of topics selected is:10\n",
      "LDA hyperparameter Alpha selected is: 0.5\n",
      "LDA hyperparameter Eta selected is: 0.001\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETERS\n",
    "\n",
    "# Number of topics\n",
    "NUMBER_TOPICS = int(os.getenv(\"NUMBER_TOPICS\", 10))\n",
    "\n",
    "# ALPHA: Dirichlet hyperparameter alpha, Document-Topic Density.\n",
    "\n",
    "# alpha controls the mixture of topics for any given document. \n",
    "# Turn it down, and the documents will likely have less of a mixture of topics.\n",
    "# Turn it up, and the documents will likely have more of a mixture of topics.\n",
    "LDA_ALPHA = os.getenv(\"LDA_ALPHA\", 0.5)\n",
    "\n",
    "# ETA: Dirichlet hyperparameter alpha, Topic-Word Density.\n",
    "\n",
    "# The beta/eta hyperparameter controls the distribution of words per topic.\n",
    "# Turn it down, and the topics will likely have less words.\n",
    "# Turn it up, and the topics will likely have more words.\n",
    "LDA_ETA = os.getenv(\"LDA_ETA\", 0.001)\n",
    "\n",
    "# Ideally, we want our composites to be made up of only a few topics\n",
    "# and our parts to belong to only some of the topics. With this in mind,\n",
    "# alpha and beta are typically set below one.\n",
    "\n",
    "print(f\"LDA hyperparameter Number of topics selected is:{NUMBER_TOPICS}\")\n",
    "print(f\"LDA hyperparameter Alpha selected is: {LDA_ALPHA}\")\n",
    "print(f\"LDA hyperparameter Eta selected is: {LDA_ETA}\")\n",
    "\n",
    "# Number of passes through the corpus during training.\n",
    "passes = 100\n",
    "\n",
    "# Maximum number of iterations through the corpus\n",
    "# when inferring the topic distribution of a corpus.\n",
    "iterations = 200\n",
    "\n",
    "# Number of documents to be used in each training chunk.\n",
    "chunksize = 100\n",
    "\n",
    "inputs = {\n",
    "    \"corpus\": corpus,\n",
    "    \"num_topics\": NUMBER_TOPICS,\n",
    "    \"id2word\": dictionary,\n",
    "    \"passes\": passes,\n",
    "    \"chunksize\": chunksize,\n",
    "    \"iterations\": iterations,\n",
    "    \"alpha\": LDA_ALPHA,\n",
    "    \"eta\": LDA_ETA\n",
    "}\n",
    "\n",
    "MODEL_NAME = \"model\" or os.getenv(\"MODEL_NAME\")\n",
    "model_name = MODEL_NAME + \"_\" + f\"t{NUMBER_TOPICS}\" \"_\" + datetime.utcnow().strftime(\n",
    "    \"%Y-%m-%d_%H:%M:%S\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store LDA model\n",
    "current_path = Path.cwd().parents[0]\n",
    "models_path = current_path.joinpath(\"models\")\n",
    "\n",
    "model_repo_path = models_path.joinpath(model_name)\n",
    "if not model_repo_path.exists():\n",
    "    os.makedirs(model_repo_path)\n",
    "\n",
    "complete_file_path = model_repo_path.joinpath(f\"{model_name}_lda_model\")\n",
    "\n",
    "ldamodel.save(str(complete_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: (0, '0.030*\"visualization\" + 0.027*\"badge\" + 0.026*\"network\" + 0.022*\"current\" + 0.020*\"optional\" + 0.019*\"discussion\" + 0.018*\"speed\" + 0.018*\"continuous\" + 0.018*\"extension\" + 0.017*\"function\"')\n",
      "\n",
      "Topic: (1, '0.027*\"bash\" + 0.022*\"script\" + 0.016*\"information\" + 0.015*\"instruction\" + 0.014*\"feature\" + 0.013*\"torch\" + 0.013*\"azure\" + 0.013*\"following\" + 0.013*\"content\" + 0.012*\"management\"')\n",
      "\n",
      "Topic: (2, '0.016*\"classification\" + 0.015*\"order\" + 0.014*\"able\" + 0.014*\"method\" + 0.014*\"prediction\" + 0.013*\"regression\" + 0.013*\"linear\" + 0.013*\"learning\" + 0.012*\"models\" + 0.011*\"link\"')\n",
      "\n",
      "Topic: (3, '0.013*\"quick\" + 0.013*\"training\" + 0.010*\"true\" + 0.009*\"compute\" + 0.008*\"feature\" + 0.008*\"master\" + 0.007*\"large\" + 0.007*\"layer\" + 0.007*\"fast\" + 0.007*\"start\"')\n",
      "\n",
      "Topic: (4, '0.310*\"image\" + 0.203*\"official\" + 0.099*\"wiki\" + 0.060*\"workshop\" + 0.049*\"nips\" + 0.047*\"asset\" + 0.045*\"array\" + 0.030*\"interpreter\" + 0.030*\"systems\" + 0.020*\"dimensional\"')\n",
      "\n",
      "Topic: (5, '0.044*\"great\" + 0.033*\"control\" + 0.029*\"language\" + 0.028*\"final\" + 0.028*\"area\" + 0.027*\"attribute\" + 0.026*\"copyright\" + 0.026*\"inc\" + 0.022*\"physical\" + 0.022*\"stuff\"')\n",
      "\n",
      "Topic: (6, '0.016*\"build\" + 0.012*\"install\" + 0.012*\"blob\" + 0.011*\"machine\" + 0.011*\"stable\" + 0.010*\"master\" + 0.010*\"data\" + 0.009*\"open\" + 0.009*\"learning\" + 0.009*\"reference\"')\n",
      "\n",
      "Topic: (7, '0.011*\"support\" + 0.010*\"download\" + 0.010*\"line\" + 0.010*\"pandas\" + 0.009*\"install\" + 0.009*\"information\" + 0.009*\"table\" + 0.009*\"default\" + 0.009*\"option\" + 0.008*\"case\"')\n",
      "\n",
      "Topic: (8, '0.062*\"text\" + 0.056*\"tree\" + 0.050*\"analysis\" + 0.042*\"common\" + 0.039*\"blob\" + 0.037*\"extension\" + 0.036*\"language\" + 0.036*\"processing\" + 0.033*\"journal\" + 0.031*\"graphs\"')\n",
      "\n",
      "Topic: (9, '0.122*\"test\" + 0.090*\"core\" + 0.086*\"run\" + 0.066*\"copy\" + 0.064*\"current\" + 0.059*\"order\" + 0.049*\"author\" + 0.045*\"free\" + 0.039*\"index\" + 0.032*\"wrapper\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics()\n",
    "for topic in topics:\n",
    "    print(f\"\\nTopic: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
