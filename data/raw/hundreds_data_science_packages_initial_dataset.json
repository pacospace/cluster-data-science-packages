{"alibi": {"file_name": "SeldonIO/alibi/README.md", "raw_text": "<p align=\"center\">\n  <img src=\"doc/source/_static/Alibi_Logo.png\" alt=\"Alibi Logo\" width=\"50%\">\n</p>\n\n[![Build Status](https://travis-ci.com/SeldonIO/alibi.svg?branch=master)](https://travis-ci.com/SeldonIO/alibi)\n[![Documentation Status](https://readthedocs.org/projects/alibi/badge/?version=latest)](https://docs.seldon.io/projects/alibi/en/latest/?badge=latest)\n[![codecov](https://codecov.io/gh/SeldonIO/alibi/branch/master/graph/badge.svg)](https://codecov.io/gh/SeldonIO/alibi)\n![Python version](https://img.shields.io/badge/python-3.6%20%7C%203.7-blue.svg)\n[![PyPI version](https://badge.fury.io/py/alibi.svg)](https://badge.fury.io/py/alibi)\n![GitHub Licence](https://img.shields.io/github/license/seldonio/alibi.svg)\n[![Slack channel](https://img.shields.io/badge/chat-on%20slack-e51670.svg)](http://seldondev.slack.com/messages/alibi)\n---\n[Alibi](https://docs.seldon.io/projects/alibi) is an open source Python library aimed at machine learning model inspection and interpretation.\nThe initial focus on the library is on black-box, instance based model explanations.\n*  [Documentation](https://docs.seldon.io/projects/alibi/en/latest/)\n\nIf you're interested in outlier detection, concept drift or adversarial instance detection, check out our sister project [alibi-detect](https://github.com/SeldonIO/alibi-detect).\n\n## Goals\n* Provide high quality reference implementations of black-box ML model explanation and interpretation algorithms\n* Define a consistent API for interpretable ML methods\n* Support multiple use cases (e.g. tabular, text and image data classification, regression)\n\n## Installation\nAlibi can be installed from [PyPI](https://pypi.org/project/alibi):\n```bash\npip install alibi\n```\nThis will install `alibi` with all its dependencies:\n```bash\n  attrs\n  beautifulsoup4\n  numpy\n  Pillow\n  pandas\n  prettyprinter\n  requests\n  scikit-learn\n  scikit-image\n  scipy\n  shap\n  spacy\n  tensorflow\n```\n\nTo run all the example notebooks, you may additionally run `pip install alibi[examples]` which will\ninstall the following:\n```bash\n  Keras\n  seaborn\n  xgboost\n```\n\n## Supported algorithms\n### Model explanations\nThese algorithms provide **instance-specific** (sometimes also called **local**) explanations of ML model\npredictions. Given a single instance and a model prediction they aim to answer the question \"Why did\nmy model make this prediction?\" The following algorithms all work with **black-box** models meaning that the\nonly requirement is to have acces to a prediction function (which could be an API endpoint for a model in production).\n\nThe following table summarizes the capabilities of the current algorithms:\n\n|Explainer|Model types|Classification|Categorical data|Tabular|Text|Images|Need training set|\n|:---|:---|:---:|:---:|:---:|:---:|:---:|:---|\n|[Anchors](https://docs.seldon.io/projects/alibi/en/latest/methods/Anchors.html)|black-box|\u2714|\u2714|\u2714|\u2714|\u2714|For Tabular|\n|[CEM](https://docs.seldon.io/projects/alibi/en/latest/methods/CEM.html)|black-box, TF/Keras|\u2714|\u2718|\u2714|\u2718|\u2714|Optional|\n|[Counterfactual Instances](https://docs.seldon.io/projects/alibi/en/latest/methods/CF.html)|black-box, TF/Keras|\u2714|\u2718|\u2714|\u2718|\u2714|No|\n|[Kernel SHAP](https://docs.seldon.io/projects/alibi/en/latest/methods/KernelSHAP.html)|black-box|\u2714|\u2714|\u2714|\u2718|\u2718|\u2714|\n|[Prototype Counterfactuals](https://docs.seldon.io/projects/alibi/en/latest/methods/CFProto.html)|black-box, TF/Keras|\u2714|\u2714|\u2714|\u2718|\u2714|Optional|\n\n - Anchor explanations ([Ribeiro et al., 2018](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf))\n   - [Documentation](https://docs.seldon.io/projects/alibi/en/latest/methods/Anchors.html)\n   - Examples:\n     [income prediction](https://docs.seldon.io/projects/alibi/en/latest/examples/anchor_tabular_adult.html),\n     [Iris dataset](https://docs.seldon.io/projects/alibi/en/latest/examples/anchor_tabular_iris.html),\n     [movie sentiment classification](https://docs.seldon.io/projects/alibi/en/latest/examples/anchor_text_movie.html),\n     [ImageNet](https://docs.seldon.io/projects/alibi/en/latest/examples/anchor_image_imagenet.html),\n     [fashion MNIST](https://docs.seldon.io/projects/alibi/en/latest/examples/anchor_image_fashion_mnist.html)\n\n- Contrastive Explanation Method (CEM, [Dhurandhar et al., 2018](https://papers.nips.cc/paper/7340-explanations-based-on-the-missing-towards-contrastive-explanations-with-pertinent-negatives))\n  - [Documentation](https://docs.seldon.io/projects/alibi/en/latest/methods/CEM.html)\n  - Examples: [MNIST](https://docs.seldon.io/projects/alibi/en/latest/examples/cem_mnist.html),\n    [Iris dataset](https://docs.seldon.io/projects/alibi/en/latest/examples/cem_iris.html)\n\n- Counterfactual Explanations (extension of\n  [Wachter et al., 2017](https://arxiv.org/abs/1711.00399))\n  - [Documentation](https://docs.seldon.io/projects/alibi/en/latest/methods/CF.html)\n  - Examples: \n    [MNIST](https://docs.seldon.io/projects/alibi/en/latest/examples/cf_mnist.html)\n\n- Kernel Shapley Additive Explanations ([Lundberg et al., 2017](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions))\n  - [Documentation](https://docs.seldon.io/projects/alibi/en/latest/methods/KernelSHAP.html)\n  - Examples:\n    [SVM with continuous data](https://docs.seldon.io/projects/alibi/en/latest/examples/kernel_shap_wine_intro.html),\n    [multinomial logistic regression with continous data](https://docs.seldon.io/projects/alibi/en/latest/examples/kernel_shap_wine_lr.html),\n    [handling categorical variables](https://docs.seldon.io/projects/alibi/en/latest/examples/kernel_shap_adult_lr.html)\n    \n- Counterfactual Explanations Guided by Prototypes ([Van Looveren et al., 2019](https://arxiv.org/abs/1907.02584))\n  - [Documentation](https://docs.seldon.io/projects/alibi/en/latest/methods/CFProto.html)\n  - Examples:\n    [MNIST](https://docs.seldon.io/projects/alibi/en/latest/examples/cfproto_mnist.html),\n    [Boston housing dataset](https://docs.seldon.io/projects/alibi/en/latest/examples/cfproto_housing.html),\n    [Adult income (one-hot)](https://docs.seldon.io/projects/alibi/en/latest/examples/cfproto_cat_adult_ohe.html),\n    [Adult income (ordinal)](https://docs.seldon.io/projects/alibi/en/latest/examples/cfproto_cat_adult_ord.html)\n\n### Model confidence metrics\nThese algorihtms provide **instance-specific** scores measuring the model confidence for making a\nparticular prediction.\n\n|Algorithm|Model types|Classification|Regression|Categorical data|Tabular|Text|Images|Need training set|\n|:---|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---|\n|[Trust Scores](https://docs.seldon.io/projects/alibi/en/latest/methods/TrustScores.html)|black-box|\u2714|\u2718|\u2718|\u2714|\u2714(1)|\u2714(2)|Yes|\n|[Linearity Measure](https://docs.seldon.io/projects/alibi/en/latest/examples/linearity_measure_iris.html)|black-box|\u2714|\u2714|\u2718|\u2714|\u2718|\u2714|Optional|\n\n(1) Depending on model\n\n(2) May require dimensionality reduction\n\n- Trust Scores ([Jiang et al., 2018](https://arxiv.org/abs/1805.11783))\n  - [Documentation](https://docs.seldon.io/projects/alibi/en/latest/methods/TrustScores.html)\n  - Examples:\n    [MNIST](https://docs.seldon.io/projects/alibi/en/latest/examples/trustscore_mnist.html),\n    [Iris dataset](https://docs.seldon.io/projects/alibi/en/latest/examples/trustscore_mnist.html)\n- Linearity Measure\n  - Examples:\n    [Iris dataset](https://docs.seldon.io/projects/alibi/en/latest/examples/linearity_measure_iris.html),\n    [fashion MNIST](https://docs.seldon.io/projects/alibi/en/latest/examples/linearity_measure_fashion_mnist.html)\n\n## Example outputs\n\n[**Anchor method applied to the InceptionV3 model trained on ImageNet:**](examples/anchor_image_imagenet.ipynb)\n\nPrediction: Persian Cat             | Anchor explanation\n:-------------------------:|:------------------:\n![Persian Cat](doc/source/methods/persiancat.png)| ![Persian Cat Anchor](doc/source/methods/persiancatanchor.png)\n\n[**Contrastive Explanation method applied to a CNN trained on MNIST:**](examples/cem_mnist.ipynb)\n\nPrediction: 4             |  Pertinent Negative: 9               | Pertinent Positive: 4\n:-------------------------:|:-------------------:|:------------------:\n![mnist_orig](doc/source/methods/mnist_orig.png)  | ![mnsit_pn](doc/source/methods/mnist_pn.png) | ![mnist_pp](doc/source/methods/mnist_pp.png)\n\n[**Trust scores applied to a softmax classifier trained on MNIST:**](examples/trustscore_mnist.ipynb)\n\n![trust_mnist](doc/source/_static/trustscores.png)\n\n## Citations\nIf you use alibi in your research, please consider citing it.\n\nBibTeX entry:\n\n```\n@software{alibi,\n  title = {Alibi: Algorithms for monitoring and explaining machine learning models},\n  author = {Klaise, Janis and Van Looveren, Arnaud and Vacanti, Giovanni and Coca, Alexandru},\n  url = {https://github.com/SeldonIO/alibi},\n  version = {0.4.0},\n  date = {2020-03-20},\n}\n```\n"}, "altair": {"file_name": "altair-viz/altair/README.md", "raw_text": "# Altair <a href=\"https://altair-viz.github.io/\"><img align=\"right\" src=\"https://altair-viz.github.io/_static/altair-logo-light.png\" height=\"50\"></img></a>\n\n[![build status](http://img.shields.io/travis/altair-viz/altair/master.svg?style=flat)](https://travis-ci.org/altair-viz/altair)\n[![github actions](https://github.com/altair-viz/altair/workflows/build/badge.svg)](https://github.com/altair-viz/altair/actions?query=workflow%3Abuild)\n[![code style black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![JOSS Paper](http://joss.theoj.org/papers/10.21105/joss.01057/status.svg)](http://joss.theoj.org/papers/10.21105/joss.01057)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/altair)](https://pypi.org/project/altair)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/altair-viz/altair_notebooks/master?urlpath=lab/tree/notebooks/Index.ipynb)\n[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/altair-viz/altair_notebooks/blob/master/notebooks/Index.ipynb)\n\n[http://altair-viz.github.io](http://altair-viz.github.io)\n\n**Altair** is a declarative statistical visualization library for Python. With Altair, you can spend more time understanding your data and its meaning. Altair's\nAPI is simple, friendly and consistent and built on top of the powerful\n[Vega-Lite](https://github.com/vega/vega-lite) JSON specification. This elegant\nsimplicity produces beautiful and effective visualizations with a minimal amount of code. *Altair is developed by [Jake Vanderplas](https://github.com/jakevdp) and [Brian\nGranger](https://github.com/ellisonbg) in close collaboration with the [UW\nInteractive Data Lab](http://idl.cs.washington.edu/).*\n\n## Altair Documentation\n\nSee [Altair's Documentation Site](http://altair-viz.github.io),\nas well as Altair's [Tutorial Notebooks](http://github.com/altair-viz/altair_notebooks).\n\n## Example\n\nHere is an example using Altair to quickly visualize and display a dataset with the native Vega-Lite renderer in the JupyterLab:\n\n```python\nimport altair as alt\n\n# load a simple dataset as a pandas DataFrame\nfrom vega_datasets import data\ncars = data.cars()\n\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color='Origin',\n)\n```\n\n![Altair Visualization](https://raw.githubusercontent.com/altair-viz/altair/master/images/cars.png)\n\nOne of the unique features of Altair, inherited from Vega-Lite, is a declarative grammar of not just visualization, but _interaction_. \nWith a few modifications to the example above we can create a linked histogram that is filtered based on a selection of the scatter plot.\n\n```python \nimport altair as alt\nfrom vega_datasets import data\n\nsource = data.cars()\n\nbrush = alt.selection(type='interval')\n\npoints = alt.Chart(source).mark_point().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color=alt.condition(brush, 'Origin', alt.value('lightgray'))\n).add_selection(\n    brush\n)\n\nbars = alt.Chart(source).mark_bar().encode(\n    y='Origin',\n    color='Origin',\n    x='count(Origin)'\n).transform_filter(\n    brush\n)\n\npoints & bars\n```\n\n![Altair Visualization Gif](https://raw.githubusercontent.com/altair-viz/altair/master/images/cars_scatter_bar.gif)\n\n\n## Getting your Questions Answered\n\nIf you have a question that is not addressed in the documentation, there are several ways to ask:\n\n- open a [Github Issue](https://github.com/altair-viz/altair/issues)\n- post a [StackOverflow Question](https://stackoverflow.com/questions/tagged/altair) (be sure to use the `altair` tag)\n- ask on the [Altair Google Group](https://groups.google.com/forum/#!forum/altair-viz)\n\nWe'll do our best to get your question answered\n\n## A Python API for statistical visualizations\n\nAltair provides a Python API for building statistical visualizations in a declarative\nmanner. By statistical visualization we mean:\n\n* The **data source** is a `DataFrame` that consists of columns of different data types (quantitative, ordinal, nominal and date/time).\n* The `DataFrame` is in a [tidy format](http://vita.had.co.nz/papers/tidy-data.pdf)\n  where the rows correspond to samples and the columns correspond to the observed variables.\n* The data is mapped to the **visual properties** (position, color, size, shape,\n  faceting, etc.) using the group-by data transformation.\n\nThe Altair API contains no actual visualization rendering code but instead\nemits JSON data structures following the\n[Vega-Lite](https://github.com/vega/vega-lite) specification. The resulting\nVega-Lite JSON data can be rendered in the following user-interfaces:\n\n* [Jupyter Notebook](https://github.com/jupyter/notebook) (by installing [ipyvega](https://github.com/vega/ipyvega)).\n* [JupyterLab](https://github.com/jupyterlab/jupyterlab) (no additional dependencies needed).\n* [nteract](https://github.com/nteract/nteract) (no additional dependencies needed).\n\n## Features\n\n* Carefully-designed, declarative Python API based on\n  [traitlets](https://github.com/ipython/traitlets).\n* Auto-generated internal Python API that guarantees visualizations are type-checked and\n  in full conformance with the [Vega-Lite](https://github.com/vega/vega-lite)\n  specification.\n* Auto-generate Altair Python code from a Vega-Lite JSON spec.\n* Display visualizations in the live Jupyter Notebook, JupyterLab, nteract, on GitHub and\n  [nbviewer](http://nbviewer.jupyter.org/).\n* Export visualizations to PNG/SVG images, stand-alone HTML pages and the\n[Online Vega-Lite Editor](https://vega.github.io/editor/#/).\n* Serialize visualizations as JSON files.\n* Explore Altair with dozens of examples in the [Example Gallery](https://altair-viz.github.io/gallery/index.html)\n\n## Installation\n\nTo use Altair for visualization, you need to install two sets of tools\n\n1. The core Altair Package and its dependencies\n\n2. The renderer for the frontend you wish to use (i.e. `Jupyter Notebook`,\n   `JupyterLab`, or `nteract`)\n\nAltair can be installed with either ``pip`` or with ``conda``.\nFor full installation instructions, please see\nhttps://altair-viz.github.io/getting_started/installation.html\n\n## Example and tutorial notebooks\n\nWe maintain a separate Github repository of Jupyter Notebooks that contain an\ninteractive tutorial and examples:\n\nhttps://github.com/altair-viz/altair_notebooks\n\nTo launch a live notebook server with those notebook using [binder](https://mybinder.org/) or\n[Colab](http://colab.research.google.com), click on one of the following badges:\n\n[![Binder](https://beta.mybinder.org/badge.svg)](https://beta.mybinder.org/v2/gh/altair-viz/altair_notebooks/master)\n[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/altair-viz/altair_notebooks/blob/master/notebooks/Index.ipynb)\n\n## Project philosophy\n\nMany excellent plotting libraries exist in Python, including the main ones:\n\n* [Matplotlib](http://matplotlib.org/)\n* [Bokeh](http://bokeh.pydata.org/en/latest/)\n* [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/#)\n* [Lightning](http://lightning-viz.org/)\n* [Plotly](https://plot.ly/)\n* [Pandas built-in plotting](http://pandas.pydata.org/pandas-docs/stable/visualization.html)\n* [HoloViews](http://holoviews.org)\n* [VisPy](http://vispy.org/)\n* [pygg](http://www.github.com/sirrice/pygg)\n\nEach library does a particular set of things well.\n\n### User challenges\n\nHowever, such a proliferation of options creates great difficulty for users\nas they have to wade through all of these APIs to find which of them is the\nbest for the task at hand. None of these libraries are optimized for\nhigh-level statistical visualization, so users have to assemble their own\nusing a mishmash of APIs. For individuals just learning data science, this\nforces them to focus on learning APIs rather than exploring their data.\n\nAnother challenge is current plotting APIs require the user to write code,\neven for incidental details of a visualization. This results in an unfortunate\nand unnecessary cognitive burden as the visualization type (histogram,\nscatterplot, etc.) can often be inferred using basic information such as the\ncolumns of interest and the data types of those columns.\n\nFor example, if you are interested in the visualization of two numerical\ncolumns, a scatterplot is almost certainly a good starting point. If you add\na categorical column to that, you probably want to encode that column using\ncolors or facets. If inferring the visualization proves difficult at times, a\nsimple user interface can construct a visualization without any coding.\n[Tableau](http://www.tableau.com/) and the [Interactive Data\nLab's](http://idl.cs.washington.edu/)\n[Polestar](https://github.com/vega/polestar) and\n[Voyager](https://github.com/vega/voyager) are excellent examples of such UIs.\n\n### Design approach and solution\n\nWe believe that these challenges can be addressed without the creation of yet\nanother visualization library that has a programmatic API and built-in\nrendering. Altair's approach to building visualizations uses a layered design\nthat leverages the full capabilities of existing visualization libraries:\n\n1. Create a constrained, simple Python API (Altair) that is purely declarative\n2. Use the API (Altair) to emit JSON output that follows the Vega-Lite spec\n3. Render that spec using existing visualization libraries\n\nThis approach enables users to perform exploratory visualizations with a much\nsimpler API initially, pick an appropriate renderer for their usage case, and\nthen leverage the full capabilities of that renderer for more advanced plot\ncustomization.\n\nWe realize that a declarative API will necessarily be limited compared to the\nfull programmatic APIs of Matplotlib, Bokeh, etc. That is a deliberate design\nchoice we feel is needed to simplify the user experience of exploratory\nvisualization.\n\n## Development install\n\nAltair requires the following dependencies:\n\n* [pandas](http://pandas.pydata.org/)\n* [traitlets](https://github.com/ipython/traitlets)\n* [IPython](https://github.com/ipython/ipython)\n\nIf you have cloned the repository, run the following command from the root of the repository:\n\n```\npip install -e .[dev]\n```\n\nIf you do not wish to clone the repository, you can install using:\n\n```\npip install git+https://github.com/altair-viz/altair\n```\n\n## Testing\n\nTo run the test suite you must have [py.test](http://pytest.org/latest/) installed.\nTo run the tests, use\n\n```\npy.test --pyargs altair\n```\n(you can omit the `--pyargs` flag if you are running the tests from a source checkout).\n\n## Feedback and Contribution\n\nSee [`CONTRIBUTING.md`](https://github.com/altair-viz/altair/blob/master/CONTRIBUTING.md)\n\n## Citing Altair\n\n[![JOSS Paper](http://joss.theoj.org/papers/10.21105/joss.01057/status.svg)](http://joss.theoj.org/papers/10.21105/joss.01057)\n\nIf you use Altair in academic work, please consider citing http://joss.theoj.org/papers/10.21105/joss.01057 as\n\n```bib\n@article{2018-altair,\n  doi = {10.21105/joss.01057},\n  url = {https://doi.org/10.21105/joss.01057},\n  year  = {2018},\n  month = {dec},\n  publisher = {The Open Journal},\n  author = {Jacob VanderPlas and Brian Granger and Jeffrey Heer and Dominik Moritz and Kanit Wongsuphasawat and Arvind Satyanarayan and Eitan Lees and Ilia Timofeev and Ben Welsh and Scott Sievert},\n  title = {Altair: Interactive Statistical Visualizations for Python},\n  journal = {Journal of Open Source Software}\n}\n```\nPlease additionally consider citing the [vega-lite](http://vega.github.io/vega-lite/) project, which Altair is based on: https://dl.acm.org/doi/10.1109/TVCG.2016.2599030\n```bib\n@article{2017-vega-lite,\n title = {Vega-Lite: A Grammar of Interactive Graphics},\n author = {Arvind Satyanarayan AND Dominik Moritz AND Kanit Wongsuphasawat AND Jeffrey Heer},\n journal = {IEEE Trans. Visualization \\& Comp. Graphics (Proc. InfoVis)},\n year = {2017},\n url = {http://idl.cs.washington.edu/papers/vega-lite},\n}\n```\n\n## Whence Altair?\n\nAltair is the [brightest star](https://en.wikipedia.org/wiki/Altair) in the constellation Aquila, and along with Deneb and Vega forms the northern-hemisphere asterism known as the [Summer Triangle](https://en.wikipedia.org/wiki/Summer_Triangle).\n"}, "ax-platform": {"file_name": "facebook/Ax/README.md", "raw_text": "<img width=\"300\" src=\"website/static/img/ax_logo_lockup.svg\" alt=\"Ax Logo\" />\n\n<hr/>\n\n[![Build Status](https://img.shields.io/pypi/v/ax-platform.svg)](https://pypi.org/project/ax-platform/)\n[![Build Status](https://img.shields.io/pypi/pyversions/ax-platform.svg)](https://pypi.org/project/ax-platform/)\n[![Build Status](https://img.shields.io/pypi/wheel/ax-platform.svg)](https://pypi.org/project/ax-platform/)\n[![Build Status](https://travis-ci.com/facebook/Ax.svg?token=m8nxq4QpA9U383aZWDyF&branch=master)](https://travis-ci.com/facebook/Ax)\n[![codecov](https://codecov.io/gh/facebook/Ax/branch/master/graph/badge.svg)](https://codecov.io/gh/facebook/Ax)\n[![Build Status](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE.md)\n\nAx is an accessible, general-purpose platform for understanding, managing,\ndeploying, and automating adaptive experiments.\n\nAdaptive experimentation is the machine-learning guided process of iteratively\nexploring a (possibly infinite) parameter space in order to identify optimal\nconfigurations in a resource-efficient manner. Ax currently supports Bayesian\noptimization and bandit optimization as exploration strategies. Bayesian\noptimization in Ax is powered by [BoTorch](https://github.com/facebookexternal/botorch),\na modern library for Bayesian optimization research built on PyTorch.\n\nFor full documentation and tutorials, see the [Ax website](https://ax.dev)\n\n## Why Ax?\n\n* **Versatility**: Ax supports different kinds of experiments, from dynamic ML-assisted A/B testing, to hyperparameter optimization in machine learning.\n* **Customization**: Ax makes it easy to add new modeling and decision algorithms, enabling research and development with minimal overhead.\n* **Production-completeness**: Ax comes with storage integration and ability to fully save and reload experiments.\n* **Support for multi-modal and constrained experimentation**: Ax allows for running and combining multiple experiments (e.g. simulation with a real-world \"online\" A/B test) and for constrained optimization (e.g. improving classification accuracy without signifant increase in resource-utilization).\n* **Efficiency in high-noise setting**: Ax offers state-of-the-art algorithms specifically geared to noisy experiments, such as simulations with reinforcement-learning agents.\n* **Ease of use**: Ax includes 3 different APIs that strike different balances between lightweight structure and flexibility. Using the most concise Loop API, a whole optimization can be done in just one function call. The Service API integrates easily with external schedulers. The most elaborate Developer API affords full algorithm customization and experiment introspection.\n\n## Getting Started\n\nTo run a simple optimization loop in Ax (using the\n[Booth response surface](https://www.sfu.ca/~ssurjano/booth.html) as the\nartificial evaluation function):\n\n```python\n>>> from ax import optimize\n>>> best_parameters, best_values, experiment, model = optimize(\n        parameters=[\n          {\n            \"name\": \"x1\",\n            \"type\": \"range\",\n            \"bounds\": [-10.0, 10.0],\n          },\n          {\n            \"name\": \"x2\",\n            \"type\": \"range\",\n            \"bounds\": [-10.0, 10.0],\n          },\n        ],\n        # Booth function\n        evaluation_function=lambda p: (p[\"x1\"] + 2*p[\"x2\"] - 7)**2 + (2*p[\"x1\"] + p[\"x2\"] - 5)**2,\n        minimize=True,\n    )\n\n# best_parameters contains {'x1': 1.02, 'x2': 2.97}; the global min is (1, 3)\n```\n\n## Installation\n\n### Requirements\nYou need Python 3.7 or later to run Ax.\n\nThe required Python dependencies are:\n\n* [botorch](https://www.botorch.org)\n* jinja2\n* pandas\n* scipy\n* sklearn\n* plotly >=2.2.1\n\n### Stable Version\n\n#### Installing via pip\nWe recommend installing Ax via pip (even if using Conda environment):\n\n```\nconda install pytorch torchvision -c pytorch  # OSX only (details below)\npip3 install ax-platform\n```\n\nInstallation will use Python wheels from PyPI, available for [OSX, Linux, and Windows](https://pypi.org/project/ax-platform/#files).\n\n*Note*: Make sure the `pip3` being used to install `ax-platform` is actually the one from the newly created Conda environment.\nIf you're using a Unix-based OS, you can use `which pip3` to check.\n\n*Recommendation for MacOS users*: PyTorch is a required dependency of BoTorch, and can be automatically installed via pip.\nHowever, **we recommend you [install PyTorch manually](https://pytorch.org/get-started/locally/#anaconda-1) before installing Ax, using the Anaconda package manager**.\nInstalling from Anaconda will link against MKL (a library that optimizes mathematical computation for Intel processors).\nThis will result in up to an order-of-magnitude speed-up for Bayesian optimization, as at the moment, installing PyTorch from pip does not link against MKL.\n\nIf you need CUDA on MacOS, you will need to build PyTorch from source. Please consult the PyTorch installation instructions above.\n\n#### Optional Dependencies\n\nTo use Ax with a notebook environment, you will need Jupyter. Install it first:\n```\npip3 install jupyter\n```\n\nIf you want to store the experiments in MySQL, you will need SQLAlchemy:\n```\npip3 install SQLAlchemy\n```\n\n### Latest Version\n\n#### Installing from Git\n\nYou can install the latest (bleeding edge) version from Git:\n\n```\npip3 install git+ssh://git@github.com/facebook/Ax.git#egg=Ax\n```\n\nSee recommendation for installing PyTorch for MacOS users above.\n\nAt times, the bleeding edge for Ax can depend on bleeding edge versions of BoTorch (or GPyTorch). We therefore recommend installing those from Git as well:\n```\npip3 install git+https://github.com/cornellius-gp/gpytorch.git\npip3 install git+https://github.com/pytorch/botorch.git\n```\n\n#### Optional Dependencies\n\nIf using Ax in Jupyter notebooks:\n\n```\npip3 install git+ssh://git@github.com/facebook/Ax.git#egg=Ax[notebook]\n```\n\nTo support plotly-based plotting in newer Jupyter notebook versions\n\n```\npip install \"notebook>=5.3\" \"ipywidgets==7.5\"\n```\n\n[See Plotly repo's README](https://github.com/plotly/plotly.py#jupyter-notebook-support) for details and JupyterLab instructions.\n\nIf storing Ax experiments via SQLAlchemy in MySQL or SQLite:\n```\npip3 install git+ssh://git@github.com/facebook/Ax.git#egg=Ax[mysql]\n```\n\n## Join the Ax Community\nSee the [CONTRIBUTING](CONTRIBUTING.md) file for how to help out.\n\nWhen contributing to Ax, we recommend cloning the [repository](https://github.com/facebook/Ax) and installing all optional dependencies:\n\n```\n# bleeding edge versions of GPyTorch + BoTorch are recommended\npip3 install git+https://github.com/cornellius-gp/gpytorch.git\npip3 install git+https://github.com/pytorch/botorch.git\n\ngit clone https://github.com/facebook/ax.git\ncd ax\npip3 install -e .[notebook,mysql,dev]\n```\n\nSee recommendation for installing PyTorch for MacOS users above.\n\n## License\n\nAx is licensed under the [MIT license](LICENSE.md).\n"}, "arviz": {"file_name": "arviz-devs/arviz/README.md", "raw_text": "<img src=\"https://arviz-devs.github.io/arviz/_static/logo.png\" height=100></img>\n\n[![Azure Build Status](https://dev.azure.com/ArviZ/ArviZ/_apis/build/status/arviz-devs.arviz?branchName=master)](https://dev.azure.com/ArviZ/ArviZ/_build/latest?definitionId=1&branchName=master)\n[![codecov](https://codecov.io/gh/arviz-devs/arviz/branch/master/graph/badge.svg)](https://codecov.io/gh/arviz-devs/arviz)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)\n[![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/arviz-devs/community)\n[![DOI](http://joss.theoj.org/papers/10.21105/joss.01143/status.svg)](https://doi.org/10.21105/joss.01143) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2540945.svg)](https://doi.org/10.5281/zenodo.2540945)\n\n# ArviZ\n\nArviZ (pronounced \"AR-_vees_\") is a Python package for exploratory analysis of Bayesian models.\nIncludes functions for posterior analysis, model checking, comparison and diagnostics.\n\n### ArviZ in other languages\nArviZ also has a Julia wrapper available [ArviZ.jl](https://arviz-devs.github.io/ArviZ.jl/stable/).\n\n## Documentation\n\nThe ArviZ documentation can be found in the [official docs](https://arviz-devs.github.io/arviz/index.html).\nFirst time users may find the [quickstart](https://arviz-devs.github.io/arviz/notebooks/Introduction.html)\nto be helpful. Additional guidance can be found in the\n[usage documentation](https://arviz-devs.github.io/arviz/usage.html).\n\n\n## Installation\n\n### Stable\nArviZ is available for installation from [PyPI](https://pypi.org/project/arviz/).\nThe latest stable version can be installed using pip:\n\n```\npip install arviz\n```\n\nArviZ is also available through [conda-forge](https://anaconda.org/conda-forge/arviz).\n\n```\nconda install -c conda-forge arviz\n```\n\n### Development\nThe latest development version can be installed from the master branch using pip:\n\n```\npip install git+git://github.com/arviz-devs/arviz.git\n```\n\nAnother option is to clone the repository and install using git and setuptools:\n\n```\ngit clone https://github.com/arviz-devs/arviz.git\ncd arviz\npython setup.py install\n```\n\n-------------------------------------------------------------------------------\n## [Gallery](https://arviz-devs.github.io/arviz/examples/index.html)\n\n<p>\n<table>\n<tr>\n\n  <td>\n  <a href=\"https://arviz-devs.github.io/arviz/examples/matplotlib/mpl_plot_forest_ridge.html\">\n  <img alt=\"Ridge plot\"\n  src=\"https://arviz-devs.github.io/arviz/_static/mpl_plot_forest_ridge_thumb.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://arviz-devs.github.io/arviz/examples/matplotlib/mpl_plot_parallel.html\">\n  <img alt=\"Parallel plot\"\n  src=\"https://arviz-devs.github.io/arviz/_static/mpl_plot_parallel_thumb.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://arviz-devs.github.io/arviz/examples/matplotlib/mpl_plot_trace.html\">\n  <img alt=\"Trace plot\"\n  src=\"https://arviz-devs.github.io/arviz/_static/mpl_plot_trace_thumb.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://arviz-devs.github.io/arviz/examples/matplotlib/mpl_plot_density.html\">\n  <img alt=\"Density plot\"\n  src=\"https://arviz-devs.github.io/arviz/_static/mpl_plot_density_thumb.png\" />\n  </a>\n  </td>\n\n  </tr>\n  <tr>\n\n  <td>\n  <a href=\"https://arviz-devs.github.io/arviz/examples/matplotlib/mpl_plot_posterior.html\">\n  <img alt=\"Posterior plot\"\n  src=\"https://arviz-devs.github.io/arviz/_static/mpl_plot_posterior_thumb.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://arviz-devs.github.io/arviz/examples/matplotlib/mpl_plot_joint.html\">\n  <img alt=\"Joint plot\"\n  src=\"https://arviz-devs.github.io/arviz/_static/mpl_plot_joint_thumb.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://arviz-devs.github.io/arviz/examples/matplotlib/mpl_plot_ppc.html\">\n  <img alt=\"Posterior predictive plot\"\n  src=\"https://arviz-devs.github.io/arviz/_static/mpl_plot_ppc_thumb.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://arviz-devs.github.io/arviz/examples/matplotlib/mpl_plot_pair.html\">\n  <img alt=\"Pair plot\"\n  src=\"https://arviz-devs.github.io/arviz/_static/mpl_plot_pair_thumb.png\" />\n  </a>\n  </td>\n\n  </tr>\n  <tr>\n\n  <td>\n  <a href=\"https://arviz-devs.github.io/arviz/examples/matplotlib/mpl_plot_energy.html\">\n  <img alt=\"Energy Plot\"\n  src=\"https://arviz-devs.github.io/arviz/_static/mpl_plot_energy_thumb.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://arviz-devs.github.io/arviz/examples/matplotlib/mpl_plot_violin.html\">\n  <img alt=\"Violin Plot\"\n  src=\"https://arviz-devs.github.io/arviz/_static/mpl_plot_violin_thumb.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://arviz-devs.github.io/arviz/examples/matplotlib/mpl_plot_forest.html\">\n  <img alt=\"Forest Plot\"\n  src=\"https://arviz-devs.github.io/arviz/_static/mpl_plot_forest_thumb.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://arviz-devs.github.io/arviz/examples/matplotlib/mpl_plot_autocorr.html\">\n  <img alt=\"Autocorrelation Plot\"\n  src=\"https://arviz-devs.github.io/arviz/_static/mpl_plot_autocorr_thumb.png\" />\n  </a>\n  </td>\n\n</tr>\n</table>\n\n## Dependencies\n\nArviZ is tested on Python 3.6, 3.7 and 3.8, and depends on NumPy, SciPy, xarray, and Matplotlib.\n\n\n## Citation\n\n\nIf you use ArviZ and want to cite it please use [![DOI](http://joss.theoj.org/papers/10.21105/joss.01143/status.svg)](https://doi.org/10.21105/joss.01143)\n\nHere is the citation in BibTeX format\n\n```\n@article{arviz_2019,\n\ttitle = {{ArviZ} a unified library for exploratory analysis of {Bayesian} models in {Python}},\n\tauthor = {Kumar, Ravin and Colin, Carroll and Hartikainen, Ari and Martin, Osvaldo A.},\n\tjournal = {The Journal of Open Source Software},\n\tyear = {2019},\n\tdoi = {10.21105/joss.01143},\n\turl = {http://joss.theoj.org/papers/10.21105/joss.01143},\n}\n```\n\n\n## Contributions\nArviZ is a community project and welcomes contributions.\nAdditional information can be found in the [Contributing Readme](https://github.com/arviz-devs/arviz/blob/master/CONTRIBUTING.md)\n\n\n## Code of Conduct\nArviZ wishes to maintain a positive community. Additional details\ncan be found in the [Code of Conduct](https://github.com/arviz-devs/arviz/blob/master/CODE_OF_CONDUCT.md)\n\n## Sponsors\n[![NumFOCUS](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org)\n"}, "autogluon": {"file_name": "awslabs/autogluon/README.md", "raw_text": "\n\n<div align=\"left\">\n  <img src=\"https://user-images.githubusercontent.com/16392542/77208906-224aa500-6aba-11ea-96bd-e81806074030.png\" width=\"350\">\n</div>\n\n## AutoML Toolkit for Deep Learning\n\n[![Build Status](http://ci.mxnet.io/view/all/job/autogluon/job/master/badge/icon)](http://ci.mxnet.io/view/all/job/autogluon/job/master/)\n[![Pypi Version](https://img.shields.io/pypi/v/autogluon.svg)](https://pypi.org/project/autogluon/#history)\n![Upload Python Package](https://github.com/awslabs/autogluon/workflows/Upload%20Python%20Package/badge.svg)\n\nAutoGluon automates machine learning tasks enabling you to easily achieve strong predictive performance in your applications.  With just a few lines of code, you can train and deploy high-accuracy deep learning models on tabular, image, and text data. \n\n## Example\n\n```python\n# First install package from terminal:  pip install mxnet autogluon\n\nfrom autogluon import TabularPrediction as task\ntrain_data = task.Dataset(file_path='https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\ntest_data = task.Dataset(file_path='https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\npredictor = task.fit(train_data=train_data, label='class')\nperformance = predictor.evaluate(test_data)\n```\n\n## Resources\n\nSee the [AutoGluon Website](http://autogluon.mxnet.io/index.html) for instructions on:\n- [Installing AutoGluon](http://autogluon.mxnet.io/index.html#installation)\n- [Learning with tabular data](http://autogluon.mxnet.io/tutorials/tabular_prediction/tabular-quickstart.html): [(tips to maximize accuracy)](https://autogluon.mxnet.io/tutorials/tabular_prediction/tabular-indepth.html#maximizing-predictive-performance)\n- [Learning with image data](http://autogluon.mxnet.io/tutorials/image_classification/beginner.html)\n- [Learning with text data](http://autogluon.mxnet.io/tutorials/text_classification/beginner.html)\n- More advanced topics such as [Neural Architecture Search](http://autogluon.mxnet.io/tutorials/nas/index.html)\n\n### Scientific Publications\n- [AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data](https://arxiv.org/pdf/2003.06505.pdf) (*Arxiv*, 2020)\n\n### Articles\n- [AutoGluon for tabular data: 3 lines of code to achieve top 1% in Kaggle competitions](https://aws.amazon.com/blogs/opensource/machine-learning-with-autogluon-an-open-source-automl-library/) (*AWS Open Source Blog*, Mar 2020)\n- [Accurate image classification in 3 lines of code with AutoGluon](https://medium.com/@zhanghang0704/image-classification-on-kaggle-using-autogluon-fc896e74d7e8) (*Medium*, Feb 2020)\n- [AutoGluon overview & example applications](https://towardsdatascience.com/autogluon-deep-learning-automl-5cdb4e2388ec?source=friends_link&sk=e3d17d06880ac714e47f07f39178fdf2) (*Towards Data Science*, Dec 2019)\n\n## Citing AutoGluon\n\nIf you use AutoGluon in a scientific publication, please cite the following paper:\n\nErickson, Nick, et al. [\"AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data.\"](https://arxiv.org/abs/2003.06505) arXiv preprint arXiv:2003.06505 (2020).\n\nBibTeX entry:\n\n```bibtex\n@article{agtabular,\n  title={AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data},\n  author={Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander},\n  journal={arXiv preprint arXiv:2003.06505},\n  year={2020}\n}\n```\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n\n## Contributing to AutoGluon\n\nWe are actively accepting code contributions to the AutoGluon project. If you are interested in contributing to AutoGluon, please read the [Contributing Guide](https://github.com/awslabs/autogluon/blob/master/CONTRIBUTING.md) to get started.\n"}, "autokeras": {"file_name": "keras-team/autokeras/README.md", "raw_text": "<img src=\"https://autokeras.com/img/row_red.svg\" alt=\"drawing\" width=\"400px\" style=\"display: block; margin-left: auto; margin-right: auto\"/>\n\n![](https://github.com/keras-team/autokeras/workflows/Tests/badge.svg?branch=master)\n[![codecov](https://codecov.io/gh/keras-team/autokeras/branch/master/graph/badge.svg)](https://codecov.io/gh/keras-team/autokeras)\n[![PyPI version](https://badge.fury.io/py/autokeras.svg)](https://badge.fury.io/py/autokeras)\n\nOfficial Website: [autokeras.com](https://autokeras.com)\n\n##\n\nAutoKeras: An AutoML system based on Keras.\nIt is developed by <a href=\"http://faculty.cs.tamu.edu/xiahu/index.html\" target=\"_blank\" rel=\"nofollow\">DATA Lab</a> at Texas A&M University.\nThe goal of AutoKeras is to make machine learning accessible for everyone.\n\n## Example\n\nHere is a short example of using the package.\n\n```\nimport autokeras as ak\n\nclf = ak.ImageClassifier()\nclf.fit(x_train, y_train)\nresults = clf.predict(x_test)\n```\n\nFor detailed tutorial, please check [here](https://autokeras.com/tutorial/overview/).\n\n## Installation\n\nTo install the package, please use the `pip` installation as follows:\n\n```shell\npip3 install autokeras\n```\n\nPlease follow the [installation guide](https://autokeras.com/install) for more details.\n\n**Note:** Currently, AutoKeras is only compatible with **Python >= 3.5** and **TensorFlow >= 2.1.0**.\n\n## Community\n<a href=\"https://keras-slack-autojoin.herokuapp.com/\"><img src=\"https://github.com/keras-team/autokeras/blob/master/docs/templates/img/slack.png?raw=true\" alt=\"drawing\" width=\"150\"/></a>\n\n[Request an invitation](https://keras-slack-autojoin.herokuapp.com/).\nUse the [#autokeras](https://app.slack.com/client/T0QKJHQRE/CSZ5MKZFU) channel for communication.\n\nYou can also follow us on Twitter [@autokeras](https://twitter.com/autokeras) for the latest news.\n\n## Contributing\n\nYou can follow the [Contributing Guide](https://autokeras.com/contributing/) to become a contributor.\n\nIf you don't know where to start, please join our community on [Slack](https://autokeras.com/#community) and ask us.\nWe will help you get started!\n\nThank all the contributors!\n\n<a href=\"https://github.com/keras-team/autokeras/graphs/contributors\"><img src=\"https://opencollective.com/autokeras/contributors.svg?avatarHeight=36&width=890&button=false\" /></a>\n\n\n## Backers\n\nWe accept financial support on [Open Collective](https://opencollective.com/autokeras).\nThank every backer for supporting us!\n\nOrganizations:\n<a href=\"https://opencollective.com/autokeras#backers\" target=\"_blank\"><img src=\"https://opencollective.com/autokeras/sponsor.svg?avatarHeight=36&width=890&button=false\"></a>\n\nIndividuals:\n<a href=\"https://opencollective.com/autokeras#backers\" target=\"_blank\"><img src=\"https://opencollective.com/autokeras/backer.svg?avatarHeight=36&width=890&button=false\"></a>\n\n## Cite this work\n\nHaifeng Jin, Qingquan Song, and Xia Hu. \"Auto-keras: An efficient neural architecture search system.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2019. ([Download](https://www.kdd.org/kdd2019/accepted-papers/view/auto-keras-an-efficient-neural-architecture-search-system))\n\nBiblatex entry:\n\n```bibtex\n@inproceedings{jin2019auto,\n  title={Auto-Keras: An Efficient Neural Architecture Search System},\n  author={Jin, Haifeng and Song, Qingquan and Hu, Xia},\n  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining},\n  pages={1946--1956},\n  year={2019},\n  organization={ACM}\n}\n```\n\n## DISCLAIMER\n\nPlease note that this is a **pre-release** version of the AutoKeras which is still undergoing final testing before its official release. The website, its software and all content found on it are provided on an\n\"as is\" and \"as available\" basis. AutoKeras does **not** give any warranties, whether express or implied, as to the suitability or usability of the website, its software or any of its content. AutoKeras will **not** be liable for any loss, whether such loss is direct, indirect, special or consequential, suffered by any party as a result of their use of the libraries or content. Any usage of the libraries is done at the user's own risk and the user will be solely responsible for any damage to any computer system or loss of data that results from such activities. Should you encounter any bugs, glitches, lack of functionality or\nother problems on the website, please let us know immediately so we\ncan rectify these accordingly. Your help in this regard is greatly\nappreciated.\n\n## Acknowledgements\n\nThe authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&M College of Engineering, and Texas A&M.\n"}, "avro": {"file_name": "apache/avro/README.md", "raw_text": "[![Build Status](https://travis-ci.org/apache/avro.svg?branch=master)](https://travis-ci.org/apache/avro)\n\n# Apache Avro\u2122\n\nApache Avro\u2122 is a data serialization system.\n\nLearn more about Avro, please visit our website at:\n\n  https://avro.apache.org/\n\nTo contribute to Avro, please read:\n\n  https://cwiki.apache.org/confluence/display/AVRO/How+To+Contribute\n"}, "beautifulsoup4": {"file_name": "beautifulsoup4_readme", "raw_text": "Beautiful Soup is a library that makes it easy to scrape information\nfrom web pages. It sits atop an HTML or XML parser, providing Pythonic\nidioms for iterating, searching, and modifying the parse tree.\n\n# Quick start\n\n```\n>>> from bs4 import BeautifulSoup\n>>> soup = BeautifulSoup(\"<p>Some<b>bad<i>HTML\")\n>>> print soup.prettify()\n<html>\n<body>\n<p>\nSome\n<b>\nbad\n<i>\nHTML\n</i>\n</b>\n</p>\n</body>\n</html>\n>>> soup.find(text=\"bad\")\nu'bad'\n>>> soup.i\n<i>HTML</i>\n#\n>>> soup = BeautifulSoup(\"<tag1>Some<tag2/>bad<tag3>XML\", \"xml\")\n#\n>>> print soup.prettify()\n<?xml version=\"1.0\" encoding=\"utf-8\">\n<tag1>\nSome\n<tag2 />\nbad\n<tag3>\nXML\n</tag3>\n</tag1>\n```\n\nTo go beyond the basics, [comprehensive documentation is available](http://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n\n# Links\n\n* [Homepage](http://www.crummy.com/software/BeautifulSoup/bs4/)\n* [Documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n* [Discussion group](http://groups.google.com/group/beautifulsoup/)\n* [Development](https://code.launchpad.net/beautifulsoup/)\n* [Bug tracker](https://bugs.launchpad.net/beautifulsoup/)\n* [Complete changelog](https://bazaar.launchpad.net/~leonardr/beautifulsoup/bs4/view/head:/CHANGELOG)\n\n# Note on Python 2 sunsetting\n\nSince 2012, Beautiful Soup has been developed as a Python 2 library\nwhich is automatically converted to Python 3 code as necessary. This\nmakes it impossible to take advantage of some features of Python\n3.\n\nFor this reason, I plan to discontinue Beautiful Soup's Python 2\nsupport at some point after December 31, 2020: one year after the\nsunset date for Python 2 itself. Beyond that point, new Beautiful Soup\ndevelopment will exclusively target Python 3. Of course, older\nreleases of Beautiful Soup, which support both versions, will continue\nto be available.\n\n# Supporting the project\n\nIf you use Beautiful Soup as part of your professional work, please consider a\n[Tidelift subscription](https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=readme).\nThis will support many of the free software projects your organization\ndepends on, not just Beautiful Soup.\n\nIf you use Beautiful Soup for personal projects, the best way to say\nthank you is to read\n[Tool Safety](https://www.crummy.com/software/BeautifulSoup/zine/), a zine I\nwrote about what Beautiful Soup has taught me about software\ndevelopment.\n\n# Building the documentation\n\nThe bs4/doc/ directory contains full documentation in Sphinx\nformat. Run `make html` in that directory to create HTML\ndocumentation.\n\n# Running the unit tests\n\nBeautiful Soup supports unit test discovery from the project root directory:\n\n```\n$ nosetests\n```\n\n```\n$ python -m unittest discover -s bs4\n```\n\nIf you checked out the source tree, you should see a script in the\nhome directory called test-all-versions. This script will run the unit\ntests under Python 2, then create a temporary Python 3 conversion of\nthe source and run the unit tests again under Python 3."}, "bentoml": {"file_name": "bentoml/BentoML/README.md", "raw_text": "[![pypi status](https://img.shields.io/pypi/v/bentoml.svg)](https://pypi.org/project/BentoML)\n[![python versions](https://img.shields.io/pypi/pyversions/bentoml.svg)](https://travis-ci.org/bentoml/BentoML)\n[![Downloads](https://pepy.tech/badge/bentoml)](https://pepy.tech/project/bentoml)\n[![build status](https://travis-ci.org/bentoml/BentoML.svg?branch=master)](https://travis-ci.org/bentoml/BentoML)\n[![Documentation Status](https://readthedocs.org/projects/bentoml/badge/?version=latest)](https://docs.bentoml.org/)\n[![join BentoML Slack](https://badgen.net/badge/Join/BentoML%20Slack/cyan?icon=slack)](https://join.slack.com/t/bentoml/shared_invite/enQtNjcyMTY3MjE4NTgzLTU3ZDc1MWM5MzQxMWQxMzJiNTc1MTJmMzYzMTYwMjQ0OGEwNDFmZDkzYWQxNzgxYWNhNjAxZjk4MzI4OGY1Yjg)\n\n> From ML model to production API endpoint with a few lines of code\n\n[![BentoML](https://raw.githubusercontent.com/bentoml/BentoML/master/docs/source/_static/img/bentoml.png)](https://github.com/bentoml/BentoML)\n\nBentoML is an open-source platform for __high-performance ML model serving__.\n\nWhat does BentoML do?\n\n* Turn your ML model into production API endpoint with just a few lines of code\n* Support all major machine learning training frameworks\n* High performance API serving system with adaptive micro-batching support\n* DevOps best practices baked in, simplify the transition from model development to production\n* Model management for teams, providing CLI and Web UI dashboard\n* Flexible model deployment orchestration with support for AWS Lambda, SageMaker, EC2, Docker, Kubernetes, KNative and more\n\n\ud83d\udc49 [Join BentoML Slack community](https://join.slack.com/t/bentoml/shared_invite/enQtNjcyMTY3MjE4NTgzLTU3ZDc1MWM5MzQxMWQxMzJiNTc1MTJmMzYzMTYwMjQ0OGEwNDFmZDkzYWQxNzgxYWNhNjAxZjk4MzI4OGY1Yjg)\n to hear about the latest development updates.\n\n---\n\n- [Getting Started](https://github.com/bentoml/BentoML#getting-started)\n- [Documentation](https://docs.bentoml.org/)\n- [Gallery](https://github.com/bentoml/gallery)\n- [Contributing](https://github.com/bentoml/BentoML#contributing)\n- [Releases](https://github.com/bentoml/BentoML#releases)\n- [License](https://github.com/bentoml/BentoML/blob/master/LICENSE)\n- [Blog](https://medium.com/bentoml)\n\n\n## Getting Started\n\nInstalling BentoML with `pip`:\n```bash\npip install bentoml\n```\n\nA minimal prediction service in BentoML looks something like this:\n\n```python\nfrom bentoml import env, artifacts, api, BentoService\nfrom bentoml.handlers import DataframeHandler\nfrom bentoml.artifact import SklearnModelArtifact\n\n@env(auto_pip_dependencies=True)\n@artifacts([SklearnModelArtifact('model')])\nclass IrisClassifier(BentoService):\n\n    @api(DataframeHandler)\n    def predict(self, df):\n        return self.artifacts.model.predict(df)\n```\n\nThis code defines a prediction service that requires a scikit-learn model, and asks\nBentoML to figure out the required PyPI pip packages automatically. It also defined\nan API, which is the entry point for accessing this prediction service. And the API is\nexpecting a `pandas.DataFrame` object as its input data.\n\nNow you are ready to train a model and serve the model with the `IrisClassifier` service\ndefined above. Save the above code to a new file `iris_classifier.py` and run the\nfollowing code:\n\n```python\nfrom sklearn import svm\nfrom sklearn import datasets\n\nfrom iris_classifier import IrisClassifier\n\nif __name__ == \"__main__\":\n    # Load training data\n    iris = datasets.load_iris()\n    X, y = iris.data, iris.target\n    \n    # Model Training\n    clf = svm.SVC(gamma='scale')\n    clf.fit(X, y)\n\n    # Create a iris classifier service instance\n    iris_classifier_service = IrisClassifier()\n\n    # Pack the newly trained model artifact\n    iris_classifier_service.pack('model', clf)\n\n    # Save the prediction service to disk for model serving\n    saved_path = iris_classifier_service.save()\n```\n\nYou've just created a BentoService SavedBundle, it's a versioned file archive that is\nready for production deployment. It contains the BentoService class you defined, all its\npython code dependencies and PyPI dependencies, and the trained scikit-learn model. By\ndefault, BentoML saves those files and related metadata under `~/bentoml` directory, but \nthis is easily customizable to a different directory or cloud storage like\n[Amazon S3](https://aws.amazon.com/s3/).\n\nYou can now start a REST API server by specifying the BentoService's name and version, \nor provide the file path to the saved bundle:\n\n```bash\nbentoml serve IrisClassifier:latest\n```\n\nAlternatively, in bash command line, you can get the absolute path to the saved\nBentoService from the JSON output of `bentoml get` command:\n```bash\nsaved_path=$(bentoml get IrisClassifier:latest -q | jq -r \".uri.uri\")\nbentoml serve $saved_path\n```\n\nThe REST API server provides web UI for testing and debugging the server. If you are\nrunning this command on your local machine, visit http://127.0.0.1:5000 in your browser\nand try out sending API request to the server. You can also send prediction request\nwith `curl` from command line:\n\n```bash\ncurl -i \\\n  --header \"Content-Type: application/json\" \\\n  --request POST \\\n  --data '[[5.1, 3.5, 1.4, 0.2]]' \\\n  http://localhost:5000/predict\n```\n\nThe BentoService SavedBundle directory is structured to work as a docker build context,\nthat can be used to build a API server docker container image:\n\n```bash\n\n\ndocker build -t my-org/iris-classifier:v1 $saved_path\n\ndocker run -p 5000:5000 -e BENTOML_ENABLE_MICROBATCH=True my-org/iris-classifier:v1\n```\n\nYou can also deploy your BentoService directly to cloud services such as AWS Lambda\nwith bentoml CLI's deployment management commands:\n\n```\n> bentoml get IrisClassifier\nBENTO_SERVICE                         CREATED_AT        APIS                       ARTIFACTS\nIrisClassifier:20200121114004_360ECB  2020-01-21 19:40  predict<DataframeHandler>  model<SklearnModelArtifact>\nIrisClassifier:20200120082658_4169CF  2020-01-20 16:27  predict<DataframeHandler>  clf<PickleArtifact>\n...\n\n> bentoml lambda deploy test-deploy -b IrisClassifier:20200121114004_360ECB\n...\n\n> bentoml deployment list\nNAME           NAMESPACE    PLATFORM    BENTO_SERVICE                         STATUS    AGE\ntest-deploy    dev          aws-lambda  IrisClassifier:20200121114004_360ECB  running   2 days and 11 hours\n...\n```\n\nMore detailed code and walkthrough can be found in the [BentoML Quickstart Guide](https://docs.bentoml.org/en/latest/quickstart.html).\n\n## Documentation\nFull documentation and API references: [https://docs.bentoml.org/](https://docs.bentoml.org/)\n\n\n## Examples\nVisit [bentoml/gallery](https://github.com/bentoml/gallery) repository for more\n examples and tutorials.\n\n#### FastAI\n* Pet Image Classification - [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/fast-ai/pet-image-classification/fast-ai-pet-image-classification.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/fast-ai/pet-image-classification/fast-ai-pet-image-classification.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/fast-ai/pet-image-classification/fast-ai-pet-image-classification.ipynb)\n* Salary Range Prediction - [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/fast-ai/salary-range-prediction/fast-ai-salary-range-prediction.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/fast-ai/salary-range-prediction/fast-ai-salary-range-prediction.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/fast-ai/salary-range-prediction/fast-ai-salary-range-prediction.ipynb)\n\n\n#### Scikit-Learn\n* Sentiment Analysis - [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/scikit-learn/sentiment-analysis/sklearn-sentiment-analysis.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/scikit-learn/sentiment-analysis/sklearn-sentiment-analysis.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/scikit-learn/sentiment-analysis/sklearn-sentiment-analysis.ipynb)\n\n\n#### PyTorch\n* Fashion MNIST - [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/pytorch/fashion-mnist/pytorch-fashion-mnist.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/pytorch/fashion-mnist/pytorch-fashion-mnist.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/pytorch/fashion-mnist/pytorch-fashion-mnist.ipynb)\n* CIFAR-10 Image Classification - [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/pytorch/cifar10-image-classification/pytorch-cifar10-image-classification.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/pytorch/cifar10-image-classification/pytorch-cifar10-image-classification.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/pytorch/cifar10-image-classification/pytorch-cifar10-image-classification.ipynb)\n\n\n#### Tensorflow Keras\n* Fashion MNIST - [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/keras/fashion-mnist/keras-fashion-mnist.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/keras/fashion-mnist/keras-fashion-mnist.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/keras/fashion-mnist/keras-fashion-mnist.ipynb)\n* Text Classification - [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/keras/text-classification/keras-text-classification.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/keras/text-classification/keras-text-classification.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/keras/text-classification/keras-text-classification.ipynb)\n* Toxic Comment Classifier - [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/keras/toxic-comment-classification/keras-toxic-comment-classification.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/keras/toxic-comment-classification/keras-toxic-comment-classification.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/keras/toxic-comment-classification/keras-toxic-comment-classification.ipynb)\n\n#### Tensorflow 2.0\n* tf.Function model - [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/tensorflow/echo/tensorflow-echo.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/tensorflow/echo/tensorflow-echo.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/tensorflow/echo/tensorflow-echo.ipynb)\n* Fashion MNIST - [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/tensorflow/fashion-mnist/tensorflow_2_fashion_mnist.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/tensorflow/fashion-mnist/tensorflow_2_fashion_mnist.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/tensorflow/fashion-mnist/tensorflow_2_fashion_mnist.ipynb)\n\n\n#### XGBoost\n* Titanic Survival Prediction - [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/xgboost/titanic-survival-prediction/xgboost-titanic-survival-prediction.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/xgboost/titanic-survival-prediction/xgboost-titanic-survival-prediction.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/xgboost/titanic-survival-prediction/xgboost-titanic-survival-prediction.ipynb)\n* League of Legend win Prediction - [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/xgboost/league-of-legend-win-prediction/xgboost-league-of-legend-win-prediction.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/xgboost/league-of-legend-win-prediction/xgboost-league-of-legend-win-prediction.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/xgboost/league-of-legend-win-prediction/xgboost-league-of-legend-win-prediction.ipynb)\n\n#### LightGBM\n* Titanic Survival Prediction -  [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/lightbgm/titanic-survival-prediction/lightbgm-titanic-survival-prediction.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/lightbgm/titanic-survival-prediction/lightbgm-titanic-survival-prediction.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/lightbgm/titanic-survival-prediction/lightbgm-titanic-survival-prediction.ipynb)\n\n#### H2O\n* Loan Default Prediction - [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/h2o/loan-prediction/h2o-loan-prediction.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/h2o/loan-prediction/h2o-loan-prediction.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/h2o/loan-prediction/h2o-loan-prediction.ipynb)\n* Prostate Cancer Prediction - [Google Colab](https://colab.research.google.com/github/bentoml/gallery/blob/master/h2o/prostate-cancer-classification/h2o-prostate-cancer-classification.ipynb) | [nbviewer](https://nbviewer.jupyter.org/github/bentoml/gallery/blob/master/h2o/prostate-cancer-classification/h2o-prostate-cancer-classification.ipynb) | [source](https://github.com/bentoml/gallery/blob/master/h2o/prostate-cancer-classification/h2o-prostate-cancer-classification.ipynb)\n\n### Deployment guides:\n* End-to-end deployment management with BentoML\n  - [BentoML AWS Lambda Deployment Guide](https://docs.bentoml.org/en/latest/deployment/aws_lambda.html)\n  - [BentoML AWS SageMaker Deployment Guide](https://docs.bentoml.org/en/latest/deployment/aws_sagemaker.html)\n\n* Deployment guides\n  - [BentoML Clipper.ai Deployment Guide](https://docs.bentoml.org/en/latest/deployment/clipper.html)\n  - [BentoML AWS ECS Deployment](https://docs.bentoml.org/en/latest/deployment/aws_ecs.html)\n  - [BentoML Google Cloud Run Deployment](https://docs.bentoml.org/en/latest/deployment/google_cloud_run.html)\n  - [BentoML Azure container instance Deployment](https://docs.bentoml.org/en/latest/deployment/azure_container_instance.html)\n  - [BentoML Knative Deployment](https://docs.bentoml.org/en/latest/deployment/knative.html)\n\n\n## Contributing\n\nHave questions or feedback? Post a [new github issue](https://github.com/bentoml/BentoML/issues/new/choose)\nor discuss in our Slack channel: [![join BentoML Slack](https://badgen.net/badge/Join/BentoML%20Slack/cyan?icon=slack)](https://join.slack.com/t/bentoml/shared_invite/enQtNjcyMTY3MjE4NTgzLTU3ZDc1MWM5MzQxMWQxMzJiNTc1MTJmMzYzMTYwMjQ0OGEwNDFmZDkzYWQxNzgxYWNhNjAxZjk4MzI4OGY1Yjg)\n\n\nWant to help build BentoML? Check out our\n[contributing guide](https://github.com/bentoml/BentoML/blob/master/CONTRIBUTING.md) and the\n[development guide](https://github.com/bentoml/BentoML/blob/master/DEVELOPMENT.md).\n\n\n## Releases\n\nBentoML is under active development and is evolving rapidly.\nCurrently it is a Beta release, __we may change APIs in future releases__.\n\nRead more about the latest features and changes in BentoML from the [releases page](https://github.com/bentoml/BentoML/releases).\n\n\n\n## Usage Tracking\n\nBentoML by default collects anonymous usage data using [Amplitude](https://amplitude.com).\nIt only collects BentoML library's own actions and parameters, no user or model data will be collected.\n[Here is the code that does it](https://github.com/bentoml/BentoML/blob/master/bentoml/utils/usage_stats.py).\n\nThis helps BentoML team to understand how the community is using this tool and\nwhat to build next. You can easily opt-out of usage tracking by running the following\ncommand:\n\n```bash\n# From terminal:\nbentoml config set usage_tracking=false\n```\n\n```python\n# From python:\nimport bentoml\nbentoml.config().set('core', 'usage_tracking', 'False')\n```\n\n## License\n\n[Apache License 2.0](https://github.com/bentoml/BentoML/blob/master/LICENSE)\n\n[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Fbentoml%2FBentoML.svg?type=large)](https://app.fossa.io/projects/git%2Bgithub.com%2Fbentoml%2FBentoML?ref=badge_large)\n"}, "bokeh": {"file_name": "bokeh/bokeh/README.md", "raw_text": "<a href=\"https://bokeh.org\">\n  <img src=\"https://static.bokeh.org/logos/logotype.svg\" height=\"60\" alt=\"Bokeh logotype\" />\n</a>\n\n----\n\n[Bokeh](https://bokeh.org) is an interactive visualization library for modern web browsers. It provides elegant, concise construction of versatile graphics, and affords high-performance interactivity over large or streaming datasets. Bokeh can help anyone who would like to quickly and easily make interactive plots, dashboards, and data applications.\n\n<table>\n<tr>\n  <td>Latest Release</td>\n  <td>\n    <img src=\"https://badge.fury.io/gh/bokeh%2Fbokeh.svg\" alt=\"Latest release version\" />\n    <a href=\"https://badge.fury.io/js/%40bokeh%2Fbokehjs\">\n      <img src=\"https://badge.fury.io/js/%40bokeh%2Fbokehjs.svg\" alt=\"npm version\">\n    </a>\n  </td>\n\n  <td>Conda</td>\n  <td>\n    <a href=\"https://docs.bokeh.org/en/latest/docs/installation.html\">\n    <img src=\"https://pyviz.org/_static/cache/bokeh_conda_downloads_badge.svg\"\n         alt=\"Conda downloads per month\" />\n    </a>\n  </td>\n</tr>\n\n<tr>\n  <td>License</td>\n  <td>\n    <a href=\"https://github.com/bokeh/bokeh/blob/master/LICENSE.txt\">\n    <img src=\"https://img.shields.io/github/license/bokeh/bokeh.svg\"\n         alt=\"Bokeh license (BSD 3-clause)\" />\n    </a>\n  </td>\n\n  <td>PyPI</td>\n  <td>\n    <a href=\"https://docs.bokeh.org/en/latest/docs/installation.html\">\n    <img src=\"https://img.shields.io/pypi/dm/bokeh.svg\"\n         alt=\"PyPI downloads per month\" />\n    </a>\n  </td>\n</tr>\n\n<tr>\n  <td>Sponsorship</td>\n  <td>\n    <a href=\"http://numfocus.org\">\n    <img src=\"https://img.shields.io/badge/powered%20by-NumFOCUS-black.svg?style=flat&colorA=5B5B5B&colorB=007D8A\"\n         alt=\"Powered by NumFOCUS\" />\n    </a>\n  </td>\n\n  <td>Live Tutorial</td>\n  <td>\n    <a href=\"https://mybinder.org/v2/gh/bokeh/bokeh-notebooks/master?filepath=tutorial%2F00%20-%20Introduction%20and%20Setup.ipynb\">\n    <img src=\"https://mybinder.org/badge_logo.svg\"\n         alt=\"Live Bokeh tutorial notebooks on MyBinder\" />\n    </a>\n  </td>\n</tr>\n\n<tr>\n  <td>Build Status</td>\n  <td>\n    <a href=\"https://github.com/bokeh/bokeh/actions\">\n    <img src=\"https://github.com/bokeh/bokeh/workflows/GitHub-CI/badge.svg?branch=master\"\n         alt=\"Current github actions build status\" />\n    </a>\n  </td>\n\n  <td>Support</td>\n  <td>\n    <a href=\"https://discourse.bokeh.org\">\n    <img src=\"https://img.shields.io/discourse/https/discourse.bokeh.org/posts.svg\"\n         alt=\"Community Support on discourse.bokeh.org\" />\n    </a>\n  </td>\n</tr>\n\n<tr>\n  <td>Static Analysis</td>\n  <td>\n    <a href=\"https://lgtm.com/projects/g/bokeh/bokeh/context:python\">\n    <img alt=\"Language grade: Python\" src=\"https://img.shields.io/lgtm/grade/python/g/bokeh/bokeh.svg?\"/>\n    </a>\n    <a href=\"https://lgtm.com/projects/g/bokeh/bokeh/context:javascript\">\n    <img alt=\"Language grade: JavaScript\" src=\"https://img.shields.io/lgtm/grade/javascript/g/bokeh/bokeh.svg\"/>\n    </a>\n  </td>\n\n  <td>Twitter</td>\n  <td>\n    <a href=\"https://twitter.com/bokeh\">\n    <img src=\"https://img.shields.io/twitter/follow/bokeh.svg?style=social&label=Follow\"\n         alt=\"Follow Bokeh on Twitter\" />\n    </a>\n  </td>\n</tr>\n\n</table>\n\n*If you like Bokeh and would like to support our mission, please consider [making a donation](https://numfocus.org/donate-to-bokeh).*\n\n<p>\n<table>\n<tr>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/image.html\">\n  <img alt=\"colormapped image plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/image_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/anscombe.html\">\n  <img alt=\"anscombe plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/anscombe_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/stocks.html\">\n  <img alt=\"stocks plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/stocks_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/lorenz.html\">\n  <img alt=\"lorenz attractor plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/lorenz_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/candlestick.html\">\n  <img alt=\"candlestick plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/candlestick_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/color_scatter.html\">\n  <img alt=\"scatter plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/scatter_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/iris_splom.html\">\n  <img alt=\"SPLOM plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/splom_t.png\" />\n  </a>\n  </td>\n\n</tr>\n<tr>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/iris.html\">\n  <img alt=\"iris dataset plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/iris_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/histogram.html\">\n  <img alt=\"histogram plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/histogram_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/periodic.html\">\n  <img alt=\"periodic table plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/periodic_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/texas.html\">\n  <img alt=\"choropleth plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/choropleth_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/burtin.html\">\n  <img alt=\"burtin antibiotic data plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/burtin_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/streamline.html\">\n  <img alt=\"streamline plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/streamline_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/image_rgba.html\">\n  <img alt=\"RGBA image plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/image_rgba_t.png\" />\n  </a>\n  </td>\n\n</tr>\n<tr>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/brewer.html\">\n  <img alt=\"stacked bars plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/stacked_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/quiver.html\">\n  <img alt=\"quiver plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/quiver_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/elements.html\">\n  <img alt=\"elements data plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/elements_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/boxplot.html\">\n  <img alt=\"boxplot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/boxplot_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/categorical.html\">\n  <img alt=\"categorical plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/categorical_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/unemployment.html\">\n  <img alt=\"unemployment data plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/unemployment_t.png\" />\n  </a>\n  </td>\n\n  <td>\n  <a href=\"https://docs.bokeh.org/en/latest/docs/gallery/les_mis.html\">\n  <img alt=\"Les Mis co-occurrence plot thumbnail\" src=\"https://docs.bokeh.org/en/latest/_images/les_mis_t.png\" />\n  </a>\n  </td>\n\n</tr>\n</table>\n</p>\n\n## Installation\n\nThe easiest way to install Bokeh is using the [Anaconda Python distribution](https://www.anaconda.com/what-is-anaconda/) and its included *Conda* package management system. To install Bokeh and its required dependencies, enter the following command at a Bash or Windows command prompt:\n\n```\nconda install bokeh\n```\n\nTo install using pip, enter the following command at a Bash or Windows command prompt:\n```\npip install bokeh\n```\n\nFor more information, refer to the [installation documentation](https://docs.bokeh.org/en/latest/docs/user_guide/quickstart.html#quick-installation).\n\n## Resources\n\nOnce Bokeh is installed, check out the [Getting Started](https://docs.bokeh.org/en/latest/docs/user_guide/quickstart.html#getting-started) section of the [Quickstart guide](https://docs.bokeh.org/en/latest/docs/user_guide/quickstart.html).\n\nVisit the [full documentation site](https://docs.bokeh.org) to view the [User's Guide](https://docs.bokeh.org/en/dev/docs/user_guide.html) or [launch the Bokeh tutorial](https://mybinder.org/v2/gh/bokeh/bokeh-notebooks/master?filepath=tutorial%2F00%20-%20Introduction%20and%20Setup.ipynb) to learn about Bokeh in live Jupyter Notebooks.\n\nCommunity support is available on the [Project Discourse](https://discourse.bokeh.org).\n\nIf you would like to contribute to Bokeh, please review the [Developer Guide](https://docs.bokeh.org/en/latest/docs/dev_guide.html) and say hello on the [Zulip Chat for Developers](https://bokeh.zulipchat.com/).\n\n## Follow us\n\nFollow us on Twitter [@bokeh](https://twitter.com/bokeh)\n\n## Sponsors\n\n### Fiscal Sponsors\n\nThe Bokeh project is grateful for [individual contributions](https://numfocus.org/donate-to-bokeh) as well as sponsorship by the organizations and companies below:\n\n<table>\n<tr>\n  <td>\n    <a href=\"https://www.numfocus.org/\">\n    <img src=\"https://static.bokeh.org/sponsor/numfocus.svg\"\n         alt=\"NumFocus Logo\" width=\"200\"/>\n    </a>\n  </td>\n  <td>\n    <a href=\"https://www.anaconda.com/\">\n    <img src=\"https://static.bokeh.org/sponsor/anaconda.png\"\n         alt=\"Anaconda Logo\" width=\"200\"/>\n    </a>\n  </td>\n  <td>\n    <a href=\"https://www.nvidia.com\">\n    <img src=\"https://static.bokeh.org/sponsor/nvidia.png\"\n         alt=\"NVidia Logo\" width=\"200\"/>\n    </a>\n  </td>\n  <td>\n    <a href=\"https://developer.nvidia.com/rapids\">\n    <img src=\"https://static.bokeh.org/sponsor/rapids.png\"\n         alt=\"Rapids Logo\" width=\"200\"/>\n    </a>\n  </td>\n</tr>\n</table>\n\n\n<table align=\"center\">\n<tr>\n  <td>\n    <a href=\"https://www.quansight.com\">\n    <img src=\"https://static.bokeh.org/sponsor/quansight.png\"\n         alt=\"Quansight Logo\" width=\"100\"/>\n    </a>\n  </td>\n  <td>\n    <a href=\"https://www.rexhomes.com/\">\n    <img src=\"https://static.bokeh.org/sponsor/rex.jpg\"\n         alt=\"Rex Logo\" width=\"100\"/>\n    </a>\n  </td>\n</tr>\n</table>\n\nIf your company uses Bokeh and is able to sponsor the project, please contact <a href=\"info@bokeh.org\">info@bokeh.org</a>\n\n*Bokeh is a Sponsored Project of NumFOCUS, a 501(c)(3) nonprofit charity in the United States. NumFOCUS provides Bokeh with fiscal, legal, and administrative support to help ensure the health and sustainability of the project. Visit [numfocus.org](https://numfocus.org) for more information.*\n\n*Donations to Bokeh are managed by NumFOCUS. For donors in the United States, your gift is tax-deductible to the extent provided by law. As with any donation, you should consult with your tax adviser about your particular tax situation.*\n\n### In-kind Sponsors\n\nThe Bokeh project is also grateful for the donation of services from the following companies:\n\n* [Amazon Web Services](https://aws.amazon.com/)\n* [GitGuardian](https://gitguardian.com/)\n* [GitHub](https://github.com/)\n* [Pingdom](https://www.pingdom.com/website-monitoring)\n* [Slack](https://slack.com)\n* [1Password](https://1password.com/)\n\n## Security\n\nTo report a security vulnerability, please use the [Tidelift security contact](https://tidelift.com/security).\nTidelift will coordinate the fix and disclosure.\n"}, "boto3": {"file_name": "boto/boto3/README.rst", "raw_text": "===============================\nBoto 3 - The AWS SDK for Python\n===============================\n\n|Build Status| |Version| |Gitter|\n\nBoto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for\nPython, which allows Python developers to write software that makes use\nof services like Amazon S3 and Amazon EC2. You can find the latest, most\nup to date, documentation at our `doc site`_, including a list of\nservices that are supported.\n\n\n.. _boto: https://docs.pythonboto.org/\n.. _`doc site`: https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n.. |Build Status| image:: http://img.shields.io/travis/boto/boto3/develop.svg?style=flat\n    :target: https://travis-ci.org/boto/boto3\n    :alt: Build Status\n.. |Gitter| image:: https://badges.gitter.im/boto/boto3.svg\n   :target: https://gitter.im/boto/boto3\n   :alt: Gitter\n.. |Downloads| image:: http://img.shields.io/pypi/dm/boto3.svg?style=flat\n    :target: https://pypi.python.org/pypi/boto3/\n    :alt: Downloads\n.. |Version| image:: http://img.shields.io/pypi/v/boto3.svg?style=flat\n    :target: https://pypi.python.org/pypi/boto3/\n    :alt: Version\n.. |License| image:: http://img.shields.io/pypi/l/boto3.svg?style=flat\n    :target: https://github.com/boto/boto3/blob/develop/LICENSE\n    :alt: License\n\nQuick Start\n-----------\nFirst, install the library and set a default region:\n\n.. code-block:: sh\n\n    $ pip install boto3\n\nNext, set up credentials (in e.g. ``~/.aws/credentials``):\n\n.. code-block:: ini\n\n    [default]\n    aws_access_key_id = YOUR_KEY\n    aws_secret_access_key = YOUR_SECRET\n\nThen, set up a default region (in e.g. ``~/.aws/config``):\n\n.. code-block:: ini\n\n    [default]\n    region=us-east-1\n\nThen, from a Python interpreter:\n\n.. code-block:: python\n\n    >>> import boto3\n    >>> s3 = boto3.resource('s3')\n    >>> for bucket in s3.buckets.all():\n            print(bucket.name)\n\nDevelopment\n-----------\n\nGetting Started\n~~~~~~~~~~~~~~~\nAssuming that you have Python and ``virtualenv`` installed, set up your\nenvironment and install the required dependencies like this instead of\nthe ``pip install boto3`` defined above:\n\n.. code-block:: sh\n\n    $ git clone https://github.com/boto/boto3.git\n    $ cd boto3\n    $ virtualenv venv\n    ...\n    $ . venv/bin/activate\n    $ pip install -r requirements.txt\n    $ pip install -e .\n\nRunning Tests\n~~~~~~~~~~~~~\nYou can run tests in all supported Python versions using ``tox``. By default,\nit will run all of the unit and functional tests, but you can also specify your own\n``nosetests`` options. Note that this requires that you have all supported\nversions of Python installed, otherwise you must pass ``-e`` or run the\n``nosetests`` command directly:\n\n.. code-block:: sh\n\n    $ tox\n    $ tox -- unit/test_session.py\n    $ tox -e py26,py33 -- integration/\n\nYou can also run individual tests with your default Python version:\n\n.. code-block:: sh\n\n    $ nosetests tests/unit\n\nGenerating Documentation\n~~~~~~~~~~~~~~~~~~~~~~~~\nSphinx is used for documentation. You can generate HTML locally with the\nfollowing:\n\n.. code-block:: sh\n\n    $ pip install -r requirements-docs.txt\n    $ cd docs\n    $ make html\n\n\nGetting Help\n------------\n\nWe use GitHub issues for tracking bugs and feature requests and have limited\nbandwidth to address them. Please use these community resources for getting\nhelp:\n\n* Ask a question on `Stack Overflow <https://stackoverflow.com/>`__ and tag it with `boto3 <https://stackoverflow.com/questions/tagged/boto3>`__\n* Come join the AWS Python community chat on `gitter <https://gitter.im/boto/boto3>`__\n* Open a support ticket with `AWS Support <https://console.aws.amazon.com/support/home#/>`__\n* If it turns out that you may have found a bug, please `open an issue <https://github.com/boto/boto3/issues/new>`__\n"}, "catboost": {"file_name": "catboost/catboost/README.md", "raw_text": "<img src=http://storage.mds.yandex.net/get-devtools-opensource/250854/catboost-logo.png width=300/>\n\n[Website](https://catboost.ai) |\n[Documentation](https://catboost.ai/docs/) |\n[Tutorials](https://catboost.ai/docs/concepts/tutorials.html) |\n[Installation](https://catboost.ai/docs/concepts/installation.html) |\n[Release Notes](https://github.com/catboost/catboost/releases)\n\n[![GitHub license](https://img.shields.io/github/license/catboost/catboost.svg)](https://github.com/catboost/catboost/blob/master/LICENSE)\n[![PyPI version](https://badge.fury.io/py/catboost.svg)](https://badge.fury.io/py/catboost)\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/catboost.svg)](https://anaconda.org/conda-forge/catboost)\n[![GitHub issues](https://img.shields.io/github/issues/catboost/catboost.svg)](https://github.com/catboost/catboost/issues)\n[![Telegram](https://img.shields.io/badge/chat-on%20Telegram-2ba2d9.svg)](https://t.me/catboost_en)\n\nCatBoost is a machine learning method based on [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) over decision trees.\n\nMain advantages of CatBoost:\n  - Superior quality when [compared](https://github.com/catboost/benchmarks/blob/master/README.md) with other GBDT libraries on many datasets.\n  - Best in class [prediction](https://catboost.ai/docs/concepts/c-plus-plus-api.html) speed.\n  - Support for both [numerical and categorical](https://catboost.ai/docs/concepts/algorithm-main-stages.html) features.\n  - Fast GPU and multi-GPU support for training out of the box.\n  - Visualization tools [included](https://catboost.ai/docs/features/visualization.html).\n\nGradient Boosting Survey\n--------------\nWe want to make the best Gradient Boosting library in the world. Please, help us to do so! Complete our [survey](https://forms.yandex.ru/surveys/10011699/?lang=en) to help us understand what is important for GBDT users.\n\nGet Started and Documentation\n--------------\nAll CatBoost documentation is available [here](https://catboost.ai/docs/concepts/about.html).\n\nInstall CatBoost by following the guide for the\n * [Python package](https://catboost.ai/docs/concepts/python-installation.html)\n * [R-package](https://catboost.ai/docs/concepts/r-installation.html)\n * [command line](https://catboost.ai/docs/concepts/cli-installation.html)\n\nNext you may want to investigate:\n* [Tutorials](https://github.com/catboost/tutorials/#readme)\n* [Training modes and metrics](https://catboost.ai/docs/concepts/loss-functions.html)\n* [Cross-validation](https://catboost.ai/docs/features/cross-validation.html#cross-validation)\n* [Parameters tuning](https://catboost.ai/docs/concepts/parameter-tuning.html)\n* [Feature importance calculation](https://catboost.ai/docs/features/feature-importances-calculation.html)\n* [Regular](https://catboost.ai/docs/features/prediction.html#prediction) and [staged](https://catboost.ai/docs/features/staged-prediction.html#staged-prediction) predictions\n\nCatboost models in production\n--------------\nIf you want to evaluate Catboost model in your application read [model api documentation](https://github.com/catboost/catboost/tree/master/catboost/CatboostModelAPI.md).\n\nQuestions and bug reports\n--------------\n* For reporting bugs please use the [catboost/bugreport](https://github.com/catboost/catboost/issues) page.\n* Ask a question on [Stack Overflow](https://stackoverflow.com/questions/tagged/catboost) with the catboost tag, we monitor this for new questions.\n* Seek prompt advice at [Telegram group](https://t.me/catboost_en) or Russian-speaking [Telegram chat](https://t.me/catboost_ru)\n\nHelp to Make CatBoost Better\n----------------------------\n* Check out [help wanted](https://github.com/catboost/catboost/labels/help%20wanted) issues to see what can be improved, or open an issue if you want something.\n* Add your stories and experience to [Awesome CatBoost](AWESOME.md).\n* To contribute to CatBoost you need to first read CLA text and add to your pull request, that you agree to the terms of the CLA. More information can be found\nin [CONTRIBUTING.md](https://github.com/catboost/catboost/blob/master/CONTRIBUTING.md)\n* Instructions for contributors can be found [here](https://catboost.ai/docs/concepts/development-and-contributions.html).\n\nNews\n--------------\nLatest news are published on [twitter](https://twitter.com/catboostml).\n\nReference Paper\n-------\nAnna Veronika Dorogush, Andrey Gulin, Gleb Gusev, Nikita Kazeev, Liudmila Ostroumova Prokhorenkova, Aleksandr Vorobev [\"Fighting biases with dynamic boosting\"](https://arxiv.org/abs/1706.09516). arXiv:1706.09516, 2017.\n\nAnna Veronika Dorogush, Vasily Ershov, Andrey Gulin [\"CatBoost: gradient boosting with categorical features support\"](http://learningsys.org/nips17/assets/papers/paper_11.pdf). Workshop on ML Systems\nat NIPS 2017.\n\nLicense\n-------\n\u00a9 YANDEX LLC, 2017-2019. Licensed under the Apache License, Version 2.0. See LICENSE file for more details.\n"}, "causalml": {"file_name": "uber/causalml/README.md", "raw_text": "<div align=\"center\">\n  <a href=\"https://github.com/uber/causalml\"><img width=\"380px\" height=\"140px\" src=\"https://raw.githubusercontent.com/uber/causalml/master/docs/_static/img/causalml_logo.png\"></a>\n</div>\n\n------------------------------------------------------\n\n[![PyPI Version](https://badge.fury.io/py/causalml.svg)](https://pypi.org/project/causalml/)\n[![Build Status](https://travis-ci.com/uber/causalml.svg?token=t7jFKh1sKGtbqHWp2sGn&branch=master)](https://travis-ci.com/uber/causalml)\n[![Documentation Status](https://readthedocs.org/projects/causalml/badge/?version=latest)](http://causalml.readthedocs.io/en/latest/?badge=latest)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3015/badge)](https://bestpractices.coreinfrastructure.org/projects/3015)\n\n\n# Disclaimer\nThis project is stable and being incubated for long-term support. It may contain new experimental code, for which APIs are subject to change.\n\n# Causal ML: A Python Package for Uplift Modeling and Causal Inference with ML\n\n**Causal ML** is a Python package that provides a suite of uplift modeling and causal inference methods using machine learning algorithms based on recent\nresearch. It provides a standard interface that allows user to estimate the Conditional Average Treatment Effect (CATE) or Individual Treatment\n Effect (ITE) from experimental or observational data. Essentially, it estimates the causal impact of intervention `T` on outcome `Y` for users\n with observed features `X`, without strong assumptions on the model form. Typical use cases include\n\n* **Campaign targeting optimization**: An important lever to increase ROI in an advertising campaign is to target the ad to the set of customers who will have a favorable response in a given KPI such as engagement or sales. CATE identifies these customers by estimating the effect of the KPI from ad exposure at the individual level from A/B experiment or historical observational data.\n\n* **Personalized engagement**: A company has multiple options to interact with its customers such as different product choices in up-sell or messaging channels for communications. One can use CATE to estimate the heterogeneous treatment effect for each customer and treatment option combination for an optimal personalized recommendation system.\n\nThe package currently supports the following methods\n\n* **Tree-based algorithms**\n    * Uplift tree/random forests on KL divergence, Euclidean Distance, and Chi-Square\n    * Uplift tree/random forests on Contextual Treatment Selection\n* **Meta-learner algorithms**\n    * S-learner\n    * T-learner\n    * X-learner\n    * R-learner\n\n\n# Installation\n\n## Prerequisites\n\nInstall dependencies:\n```\n$ pip install -r requirements.txt\n```\n\nInstall from pip:\n\n```\n$ pip install causalml\n```\n\nInstall from source:\n\n```\n$ git clone https://github.com/uber/causalml.git\n$ cd causalml\n$ python setup.py build_ext --inplace\n$ python setup.py install\n```\n\n\n# Quick Start\n\n## Average Treatment Effect Estimation with S, T, X, and R Learners\n\n```python\nfrom causalml.inference.meta import LRSRegressor\nfrom causalml.inference.meta import XGBTRegressor, MLPTRegressor\nfrom causalml.inference.meta import BaseXRegressor\nfrom causalml.inference.meta import BaseRRegressor\nfrom xgboost import XGBRegressor\nfrom causalml.dataset import synthetic_data\n\ny, X, treatment, _, _, e = synthetic_data(mode=1, n=1000, p=5, sigma=1.0)\n\nlr = LRSRegressor()\nte, lb, ub = lr.estimate_ate(X, treatment, y)\nprint('Average Treatment Effect (Linear Regression): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n\nxg = XGBTRegressor(random_state=42)\nte, lb, ub = xg.estimate_ate(X, treatment, y)\nprint('Average Treatment Effect (XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n\nnn = MLPTRegressor(hidden_layer_sizes=(10, 10),\n                 learning_rate_init=.1,\n                 early_stopping=True,\n                 random_state=42)\nte, lb, ub = nn.estimate_ate(X, treatment, y)\nprint('Average Treatment Effect (Neural Network (MLP)): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n\nxl = BaseXRegressor(learner=XGBRegressor(random_state=42))\nte, lb, ub = xl.estimate_ate(X, e, treatment, y)\nprint('Average Treatment Effect (BaseXRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n\nrl = BaseRRegressor(learner=XGBRegressor(random_state=42))\nte, lb, ub =  rl.estimate_ate(X=X, p=e, treatment=treatment, y=y)\nprint('Average Treatment Effect (BaseRRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n```\n\nSee the [Meta-learner example notebook](https://github.com/uber/causalml/blob/master/examples/meta_learners_with_synthetic_data.ipynb) for details.\n\n\n## Interpretable Causal ML\n\nCausal ML provides methods to interpret the treatment effect models trained as follows:\n\n### Meta Learner Feature Importances\n\n```python\nfrom causalml.inference.meta import BaseSRegressor, BaseTRegressor, BaseXRegressor, BaseRRegressor\n\nslearner = BaseSRegressor(LGBMRegressor(), control_name='control')\nslearner.estimate_ate(X, w_multi, y)\nslearner_tau = slearner.fit_predict(X, w_multi, y)\n\nmodel_tau_feature = RandomForestRegressor()  # specify model for model_tau_feature\n\nslearner.get_importance(X=X, tau=slearner_tau, model_tau_feature=model_tau_feature,\n                        normalize=True, method='auto', features=feature_names)\n\n# Using the feature_importances_ method in the base learner (LGBMRegressor() in this example)\nslearner.plot_importance(X=X, tau=slearner_tau, normalize=True, method='auto')\n\n# Using eli5's PermutationImportance\nslearner.plot_importance(X=X, tau=slearner_tau, normalize=True, method='permutation')\n\n# Using SHAP\nshap_slearner = slearner.get_shap_values(X=X, tau=slearner_tau)\n\n# Plot shap values without specifying shap_dict\nslearner.plot_shap_values(X=X, tau=slearner_tau)\n\n# Plot shap values WITH specifying shap_dict\nslearner.plot_shap_values(shap_dict=shap_slearner)\n\n# interaction_idx set to 'auto' (searches for feature with greatest approximate interaction)\nslearner.plot_shap_dependence(treatment_group='treatment_A',\n                              feature_idx=1,\n                              X=X,\n                              tau=slearner_tau,\n                              interaction_idx='auto')\n```\n<div align=\"center\">\n  <img width=\"629px\" height=\"618px\" src=\"https://raw.githubusercontent.com/uber/causalml/master/docs/_static/img/shap_vis.png\">\n</div>\n\nSee the [feature interpretations example notebook](https://github.com/uber/causalml/blob/master/examples/feature_interpretations_example.ipynb) for details.\n\n### Uplift Tree Visualization\n\n```python\nfrom IPython.display import Image\nfrom causalml.inference.tree import UpliftTreeClassifier, UpliftRandomForestClassifier\nfrom causalml.inference.tree import uplift_tree_string, uplift_tree_plot\n\nuplift_model = UpliftTreeClassifier(max_depth=5, min_samples_leaf=200, min_samples_treatment=50,\n                                    n_reg=100, evaluationFunction='KL', control_name='control')\n\nuplift_model.fit(df[features].values,\n                 treatment=df['treatment_group_key'].values,\n                 y=df['conversion'].values)\n\ngraph = uplift_tree_plot(uplift_model.fitted_uplift_tree, features)\nImage(graph.create_png())\n```\n<div align=\"center\">\n  <img width=\"800px\" height=\"479px\" src=\"https://raw.githubusercontent.com/uber/causalml/master/docs/_static/img/uplift_tree_vis.png\">\n</div>\n\nSee the [Uplift Tree visualization example notebook](https://github.com/uber/causalml/blob/master/examples/uplift_tree_visualization.ipynb) for details.\n\n\n# Contributing\n\nWe welcome community contributors to the project. Before you start, please read our [code of conduct](https://github.com/uber/causalml/blob/master/CODE_OF_CONDUCT.md) and check out [contributing guidelines](./CONTRIBUTING.md) first.\n\n\n# Versioning\n\nWe document versions and changes in our [changelog](https://github.com/uber/causalml/blob/master/docs/changelog.rst).\n\n\n# License\n\nThis project is licensed under the Apache 2.0 License - see the [LICENSE](https://github.com/uber/causalml/blob/master/LICENSE) file for details.\n\n\n# References\n\n## Documentation\n* [Causal ML API documentation](https://causalml.readthedocs.io/en/latest/about.html)\n\n## Citation\nTo cite CausalML in publications, you can refer to the following sources:\n\nWhitepaper:\n[CausalML: Python Package for Causal Machine Learning](https://arxiv.org/abs/2002.11631)\n\nBibtex:\n> @misc{chen2020causalml,\n>    title={CausalML: Python Package for Causal Machine Learning},\n>    author={Huigang Chen and Totte Harinen and Jeong-Yoon Lee and Mike Yung and Zhenyu Zhao},\n>    year={2020},\n>    eprint={2002.11631},\n>    archivePrefix={arXiv},\n>    primaryClass={cs.CY}\n>}\n\n\n## Papers\n\n* Nicholas J Radcliffe and Patrick D Surry. Real-world uplift modelling with significance based uplift trees. White Paper TR-2011-1, Stochastic Solutions, 2011.\n* Yan Zhao, Xiao Fang, and David Simchi-Levi. Uplift modeling with multiple treatments and general response types. Proceedings of the 2017\nSIAM International Conference on Data Mining, SIAM, 2017.\n* S\u00f6ren R. K\u00fcnzel, Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. Metalearners for estimating heterogeneous treatment effects using machine learning.\nProceedings of the National Academy of Sciences, 2019.\n* Xinkun Nie and Stefan Wager. Quasi-Oracle Estimation of Heterogeneous Treatment Effects. Atlantic Causal Inference Conference, 2018.\n\n## Related projects\n\n* [uplift](https://cran.r-project.org/web/packages/uplift/index.html): uplift models in R\n* [grf](https://cran.r-project.org/web/packages/grf/index.html): generalized random forests that include heterogeneous treatment effect estimation in R\n* [rlearner](https://github.com/xnie/rlearner): A R package that implements R-Learner\n* [DoWhy](https://github.com/Microsoft/dowhy):  Causal inference in Python based on Judea Pearl's do-calculus\n* [EconML](https://github.com/microsoft/EconML): A Python package that implements heterogeneous treatment effect estimators from econometrics and machine learning methods\n"}, "cloudpickle": {"file_name": "cloudpipe/cloudpickle/README.md", "raw_text": "# cloudpickle\n\n[![Automated Tests](https://github.com/cloudpipe/cloudpickle/workflows/Automated%20Tests/badge.svg?branch=master&event=push)](https://github.com/cloudpipe/cloudpickle/actions)\n[![codecov.io](https://codecov.io/github/cloudpipe/cloudpickle/coverage.svg?branch=master)](https://codecov.io/github/cloudpipe/cloudpickle?branch=master)\n\n`cloudpickle` makes it possible to serialize Python constructs not supported\nby the default `pickle` module from the Python standard library.\n\n`cloudpickle` is especially useful for **cluster computing** where Python\ncode is shipped over the network to execute on remote hosts, possibly close\nto the data.\n\nAmong other things, `cloudpickle` supports pickling for **lambda functions**\nalong with **functions and classes defined interactively** in the\n`__main__` module (for instance in a script, a shell or a Jupyter notebook).\n\nCloudpickle can only be used to send objects between the **exact same version\nof Python**.\n\nUsing `cloudpickle` for **long-term object storage is not supported and\nstrongly discouraged.**\n\n**Security notice**: one should **only load pickle data from trusted sources** as\notherwise `pickle.load` can lead to arbitrary code execution resulting in a critical\nsecurity vulnerability.\n\n\nInstallation\n------------\n\nThe latest release of `cloudpickle` is available from\n[pypi](https://pypi.python.org/pypi/cloudpickle):\n\n    pip install cloudpickle\n\n\nExamples\n--------\n\nPickling a lambda expression:\n\n```python\n>>> import cloudpickle\n>>> squared = lambda x: x ** 2\n>>> pickled_lambda = cloudpickle.dumps(squared)\n\n>>> import pickle\n>>> new_squared = pickle.loads(pickled_lambda)\n>>> new_squared(2)\n4\n```\n\nPickling a function interactively defined in a Python shell session\n(in the `__main__` module):\n\n```python\n>>> CONSTANT = 42\n>>> def my_function(data):\n...    return data + CONSTANT\n...\n>>> pickled_function = cloudpickle.dumps(my_function)\n>>> pickle.loads(pickled_function)(43)\n85\n```\n\nRunning the tests\n-----------------\n\n- With `tox`, to test run the tests for all the supported versions of\n  Python and PyPy:\n\n      pip install tox\n      tox\n\n  or alternatively for a specific environment:\n\n      tox -e py37\n\n\n- With `py.test` to only run the tests for your current version of\n  Python:\n\n      pip install -r dev-requirements.txt\n      PYTHONPATH='.:tests' py.test\n\n\nNote about function Annotations\n-------------------------------\n\nNote that because of design issues `Python`'s `typing` module, `cloudpickle`\nsupports pickling type annotations of dynamic functions for `Python` 3.7 and\nlater.  On `Python` 3.4, 3.5 and 3.6, those type annotations will be dropped\nsilently during pickling (example below):\n\n```python\n>>> import typing\n>>> import cloudpickle\n>>> def f(x: typing.Union[list, int]):\n...     return x\n>>> f\n<function __main__.f(x:Union[list, int])>\n>>> cloudpickle.loads(cloudpickle.dumps(f))  # drops f's annotations\n<function __main__.f(x)>\n```\n\nHistory\n-------\n\n`cloudpickle` was initially developed by [picloud.com](http://web.archive.org/web/20140721022102/http://blog.picloud.com/2013/11/17/picloud-has-joined-dropbox/) and shipped as part of\nthe client SDK.\n\nA copy of `cloudpickle.py` was included as part of PySpark, the Python\ninterface to [Apache Spark](https://spark.apache.org/). Davies Liu, Josh\nRosen, Thom Neale and other Apache Spark developers improved it significantly,\nmost notably to add support for PyPy and Python 3.\n\nThe aim of the `cloudpickle` project is to make that work available to a wider\naudience outside of the Spark ecosystem and to make it easier to improve it\nfurther notably with the help of a dedicated non-regression test suite.\n"}, "ConfigSpace": {"file_name": "automl/ConfigSpace/README.md", "raw_text": "# ConfigSpace\n\nA simple python module to manage configuration spaces for algorithm configuration\nand hyperparameter optimization tasks. Includes various scripts to translate \nbetween different text formats for configuration space description. \nDistributed under BSD 3-clause, see LICENSE except all files in the directory\nConfigSpace.nx, which are copied from the networkx package and licensed\nunder a BSD license.\n\n[![Build Status](https://travis-ci.org/automl/ConfigSpace.svg?branch=master)](https://travis-ci.org/automl/ConfigSpace)\n\nThe documentation can be found at [https://automl.github.io/ConfigSpace/master/](https://automl.github.io/ConfigSpace/master/).\nFurther examples can be found in the [SMAC documentation](https://automl.github.io/SMAC3/stable/quickstart.html#using-smac-in-python-svm).\n\n## Citing the ConfigSpace\n\n```bibtex\n@article{\n    title   = {BOAH: A Tool Suite for Multi-Fidelity Bayesian Optimization & Analysis of Hyperparameters},\n    author  = {M. Lindauer and K. Eggensperger and M. Feurer and A. Biedenkapp and J. Marben and P. M\u00fcller and F. Hutter},\n    journal = {arXiv:1908.06756 {[cs.LG]}},\n    date    = {2019},\n}\n```\n"}, "cortex": {"file_name": "cortexlabs/cortex/README.md", "raw_text": "# Deploy machine learning models in production\n\nCortex is an open source platform for deploying machine learning models as production web services.\n\n<br>\n\n<!-- Delete on release branches -->\n<!-- CORTEX_VERSION_README_MINOR -->\n[install](https://cortex.dev/install) \u2022 [tutorial](https://cortex.dev/iris-classifier) \u2022 [docs](https://cortex.dev) \u2022 [examples](https://github.com/cortexlabs/cortex/tree/0.15/examples) \u2022 [we're hiring](https://angel.co/cortex-labs-inc/jobs) \u2022 [email us](mailto:hello@cortex.dev) \u2022 [chat with us](https://gitter.im/cortexlabs/cortex)<br><br>\n\n<!-- Set header Cache-Control=no-cache on the S3 object metadata (see https://help.github.com/en/articles/about-anonymized-image-urls) -->\n![Demo](https://d1zqebknpdh033.cloudfront.net/demo/gif/v0.13_2.gif)\n\n<br>\n\n## Key features\n\n* **Multi framework:** Cortex supports TensorFlow, PyTorch, scikit-learn, XGBoost, and more.\n* **Autoscaling:** Cortex automatically scales APIs to handle production workloads.\n* **CPU / GPU support:** Cortex can run inference on CPU or GPU infrastructure.\n* **Spot instances:** Cortex supports EC2 spot instances.\n* **Rolling updates:** Cortex updates deployed APIs without any downtime.\n* **Log streaming:** Cortex streams logs from deployed models to your CLI.\n* **Prediction monitoring:** Cortex monitors network metrics and tracks predictions.\n* **Minimal configuration:** Cortex deployments are defined in a single `cortex.yaml` file.\n\n<br>\n\n## Spinning up a cluster\n\nCortex is designed to be self-hosted on any AWS account. You can spin up a cluster with a single command:\n\n<!-- CORTEX_VERSION_README_MINOR -->\n```bash\n# install the CLI on your machine\n$ bash -c \"$(curl -sS https://raw.githubusercontent.com/cortexlabs/cortex/0.15/get-cli.sh)\"\n\n# provision infrastructure on AWS and spin up a cluster\n$ cortex cluster up\n\naws region: us-west-2\naws instance type: g4dn.xlarge\nspot instances: yes\nmin instances: 0\nmax instances: 5\n\naws resource                                cost per hour\n1 eks cluster                               $0.10\n0 - 5 g4dn.xlarge instances for your apis   $0.1578 - $0.526 each (varies based on spot price)\n0 - 5 20gb ebs volumes for your apis        $0.003 each\n1 t3.medium instance for the operator       $0.0416\n1 20gb ebs volume for the operator          $0.003\n2 elastic load balancers                    $0.025 each\n\nyour cluster will cost $0.19 - $2.84 per hour based on the cluster size and spot instance availability\n\n\uffee spinning up your cluster ...\n\nyour cluster is ready!\n```\n\n<br>\n\n## Deploying a model\n\n### Implement your predictor\n\n```python\n# predictor.py\n\nclass PythonPredictor:\n    def __init__(self, config):\n        self.model = download_model()\n\n    def predict(self, payload):\n        return self.model.predict(payload[\"text\"])\n```\n\n### Configure your deployment\n\n```yaml\n# cortex.yaml\n\n- name: sentiment-classifier\n  predictor:\n    type: python\n    path: predictor.py\n  tracker:\n    model_type: classification\n  compute:\n    gpu: 1\n    mem: 4G\n```\n\n### Deploy to AWS\n\n```bash\n$ cortex deploy\n\ncreating sentiment-classifier\n```\n\n### Serve real-time predictions\n\n```bash\n$ curl http://***.amazonaws.com/sentiment-classifier \\\n    -X POST -H \"Content-Type: application/json\" \\\n    -d '{\"text\": \"the movie was amazing!\"}'\n\npositive\n```\n\n### Monitor your deployment\n\n```bash\n$ cortex get sentiment-classifier --watch\n\nstatus   up-to-date   requested   last update   avg request   2XX\nlive     1            1           8s            24ms          12\n\nclass     count\npositive  8\nnegative  4\n```\n\n<br>\n\n## What is Cortex similar to?\n\nCortex is an open source alternative to serving models with SageMaker or building your own model deployment platform on top of AWS services like Elastic Kubernetes Service (EKS), Elastic Container Service (ECS), Lambda, Fargate, and Elastic Compute Cloud (EC2) and open source projects like Docker, Kubernetes, and TensorFlow Serving.\n\n<br>\n\n## How does Cortex work?\n\nThe CLI sends configuration and code to the cluster every time you run `cortex deploy`. Each model is loaded into a Docker container, along with any Python packages and request handling code. The model is exposed as a web service using Elastic Load Balancing (ELB), TensorFlow Serving, and ONNX Runtime. The containers are orchestrated on Elastic Kubernetes Service (EKS) while logs and metrics are streamed to CloudWatch.\n\n<br>\n\n## Examples of Cortex deployments\n\n<!-- CORTEX_VERSION_README_MINOR x5 -->\n* [Sentiment analysis](https://github.com/cortexlabs/cortex/tree/0.15/examples/tensorflow/sentiment-analyzer): deploy a BERT model for sentiment analysis.\n* [Image classification](https://github.com/cortexlabs/cortex/tree/0.15/examples/tensorflow/image-classifier): deploy an Inception model to classify images.\n* [Search completion](https://github.com/cortexlabs/cortex/tree/0.15/examples/pytorch/search-completer): deploy Facebook's RoBERTa model to complete search terms.\n* [Text generation](https://github.com/cortexlabs/cortex/tree/0.15/examples/pytorch/text-generator): deploy Hugging Face's DistilGPT2 model to generate text.\n* [Iris classification](https://github.com/cortexlabs/cortex/tree/0.15/examples/sklearn/iris-classifier): deploy a scikit-learn model to classify iris flowers.\n"}, "cudf": {"file_name": "rapidsai/cudf/README.md", "raw_text": "# <div align=\"left\"><img src=\"img/rapids_logo.png\" width=\"90px\"/>&nbsp;cuDF - GPU DataFrames</div>\n\n[![Build Status](https://gpuci.gpuopenanalytics.com/job/rapidsai/job/gpuci/job/cudf/job/branches/job/cudf-branch-pipeline/badge/icon)](https://gpuci.gpuopenanalytics.com/job/rapidsai/job/gpuci/job/cudf/job/branches/job/cudf-branch-pipeline/)\n\n**NOTE:** For the latest stable [README.md](https://github.com/rapidsai/cudf/blob/master/README.md) ensure you are on the `master` branch.\n\nBuilt based on the [Apache Arrow](http://arrow.apache.org/) columnar memory format, cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.\n\ncuDF provides a pandas-like API that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming.\n\nFor example, the following snippet downloads a CSV, then uses the GPU to parse it into rows and columns and run calculations:\n```python\nimport cudf, io, requests\nfrom io import StringIO\n\nurl = \"https://github.com/plotly/datasets/raw/master/tips.csv\"\ncontent = requests.get(url).content.decode('utf-8')\n\ntips_df = cudf.read_csv(StringIO(content))\ntips_df['tip_percentage'] = tips_df['tip'] / tips_df['total_bill'] * 100\n\n# display average tip by dining party size\nprint(tips_df.groupby('size').tip_percentage.mean())\n```\n\nOutput:\n```\nsize\n1    21.729201548727808\n2    16.571919173482897\n3    15.215685473711837\n4    14.594900639351332\n5    14.149548965142023\n6    15.622920072028379\nName: tip_percentage, dtype: float64\n```\n\nFor additional examples, browse our complete [API documentation](https://docs.rapids.ai/api/cudf/stable/), or check out our more detailed [notebooks](https://github.com/rapidsai/notebooks-contrib).\n\n## Quick Start\n\nPlease see the [Demo Docker Repository](https://hub.docker.com/r/rapidsai/rapidsai/), choosing a tag based on the NVIDIA CUDA version you\u2019re running. This provides a ready to run Docker container with example notebooks and data, showcasing how you can utilize cuDF.\n\n## Installation\n\n### Conda\n\ncuDF can be installed with conda ([miniconda](https://conda.io/miniconda.html), or the full [Anaconda distribution](https://www.anaconda.com/download)) from the `rapidsai` channel:\n\nFor `cudf version == 0.13` :\n```bash\n# for CUDA 9.2\nconda install -c rapidsai -c nvidia -c numba -c conda-forge \\\n    cudf=0.13 python=3.6 cudatoolkit=9.2\n\n# or, for CUDA 10.0\nconda install -c rapidsai -c nvidia -c numba -c conda-forge \\\n    cudf=0.13 python=3.6 cudatoolkit=10.0\n\n# or, for CUDA 10.1\nconda install -c rapidsai -c nvidia -c numba -c conda-forge \\\n    cudf=0.13 python=3.6 cudatoolkit=10.1\n```\n\nFor the nightly version of `cudf` :\n```bash\n# for CUDA 9.2\nconda install -c rapidsai-nightly -c nvidia -c numba -c conda-forge \\\n    cudf python=3.6 cudatoolkit=9.2\n\n# or, for CUDA 10.0\nconda install -c rapidsai-nightly -c nvidia -c numba -c conda-forge \\\n    cudf python=3.6 cudatoolkit=10.0\n```\n\nNote: cuDF is supported only on Linux, and with Python versions 3.6 or 3.7.\n\nSee the [Get RAPIDS version picker](https://rapids.ai/start.html) for more OS and version info. \n\n## Build/Install from Source\nSee build [instructions](CONTRIBUTING.md#setting-up-your-build-environment).\n\n## Contributing\n\nPlease see our [guide for contributing to cuDF](CONTRIBUTING.md).\n\n## Contact\n\nFind out more details on the [RAPIDS site](https://rapids.ai/community.html)\n\n## <div align=\"left\"><img src=\"img/rapids_logo.png\" width=\"265px\"/></div> Open GPU Data Science\n\nThe RAPIDS suite of open source software libraries aim to enable execution of end-to-end data science and analytics pipelines entirely on GPUs. It relies on NVIDIA\u00ae CUDA\u00ae primitives for low-level compute optimization, but exposing that GPU parallelism and high-bandwidth memory speed through user-friendly Python interfaces.\n\n<p align=\"center\"><img src=\"img/rapids_arrow.png\" width=\"80%\"/></p>\n\n### Apache Arrow on GPU\n\nThe GPU version of [Apache Arrow](https://arrow.apache.org/) is a common API that enables efficient interchange of tabular data between processes running on the GPU. End-to-end computation on the GPU avoids unnecessary copying and converting of data off the GPU, reducing compute time and cost for high-performance analytics common in artificial intelligence workloads. As the name implies, cuDF uses the Apache Arrow columnar data format on the GPU. Currently, a subset of the features in Apache Arrow are supported.\n"}, "cuml": {"file_name": "rapidsai/cuml/README.md", "raw_text": "# <div align=\"left\"><img src=\"img/rapids_logo.png\" width=\"90px\"/>&nbsp;cuML - GPU Machine Learning Algorithms</div>\n\n[![Build Status](https://gpuci.gpuopenanalytics.com/job/rapidsai/job/gpuci/job/cuml/job/branches/job/cuml-branch-pipeline/badge/icon)](https://gpuci.gpuopenanalytics.com/job/rapidsai/job/gpuci/job/cuml/job/branches/job/cuml-branch-pipeline/)\n\ncuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions that share compatible APIs with other [RAPIDS](https://rapids.ai/) projects.\n\ncuML enables data scientists, researchers, and software engineers to run\ntraditional tabular ML tasks on GPUs without going into the details of CUDA\nprogramming. In most cases, cuML's Python API matches the API from\n[scikit-learn](https://scikit-learn.org).\n\n\nFor large datasets, these GPU-based implementations can complete 10-50x faster\nthan their CPU equivalents. For details on performance, see the [cuML Benchmarks\nNotebook](https://github.com/rapidsai/notebooks-contrib/blob/master/intermediate_notebooks/benchmarks/cuml_benchmarks.ipynb).\n\nAs an example, the following Python snippet loads input and computes DBSCAN clusters, all on GPU:\n```python\nimport cudf\nfrom cuml.cluster import DBSCAN\n\n# Create and populate a GPU DataFrame\ngdf_float = cudf.DataFrame()\ngdf_float['0'] = [1.0, 2.0, 5.0]\ngdf_float['1'] = [4.0, 2.0, 1.0]\ngdf_float['2'] = [4.0, 2.0, 1.0]\n\n# Setup and fit clusters\ndbscan_float = DBSCAN(eps=1.0, min_samples=1)\ndbscan_float.fit(gdf_float)\n\nprint(dbscan_float.labels_)\n```\n\nOutput:\n```\n0    0\n1    1\n2    2\ndtype: int32\n```\n\ncuML also features multi-GPU and multi-node-multi-GPU operation, using [Dask](https://www.dask.org), for a\ngrowing list of algorithms. The following Python snippet reads input from a CSV file and performs\na NearestNeighbors query across a cluster of Dask workers, using multiple GPUs on a single node:\n```python\n# Create a Dask CUDA cluster w/ one worker per device\nfrom dask_cuda import LocalCUDACluster\ncluster = LocalCUDACluster()\n\n# Read CSV file in parallel across workers\nimport dask_cudf\ndf = dask_cudf.read_csv(\"/path/to/csv\")\n\n# Fit a NearestNeighbors model and query it\nfrom cuml.dask.neighbors import NearestNeighbors\nnn = NearestNeighbors(n_neighbors = 10)\nnn.fit(df)\nneighbors = nn.kneighbors(df)\n```\n\n\nFor additional examples, browse our complete [API\ndocumentation](https://docs.rapids.ai/api/cuml/stable/), or check out our\nintroductory [walkthrough\nnotebooks](https://github.com/rapidsai/notebooks/tree/master/cuml). Finally, you\ncan find complete end-to-end examples in the [notebooks-contrib\nrepo](https://github.com/rapidsai/notebooks-contrib).\n\n\n### Supported Algorithms\n| Category | Algorithm | Notes |\n| --- | --- | --- |\n| **Clustering** |  Density-Based Spatial Clustering of Applications with Noise (DBSCAN) | |\n|  | K-Means | Multi-node multi-GPU via Dask |\n| **Dimensionality Reduction** | Principal Components Analysis (PCA) | Multi-node multi-GPU via Dask|\n| | Truncated Singular Value Decomposition (tSVD) | Multi-node multi-GPU via Dask |\n| | Uniform Manifold Approximation and Projection (UMAP) | |\n| | Random Projection | |\n| | t-Distributed Stochastic Neighbor Embedding (TSNE) | |\n| **Linear Models for Regression or Classification** | Linear Regression (OLS) | Multi-node multi-GPU via Dask |\n| | Linear Regression with Lasso or Ridge Regularization | Multi-node multi-GPU via Dask |\n| | ElasticNet Regression | |\n| | Logistic Regression | |\n| | Stochastic Gradient Descent (SGD), Coordinate Descent (CD), and Quasi-Newton (QN) (including L-BFGS and OWL-QN) solvers for linear models  | |\n| **Nonlinear Models for Regression or Classification** | Random Forest (RF) Classification | Experimental multi-node multi-GPU via Dask |\n| | Random Forest (RF) Regression | Experimental multi-node multi-GPU via Dask |\n| | Inference for decision tree-based models | Forest Inference Library (FIL) |\n|  | K-Nearest Neighbors (KNN) | Multi-node multi-GPU via Dask, uses [Faiss](https://github.com/facebookresearch/faiss) for Nearest Neighbors Query. |\n|  | K-Nearest Neighbors (KNN) Classification | |\n|  | K-Nearest Neighbors (KNN) Regression | |\n|  | Support Vector Machine Classifier (SVC) | |\n|  | Epsilon-Support Vector Regression (SVR) | |\n| **Time Series** | Linear Kalman Filter | |\n|  | Holt-Winters Exponential Smoothing | |\n|  | Auto-regressive Integrated Moving Average (ARIMA) | Supports seasonality (SARIMA) |\n---\n\n## Installation\n\nSee [the RAPIDS Release\nSelector](https://rapids.ai/start.html#rapids-release-selector) for the command\nline to install either nightly or official release cuML packages via Conda or\nDocker.\n\n## Build/Install from Source\nSee the build [guide](BUILD.md).\n\n## Contributing\n\nPlease see our [guide for contributing to cuML](CONTRIBUTING.md).\n\n## References\n\nFor additional details on the technologies behind cuML, as well as a broader overview of the Python Machine Learning landscape, see [_Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence_ (2020)](https://arxiv.org/abs/2002.04803) by Sebastian Raschka, Joshua Patterson, and Corey Nolet.\n\nPlease consider citing this when using cuML in a project. You can use the citation BibTeX:\n\n> @article{raschka2020machine,\n>   title={Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence},\n>   author={Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},\n>   journal={arXiv preprint arXiv:2002.04803},\n>   year={2020}\n> }\n\n## Contact\n\nFind out more details on the [RAPIDS site](https://rapids.ai/community.html)\n\n## <div align=\"left\"><img src=\"img/rapids_logo.png\" width=\"265px\"/></div> Open GPU Data Science\n\nThe RAPIDS suite of open source software libraries aim to enable execution of end-to-end data science and analytics pipelines entirely on GPUs. It relies on NVIDIA\u00ae CUDA\u00ae primitives for low-level compute optimization, but exposing that GPU parallelism and high-bandwidth memory speed through user-friendly Python interfaces.\n\n<p align=\"center\"><img src=\"img/rapids_arrow.png\" width=\"80%\"/></p>\n"}, "cupy": {"file_name": "cupy/cupy/README.md", "raw_text": "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/cupy/cupy/master/docs/image/cupy_logo_1000px.png\" width=\"400\"/></div>\n\n# CuPy : NumPy-like API accelerated with CUDA\n\n[![pypi](https://img.shields.io/pypi/v/cupy.svg)](https://pypi.python.org/pypi/cupy)\n[![GitHub license](https://img.shields.io/github/license/cupy/cupy.svg)](https://github.com/cupy/cupy)\n[![travis](https://img.shields.io/travis/cupy/cupy.svg)](https://travis-ci.org/cupy/cupy)\n[![coveralls](https://img.shields.io/coveralls/cupy/cupy.svg)](https://coveralls.io/github/cupy/cupy)\n[![Read the Docs](https://readthedocs.org/projects/cupy/badge/?version=stable)](https://docs-cupy.chainer.org/en/stable/)\n\n[**Website**](https://cupy.chainer.org/)\n| [**Docs**](https://docs-cupy.chainer.org/en/stable/)\n| [**Install Guide**](https://docs-cupy.chainer.org/en/stable/install.html)\n| [**Tutorial**](https://docs-cupy.chainer.org/en/stable/tutorial/)\n| **Examples** ([Official](https://github.com/cupy/cupy/tree/master/examples))\n| **Forum** ([en](https://groups.google.com/forum/#!forum/cupy), [ja](https://groups.google.com/forum/#!forum/cupy-ja))\n\n*CuPy* is an implementation of NumPy-compatible multi-dimensional array on CUDA.\nCuPy consists of the core multi-dimensional array class, `cupy.ndarray`, and many functions on it.\nIt supports a subset of `numpy.ndarray` interface.\n\n## Installation\n\nFor detailed instructions on installing CuPy, see [the installation guide](https://docs-cupy.chainer.org/en/stable/install.html).\n\nYou can install CuPy using `pip`:\n\n```sh\n(Binary Package for CUDA 8.0)\n$ pip install cupy-cuda80\n\n(Binary Package for CUDA 9.0)\n$ pip install cupy-cuda90\n\n(Binary Package for CUDA 9.1)\n$ pip install cupy-cuda91\n\n(Binary Package for CUDA 9.2)\n$ pip install cupy-cuda92\n\n(Binary Package for CUDA 10.0)\n$ pip install cupy-cuda100\n\n(Binary Package for CUDA 10.1)\n$ pip install cupy-cuda101\n\n(Binary Package for CUDA 10.2)\n$ pip install cupy-cuda102\n\n(Source Package)\n$ pip install cupy\n```\n\nThe latest version of cuDNN and NCCL libraries are included in binary packages (wheels).\nFor the source package, you will need to install cuDNN/NCCL before installing CuPy, if you want to use it.\n\n## Run with Docker\n\nWe provide the official Docker image.\nUse [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) command to run CuPy image with GPU.\nYou can login to the environment with bash, and run the Python interpreter.\n\n```\n$ nvidia-docker run -it cupy/cupy /bin/bash\n```\n\n## Development\n\nPlease see [the contribution guide](https://docs-cupy.chainer.org/en/stable/contribution.html).\n\n## More information\n\n- [Release notes](https://github.com/cupy/cupy/releases)\n- [Projects using CuPy](https://github.com/cupy/cupy/wiki/Projects-using-CuPy)\n\n## License\n\nMIT License (see `LICENSE` file).\n\nCuPy is designed based on NumPy's API and SciPy's API (see `docs/LICENSE_THIRD_PARTY` file).\n\n## Reference\n\nRyosuke Okuta, Yuya Unno, Daisuke Nishino, Shohei Hido and Crissman Loomis.\nCuPy: A NumPy-Compatible Library for NVIDIA GPU Calculations.\n*Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)*, (2017).\n[URL](http://learningsys.org/nips17/assets/papers/paper_16.pdf)\n\n```\n@inproceedings{cupy_learningsys2017,\n  author       = \"Okuta, Ryosuke and Unno, Yuya and Nishino, Daisuke and Hido, Shohei and Loomis, Crissman\",\n  title        = \"CuPy: A NumPy-Compatible Library for NVIDIA GPU Calculations\",\n  booktitle    = \"Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)\",\n  year         = \"2017\",\n  url          = \"http://learningsys.org/nips17/assets/papers/paper_16.pdf\"\n}\n```\n"}, "Cython": {"file_name": "cython/cython/README.rst", "raw_text": "Welcome to Cython!\n==================\n\nCython is a language that makes writing C extensions for\nPython as easy as Python itself.  Cython is based on\nPyrex, but supports more cutting edge functionality and\noptimizations.\n\nThe Cython language is very close to the Python language, but Cython\nadditionally supports calling C functions and declaring C types on variables\nand class attributes.  This allows the compiler to generate very efficient C\ncode from Cython code.\n\nThis makes Cython the ideal language for wrapping external C libraries, and\nfor fast C modules that speed up the execution of Python code.\n\n* Official website: https://cython.org/\n* Documentation: http://docs.cython.org/\n* Github repository: https://github.com/cython/cython\n* Wiki: https://github.com/cython/cython/wiki\n\nYou can **support the Cython project** via\n`Github Sponsors <https://github.com/users/scoder/sponsorship>`_ or\n`Tidelift <https://tidelift.com/subscription/pkg/pypi-cython>`_.\n\n\nInstallation:\n-------------\n\nIf you already have a C compiler, just do::\n\n   pip install Cython\n\notherwise, see `the installation page <http://docs.cython.org/en/latest/src/quickstart/install.html>`_.\n\n\nLicense:\n--------\n\nThe original Pyrex program was licensed \"free of restrictions\" (see below).\nCython itself is licensed under the permissive **Apache License**.\n\nSee `LICENSE.txt <https://github.com/cython/cython/blob/master/LICENSE.txt>`_.\n\n\nContributing:\n-------------\n\nWant to contribute to the Cython project?\nHere is some `help to get you started <https://github.com/cython/cython/blob/master/docs/CONTRIBUTING.rst>`_.\n\nWe are currently building the next great Cython edition:\n`Cython 3.0 <https://github.com/cython/cython/milestone/58>`_.\nYou can help us make the life of Python 3.x users easier.\n\n\nGet the full source history:\n----------------------------\n\nNote that Cython used to ship the full version control repository in its source\ndistribution, but no longer does so due to space constraints.  To get the\nfull source history from a downloaded source archive, make sure you have git\ninstalled, then step into the base directory of the Cython source distribution\nand type::\n\n    make repo\n\n\nThe following is from Pyrex:\n------------------------------------------------------\nThis is a development version of Pyrex, a language\nfor writing Python extension modules.\n\nFor more info, see:\n\n* Doc/About.html for a description of the language\n* INSTALL.txt    for installation instructions\n* USAGE.txt      for usage instructions\n* Demos          for usage examples\n\nComments, suggestions, bug reports, etc. are\nwelcome!\n\nCopyright stuff: Pyrex is free of restrictions. You\nmay use, redistribute, modify and distribute modified\nversions.\n\nThe latest version of Pyrex can be found `here <http://www.cosc.canterbury.ac.nz/~greg/python/Pyrex/>`_.\n\n| Greg Ewing, Computer Science Dept\n| University of Canterbury\n| Christchurch, New Zealand\n\n A citizen of NewZealandCorp, a wholly-owned subsidiary of USA Inc.\n"}, "dagster": {"file_name": "dagster-io/dagster/README.md", "raw_text": "<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/609349/57987382-7e294500-7a35-11e9-9c6a-f73e0f1d3a1c.png\" />\n<br /><br />\n<a href=\"https://badge.fury.io/py/dagster\"><img src=\"https://badge.fury.io/py/dagster.svg\"></>\n<a href=\"https://coveralls.io/github/dagster-io/dagster?branch=master\"><img src=\"https://coveralls.io/repos/github/dagster-io/dagster/badge.svg?branch=master\"></a>\n<a href=\"https://buildkite.com/dagster/dagster\"><img src=\"https://badge.buildkite.com/888545beab829e41e5d7303db15525a2bc3b0f0e33a72759ac.svg?branch=master\"></a>\n<a href=\"https://docs.dagster.io/\"></a>\n</p>\n\n# Dagster\n\nDagster is a system for building modern data applications.\n\n- **Elegant programming model:** Dagster is a set of abstractions for building self-describing, testable, and reliable data applications. It embraces the principles of functional data programming; gradual, optional typing; and testability as a first-class value.\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/4531914/79161353-366b8480-7d90-11ea-83ce-c8a9522359d5.gif\" />\n</p>\n\n- **Flexible & incremental:** Dagster integrates with your existing tools and infrastructure, and can invoke any computation\u2013whether it be Spark, Python, a Jupyter notebook, or SQL. It is also designed to deploy to any workflow engine, such as Airflow.\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/4531914/79161365-3d929280-7d90-11ea-9216-c88cce41d3f1.gif\" />\n</p>\n\n- **Beautiful tools:** Dagster's development environment, dagit\u2013designed for data engineers, machine learning engineers, data scientists\u2013enables astoundingly productive local development.\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/4531914/79161362-3bc8cf00-7d90-11ea-8974-17edbde3dc0d.gif\" />\n</p>\n\n## Getting Started\n\n### Installation\n\n<p align=\"center\">\n<code>pip install dagster dagit</code>\n</p>\n\nThis installs two modules:\n\n- **dagster** | The core programming model and abstraction stack; stateless, single-node,\n  single-process and multi-process execution engines; and a CLI tool for driving those engines.\n- **dagit** | A UI and rich development environment for Dagster, including a DAG browser, a type-aware config editor, and a streaming execution interface.\n  <br/>\n\n### Hello dagster \ud83d\udc4b\n\n**`hello_dagster.py`**\n\n```python\nfrom dagster import execute_pipeline, pipeline, solid\n\n\n@solid\ndef get_name(_):\n    return 'dagster'\n\n\n@solid\ndef hello(context, name: str):\n    context.log.info('Hello, {name}!'.format(name=name))\n\n\n@pipeline\ndef hello_pipeline():\n    hello(get_name())\n```\n\nLet's execute our first pipeline via any of three different mechanisms:\n\n- From arbitrary Python scripts, use dagster\u2019s Python API\n\n  ```python\n  if __name__ == \"__main__\":\n      execute_pipeline(hello_pipeline)  # Hello, dagster!\n  ```\n\n- From the command line, use the dagster CLI\n\n  ```bash\n  $ dagster pipeline execute -f hello_dagster.py -n hello_pipeline\n  ```\n\n- From a rich graphical interface, use the dagit GUI tool\n  ```bash\n  $ dagit -f hello_dagster.py -n hello_pipeline\n  ```\n  Navigate to http://localhost:3000 and start your journey with Dagit.\n\n## Learn\n\nNext, jump right into our [tutorial](https://docs.dagster.io/latest/tutorial/), or read our [complete documentation](https://docs.dagster.io). If you're actively using Dagster or have questions on getting started, we'd love to hear from you:\n\n<br />\n<p align=\"center\">\n<a href=\"https://join.slack.com/t/dagster/shared_invite/enQtNjEyNjkzNTA2OTkzLTI0MzdlNjU0ODVhZjQyOTMyMGM1ZDUwZDQ1YjJmYjI3YzExZGViMDI1ZDlkNTY5OThmYWVlOWM1MWVjN2I3NjU\"><img src=\"https://user-images.githubusercontent.com/609349/63558739-f60a7e00-c502-11e9-8434-c8a95b03ce62.png\" width=160px; /></a>\n</p>\n\n## Contributing\n\nFor details on contributing or running the project for development, check out our [contributing guide](https://docs.dagster.io/latest/community/contributing/).\n<br />\n\n## Integrations\n\nDagster works with the tools and systems that you're already using with your data, including:\n\n<table>\n\t<thead>\n\t\t<tr style=\"background-color: #ddd\" align=\"center\">\n\t\t\t<td colspan=2><b>Integration</b></td>\n\t\t\t<td><b>Dagster Library</b></td>\n\t\t</tr>\n\t</thead>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td align=\"center\" style=\"border-right: 0px\"><img style=\"vertical-align:middle\"  src=\"https://user-images.githubusercontent.com/609349/57987547-a7e36b80-7a37-11e9-95ae-4c4de2618e87.png\"></td>\n\t\t\t<td style=\"border-left: 0px\"> <b>Apache Airflow</b></td>\n\t\t\t<td><a href=\"https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-airflow\" />dagster-airflow</a><br />Allows Dagster pipelines to be scheduled and executed, either containerized or uncontainerized, as <a href=\"https://github.com/apache/airflow\">Apache Airflow DAGs</a>.</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td align=\"center\" style=\"border-right: 0px\"><img style=\"vertical-align:middle\"  src=\"https://user-images.githubusercontent.com/609349/57987976-5ccc5700-7a3d-11e9-9fa5-1a51299b1ccb.png\"></td>\n\t\t\t<td style=\"border-left: 0px\"> <b>Apache Spark</b></td>\n\t\t\t<td><a href=\"https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-spark\" />dagster-spark</a> &middot;\u00a0<a href=\"https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-pyspark\" />dagster-pyspark</a>\n\t\t\t<br />Libraries for interacting with Apache Spark and Pyspark.\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td align=\"center\" style=\"border-right: 0px\"><img style=\"vertical-align:middle\"  src=\"https://user-images.githubusercontent.com/609349/58348728-48f66b80-7e16-11e9-9e9f-1a0fea9a49b4.png\"></td>\n\t\t\t<td style=\"border-left: 0px\"> <b>Dask</b></td>\n\t\t\t<td><a href=\"https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-dask\" />dagster-dask</a>\n\t\t\t<br />Provides a Dagster integration with Dask / Dask.Distributed.\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td align=\"center\" style=\"border-right: 0px\"><img style=\"vertical-align:middle\" src=\"https://user-images.githubusercontent.com/609349/58349731-f36f8e00-7e18-11e9-8a2e-86e086caab66.png\"></td>\n\t\t\t<td style=\"border-left: 0px\"> <b>Datadog</b></td>\n\t\t\t<td><a href=\"https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-datadog\" />dagster-datadog</a>\n\t\t\t<br />Provides a Dagster resource for publishing metrics to Datadog.\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td align=\"center\" style=\"border-right: 0px\"><img style=\"vertical-align:middle\" src=\"https://user-images.githubusercontent.com/609349/57987809-bf245800-7a3b-11e9-8905-494ed99d0852.png\" />\n\t\t\t&nbsp;/&nbsp; <img style=\"vertical-align:middle\" src=\"https://user-images.githubusercontent.com/609349/57987827-fa268b80-7a3b-11e9-8a18-b675d76c19aa.png\">\n\t\t\t</td>\n\t\t\t<td style=\"border-left: 0px\"> <b>Jupyter / Papermill</b></td>\n\t\t\t<td><a href=\"https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagstermill\" />dagstermill</a><br />Built on the <a href=\"https://github.com/nteract/papermill\">papermill library</a>, dagstermill is meant for integrating productionized Jupyter notebooks into dagster pipelines.</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td align=\"center\" style=\"border-right: 0px\"><img style=\"vertical-align:middle\"  src=\"https://user-images.githubusercontent.com/609349/57988016-f431aa00-7a3d-11e9-8cb6-1309d4246b27.png\"></td>\n\t\t\t<td style=\"border-left: 0px\"> <b>PagerDuty</b></td>\n\t\t\t<td><a href=\"https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-pagerduty\" />dagster-pagerduty</a>\n\t\t\t<br />A library for creating PagerDuty alerts from Dagster workflows.\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td align=\"center\" style=\"border-right: 0px\"><img style=\"vertical-align:middle\" src=\"https://user-images.githubusercontent.com/609349/58349397-fcac2b00-7e17-11e9-900c-9ab8cf7cb64a.png\"></td>\n\t\t\t<td style=\"border-left: 0px\"> <b>Snowflake</b></td>\n\t\t\t<td><a href=\"https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-snowflake\" />dagster-snowflake</a>\n\t\t\t<br />A library for interacting with the Snowflake Data Warehouse.\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr style=\"background-color: #ddd\">\n\t\t\t<td colspan=2 align=\"center\"><b>Cloud Providers</b></td>\n\t\t\t<td><b></b></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td align=\"center\" style=\"border-right: 0px\"><img style=\"vertical-align:middle\" src=\"https://user-images.githubusercontent.com/609349/57987557-c2b5e000-7a37-11e9-9310-c274481a4682.png\"> </td>\n\t\t\t<td style=\"border-left: 0px\"><b>AWS</b></td>\n\t\t\t<td><a href=\"https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-aws\" />dagster-aws</a>\n\t\t\t<br />A library for interacting with Amazon Web Services. Provides integrations with S3, EMR, and (coming soon!) Redshift.\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td align=\"center\" style=\"border-right: 0px\"><img style=\"vertical-align:middle\" src=\"https://user-images.githubusercontent.com/609349/57987566-f98bf600-7a37-11e9-81fa-b8ca1ea6cc1e.png\"> </td>\n\t\t\t<td style=\"border-left: 0px\"><b>GCP</b></td>\n\t\t\t<td><a href=\"https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-gcp\" />dagster-gcp</a>\n\t\t\t<br />A library for interacting with Google Cloud Platform. Provides integrations with BigQuery and Cloud Dataproc.\n\t\t\t</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\nThis list is growing as we are actively building more integrations, and we welcome contributions!\n\n## Example Projects\n\nSeveral example projects are provided under the examples folder demonstrating how to use Dagster, including:\n\n1. [**examples/airline-demo**](https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/airline_demo): A substantial demo project illustrating how these tools can be used together to manage a realistic data pipeline.\n2. [**examples/event-pipeline-demo**](https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/event_pipeline_demo): An example illustrating a typical web event processing pipeline with S3, Scala Spark, and Snowflake.\n"}, "dalex": {"file_name": "ModelOriented/DALEX/README.md", "raw_text": "# moDel Agnostic Language for Exploration and eXplanation <img src=\"man/figures/logo.png\" align=\"right\" width=\"150\"/>\n\n[![Build Status](https://api.travis-ci.org/ModelOriented/DALEX.png)](https://travis-ci.org/ModelOriented/DALEX)\n[![Coverage\nStatus](https://img.shields.io/codecov/c/github/ModelOriented/DALEX/master.svg)](https://codecov.io/github/ModelOriented/DALEX?branch=master)\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/DALEX)](https://cran.r-project.org/package=DALEX)\n[![Total Downloads](http://cranlogs.r-pkg.org/badges/grand-total/DALEX?color=orange)](http://cranlogs.r-pkg.org/badges/grand-total/DALEX)\n[![DrWhy-eXtrAI](https://img.shields.io/badge/DrWhy-BackBone-373589)](http://drwhy.ai/#BackBone)\n\n## Overview\n\nUnverified black box model is the path to the failure. Opaqueness leads to distrust. Distrust leads to ignoration. Ignoration leads to rejection.\n\nThe `DALEX` package xrays any model and helps to explore and explain its behaviour, helps to understand how complex models are working. The main function `explain()` creates a wrapper around a predictive model. Wrapped models may then be explored and compared with a collection of local and global explainers. Recent developents from the area of Interpretable Machine Learning/eXplainable Artificial Intelligence.\n\nThe philosophy behind `DALEX` explanations is described in the [Explanatory Model Analysis](https://pbiecek.github.io/ema/) e-book. The `DALEX` package is a part of [DrWhy.AI](http://DrWhy.AI) universe.\n\nIf you work with `scikitlearn`, `keras`, `H2O`, `mljar` or `mlr`, you may be interested in the [DALEXtra](https://github.com/ModelOriented/DALEXtra) package. It is an extension pack for `DALEX` with easy to use connectors to models created in these libraries.\n\n<p align=\"center\">\n<a href=\"https://pbiecek.github.io/ema/introduction.html#bookstructure\"><img src=\"misc/DALEXpiramide.png\" width=\"800\"/></a>\n</p>\n\n\n## Installation\n\nThe easiest way to get the **R** version of DALEX is to install it from [CRAN](https://cran.r-project.org/package=DALEX)\n\n```\ninstall.packages(\"DALEX\")\n```\n\nThe **Python** version of dalex is available on [pip](https://pypi.org/project/dalex/)\n\n```\npip install dalex\n```\n\n## Learn more\n\nMachine Learning models are widely used and have various applications in classification or regression tasks. Due to increasing computational power, availability of new data sources and new methods, ML models are more and more complex. Models created with techniques like boosting, bagging of neural networks are true black boxes. It is hard to trace the link between input variables and model outcomes. They are use because of high performance, but lack of interpretability is one of their weakest sides.\n\nIn many applications we need to know, understand or prove how input variables are used in the model and what impact do they have on final model prediction. `DALEX` is a set of tools that help to understand how complex models are working.\n\n## Cheatsheet\n\n<p align=\"center\">\n<a href=\"https://github.com/ModelOriented/DALEX/raw/master/misc/cheatsheet_local_explainers.png\"><img src=\"https://github.com/ModelOriented/DALEX/raw/master/misc/cheatsheet_local_explainers.png\" width=\"500\"/></a>\n</p>\n\n\n### DALEX show-cases\n\n* [XAI in the jungle of competing frameworks for machine learning](https://medium.com/@ModelOriented/xai-in-the-jungle-of-competing-frameworks-for-machine-learning-fa6e96a99644)\n* [Gentle introduction to DALEX with examples](https://pbiecek.github.io/DALEX_docs/) showt introduction to the `DALEX` package.\n* [How to compare models created in different languages](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/Multilanguages_comparision.html) crosscomparison of gbm and CatBoost in R / gbm in h2o / gbm in python\n* [How to use DALEX for fraud detection](https://rawgit.com/pbiecek/DALEX_docs/master/vignettes/DALEXverse%20and%20fraud%20detection.html)\n* [How to use DALEX with keras](https://rawgit.com/pbiecek/DALEX_docs/master/vignettes/DALEX_and_keras.html)\n* [How to use DALEX with parsnip](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_parsnip.html)\n* [How to use DALEX with caret](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_caret.html)\n* [How to use DALEX with mlr](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_mlr.html)\n* [How to use DALEX with H2O](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_h2o.html)\n* [How to use DALEX with xgboost package](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_and_xgboost.html)\n* [How to use DALEX for teaching. Part 1](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_teaching.html)\n* [How to use DALEX for teaching. Part 2](https://raw.githack.com/pbiecek/DALEX_docs/master/examples/What%20they%20have%20learned%20-%20part%202.html)\n* [breakDown vs lime vs shapleyR](https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/Comparison_between_breakdown%2C_lime%2C_shapley.html)\n\n### Talks about DALEX\n\n* [Talk about DALEX at Complexity Institute / NTU February 2018](https://github.com/pbiecek/pbiecek.github.io/blob/master/Presentations/DALEX_at_NTU_2018.pdf)\n* [Talk about DALEX at SER / WTU April 2018](https://github.com/pbiecek/Talks/blob/master/2018/SER_DALEX.pdf)\n* [Talk about DALEX at STWUR May 2018 (in Polish)](https://github.com/STWUR/eRementarz-29-05-2018)\n* [Talk about DALEX at BayArea 2018](https://github.com/pbiecek/Talks/blob/master/2018/DALEX_BayArea.pdf)\n* [Talk about DALEX at PyData Warsaw 2018](https://github.com/pbiecek/Talks/blob/master/2018/DALEX_PyDataWarsaw2018.pdf)\n\n\n## Why\n\n76 years ago Isaac Asimov devised [Three Laws of Robotics](https://en.wikipedia.org/wiki/Three_Laws_of_Robotics): 1) a robot may not injure a human being, 2) a robot must obey the orders given it by human beings and 3) A robot must protect its own existence. These laws impact discussion around [Ethics of AI](https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence). Today\u2019s robots, like cleaning robots, robotic pets or autonomous cars are far from being conscious enough to be under Asimov\u2019s ethics.\n\nToday we are surrounded by complex predictive algorithms used for decision making. Machine learning models are used in health care, politics, education, judiciary and many other areas. Black box predictive models have far larger influence on our lives than physical robots. Yet, applications of such models are left unregulated despite many examples of their potential harmfulness. See *Weapons of Math Destruction* by Cathy O'Neil for an excellent overview of potential problems.\n\nIt's clear that we need to control algorithms that may affect us. Such control is in our civic rights. Here we propose three requirements that any predictive model should fulfill.\n\n-\t**Prediction's justifications**. For every prediction of a model one should be able to understand which variables affect the prediction and how strongly. Variable attribution to final prediction.\n-\t**Prediction's speculations**. For every prediction of a model one should be able to understand how the model prediction would change if input variables were changed. Hypothesizing about what-if scenarios.\n-\t**Prediction's validations** For every prediction of a model one should be able to verify how strong are evidences that confirm this particular prediction.\n\nThere are two ways to comply with these requirements.\nOne is to use only models that fulfill these conditions by design. White-box models like linear regression or decision trees. In many cases the price for transparency is lower performance.\nThe other way is to use approximated explainers \u2013 techniques that find only approximated answers, but work for any black box model. Here we present such techniques.\n\n\n## Acknowledgments\n\nWork on this package was financially supported by the 'NCN Opus grant 2016/21/B/ST6/02176'.\n"}, "dask": {"file_name": "dask/dask/README.rst", "raw_text": "Dask\n====\n\n|Linux Build Status| |Windows Build Status| |Coverage| |Doc Status| |Gitter| |Version Status| |NumFOCUS|\n\nDask is a flexible parallel computing library for analytics.  See\ndocumentation_ for more information.\n\n\nLICENSE\n-------\n\nNew BSD. See `License File <https://github.com/dask/dask/blob/master/LICENSE.txt>`__.\n\n.. _documentation: https://dask.org\n.. |Linux Build Status| image:: https://travis-ci.org/dask/dask.svg?branch=master\n   :target: https://travis-ci.org/dask/dask\n.. |Windows Build Status| image:: https://github.com/dask/dask/workflows/Windows%20CI/badge.svg?branch=master\n   :target: https://github.com/dask/dask/actions?query=workflow%3A%22Windows+CI%22\n.. |Coverage| image:: https://coveralls.io/repos/dask/dask/badge.svg\n   :target: https://coveralls.io/r/dask/dask\n   :alt: Coverage status\n.. |Doc Status| image:: https://readthedocs.org/projects/dask/badge/?version=latest\n   :target: https://dask.org\n   :alt: Documentation Status\n.. |Gitter| image:: https://badges.gitter.im/Join%20Chat.svg\n   :alt: Join the chat at https://gitter.im/dask/dask\n   :target: https://gitter.im/dask/dask?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge\n.. |Version Status| image:: https://img.shields.io/pypi/v/dask.svg\n   :target: https://pypi.python.org/pypi/dask/\n.. |NumFOCUS| image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A\n   :target: https://www.numfocus.org/\n"}, "dgl": {"file_name": "dmlc/dgl/README.md", "raw_text": "# Deep Graph Library (DGL)\n\n[![PyPi Latest Release](https://img.shields.io/pypi/v/dgl.svg)](https://pypi.org/project/dgl/)\n[![Conda Latest Release](https://anaconda.org/dglteam/dgl/badges/version.svg)](https://anaconda.org/dglteam/dgl)\n[![Build Status](http://ci.dgl.ai:80/buildStatus/icon?job=DGL/master)](http://ci.dgl.ai:80/job/DGL/job/master/)\n[![Benchmark by ASV](http://img.shields.io/badge/benchmarked%20by-asv-green.svg?style=flat)](https://asv.dgl.ai/)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](./LICENSE)\n\nDocumentation ([Latest](https://docs.dgl.ai/en/latest/) | [Stable](https://docs.dgl.ai)) | [DGL at a glance](https://docs.dgl.ai/tutorials/basics/1_first.html#sphx-glr-tutorials-basics-1-first-py) | [Model Tutorials](https://docs.dgl.ai/tutorials/models/index.html) | [Discussion Forum](https://discuss.dgl.ai)\n\n\nDGL is an easy-to-use, high performance and scalable Python package for deep learning on graphs. DGL is framework agnostic, meaning if a deep graph model is a component of an end-to-end application, the rest of the logics can be implemented in any major frameworks, such as PyTorch, Apache MXNet or TensorFlow.\n\n<p align=\"center\">\n  <img src=\"http://data.dgl.ai/asset/image/DGL-Arch.png\" alt=\"DGL v0.4 architecture\" width=\"600\">\n  <br>\n  <b>Figure</b>: DGL Overall Architecture\n</p>\n\n## <img src=\"http://data.dgl.ai/asset/image/new.png\" width=\"30\">DGL News\n*03/31/2020*: The new **v0.4.3 release** includes official TensorFlow support, with 15 popular GNN modules. DGL-KE and DGL-LifeSci, two packages for knowledge graph embedding and chemi- and bio-informatics respectively, have graduated as standalone packages and can be installed by pip and conda. The new release provides full support of graph sampling on heterogeneous graphs, with multi-GPU acceleration. See our [new feature walkthrough](https://www.dgl.ai/release/2020/04/01/release.html) and [release note](https://github.com/dmlc/dgl/releases/tag/0.4.3).\n\n*03/02/2020*: **Check out this cool paper: [Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982)!**  It includes a DGL-based benchmark framework for novel medium-scale graph datasets, covering mathematical modeling, computer vision, chemistry and combinatorial problems.  See [repo here](https://github.com/graphdeeplearning/benchmarking-gnns).\n\n## Using DGL\n\n**A data scientist** may want to apply a pre-trained model to your data right away. For this you can use DGL's [Application packages, formally *Model Zoo*](https://github.com/dmlc/dgl/tree/master/apps). Application packages are developed for domain applications, as is the case for [DGL-LifeScience](https://github.com/dmlc/dgl/tree/master/apps/life_sci). We will soon add model zoo for knowledge graph embedding learning and recommender systems. Here's how you will use a pretrained model:\n```python\nfrom dgllife.data import Tox21\nfrom dgllife.model import load_pretrained\nfrom dgllife.utils import smiles_to_bigraph, CanonicalAtomFeaturizer\n\ndataset = Tox21(smiles_to_bigraph, CanonicalAtomFeaturizer())\nmodel = load_pretrained('GCN_Tox21') # Pretrained model loaded\nmodel.eval()\n\nsmiles, g, label, mask = dataset[0]\nfeats = g.ndata.pop('h')\nlabel_pred = model(g, feats)\nprint(smiles)                   # CCOc1ccc2nc(S(N)(=O)=O)sc2c1\nprint(label_pred[:, mask != 0]) # Mask non-existing labels\n# tensor([[ 1.4190, -0.1820,  1.2974,  1.4416,  0.6914,  \n# 2.0957,  0.5919,  0.7715, 1.7273,  0.2070]])\n```\n\n**Further reading**: DGL is released as a managed service on AWS SageMaker, see the medium posts for an easy trip to DGL on SageMaker([part1](https://medium.com/@julsimon/a-primer-on-graph-neural-networks-with-amazon-neptune-and-the-deep-graph-library-5ce64984a276) and [part2](https://medium.com/@julsimon/deep-graph-library-part-2-training-on-amazon-sagemaker-54d318dfc814)).\n\n**Researchers** can start from the growing list of [models implemented in DGL](https://github.com/dmlc/dgl/tree/master/examples). Developing new models does not mean that you have to start from scratch. Instead, you can reuse many [pre-built modules](https://docs.dgl.ai/api/python/nn.html). Here is how to get a standard two-layer graph convolutional model with a pre-built GraphConv module:\n```python\nfrom dgl.nn.pytorch import GraphConv\nimport torch.nn.functional as F\n\n# build a two-layer GCN with ReLU as the activation in between\nclass GCN(nn.Module):\n    def __init__(self, in_feats, h_feats, num_classes):\n        super(GCN, self).__init__()\n        self.gcn_layer1 = GraphConv(in_feats, h_feats)\n        self.gcn_layer2 = GraphConv(h_feats, num_classes)\n    \n    def forward(self, graph, inputs):\n        h = self.gcn_layer1(graph, inputs)\n        h = F.relu(h)\n        h = self.gcn_layer2(graph, h)\n        return h\n```\n\nNext level down, you may want to innovate your own module. DGL offers a succinct message-passing interface (see tutorial [here](https://docs.dgl.ai/tutorials/basics/3_pagerank.html)). Here is how Graph Attention Network (GAT) is implemented ([complete codes](https://docs.dgl.ai/api/python/nn.pytorch.html#gatconv)). Of course, you can also find GAT as a module [GATConv](https://docs.dgl.ai/api/python/nn.pytorch.html#gatconv):\n```python\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define a GAT layer\nclass GATLayer(nn.Module):\n    def __init__(self, in_feats, out_feats):\n        super(GATLayer, self).__init__()\n        self.linear_func = nn.Linear(in_feats, out_feats, bias=False)\n        self.attention_func = nn.Linear(2 * out_feats, 1, bias=False)\n        \n    def edge_attention(self, edges):\n        concat_z = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n        src_e = self.attention_func(concat_z)\n        src_e = F.leaky_relu(src_e)\n        return {'e': src_e}\n    \n    def message_func(self, edges):\n        return {'z': edges.src['z'], 'e':edges.data['e']}\n        \n    def reduce_func(self, nodes):\n        a = F.softmax(nodes.mailbox['e'], dim=1)\n        h = torch.sum(a * nodes.mailbox['z'], dim=1)\n        return {'h': h}\n                               \n    def forward(self, graph, h):\n        z = self.linear_func(h)\n        graph.ndata['z'] = z\n        graph.apply_edges(self.edge_attention)\n        graph.update_all(self.message_func, self.reduce_func)\n        return graph.ndata.pop('h')\n```\n## Performance and Scalability\n\n**Microbenchmark on speed and memory usage**: While leaving tensor and autograd functions to backend frameworks (e.g. PyTorch, MXNet, and TensorFlow), DGL aggressively optimizes storage and computation with its own kernels. Here's a comparison to another popular package -- PyTorch Geometric (PyG). The short story is that raw speed is similar, but DGL has much better memory management.\n\n\n| Dataset  |    Model     |                   Accuracy                   |                    Time <br> PyG &emsp;&emsp; DGL                    |           Memory <br> PyG &emsp;&emsp; DGL            |\n| -------- |:------------:|:--------------------------------------------:|:--------------------------------------------------------------------:|:-----------------------------------------------------:|\n| Cora     | GCN <br> GAT | 81.31 &plusmn; 0.88 <br> 83.98 &plusmn; 0.52 | <b>0.478</b> &emsp;&emsp; 0.666 <br> 1.608 &emsp;&emsp; <b>1.399</b> | 1.1 &emsp;&emsp; 1.1 <br> 1.2 &emsp;&emsp; <b>1.1</b> |\n| CiteSeer | GCN <br> GAT | 70.98 &plusmn; 0.68 <br> 69.96 &plusmn; 0.53 | <b>0.490</b> &emsp;&emsp; 0.674 <br> 1.606 &emsp;&emsp; <b>1.399</b> | 1.1 &emsp;&emsp; 1.1 <br> 1.3 &emsp;&emsp; <b>1.1</b> |\n| PubMed   | GCN <br> GAT | 79.00 &plusmn; 0.41 <br> 77.65 &plusmn; 0.32 | <b>0.491</b> &emsp;&emsp; 0.690 <br> 1.946 &emsp;&emsp; <b>1.393</b> | 1.1 &emsp;&emsp; 1.1 <br> 1.6 &emsp;&emsp; <b>1.1</b> |\n| Reddit   |     GCN      |             93.46 &plusmn; 0.06              |                    *OOM*&emsp;&emsp; <b>28.6</b>                     |            *OOM* &emsp;&emsp; <b>11.7</b>             |\n| Reddit-S |     GCN      |                     N/A                      |                    29.12 &emsp;&emsp; <b>9.44</b>                    |             15.7 &emsp;&emsp; <b>3.6</b>              |\n\nTable: Training time(in seconds) for 200 epochs and memory consumption(GB)\n\nHere is another comparison of DGL on TensorFlow backend with other TF-based GNN tools (training time in seconds for one epoch):\n\n| Dateset | Model | DGL | GraphNet | tf_geometric |\n| ------- | ----- | --- | -------- | ------------ |\n| Core | GCN | 0.0148 | 0.0152 | 0.0192 |\n| Reddit | GCN | 0.1095 | OOM | OOM |\n| PubMed | GCN | 0.0156 | 0.0553 | 0.0185 |\n| PPI | GCN | 0.09 | 0.16 | 0.21 |\n| Cora | GAT | 0.0442 | n/a | 0.058 |\n| PPI | GAT | 0.398 | n/a | 0.752 |\n\nHigh memory utilization allows DGL to push the limit of single-GPU performance, as seen in below images.\n| <img src=\"http://data.dgl.ai/asset/image/DGLvsPyG-time1.png\" width=\"400\"> | <img src=\"http://data.dgl.ai/asset/image/DGLvsPyG-time2.png\" width=\"400\"> |\n| -------- | -------- |\n\n**Scalability**: DGL has fully leveraged multiple GPUs in both one machine and clusters for increasing training speed, and has better performance than alternatives, as seen in below images.\n\n<p align=\"center\">\n  <img src=\"http://data.dgl.ai/asset/image/one-four-GPUs.png\" width=\"600\">\n</p>\n\n| <img src=\"http://data.dgl.ai/asset/image/one-four-GPUs-DGLvsGraphVite.png\"> |  <img src=\"http://data.dgl.ai/asset/image/one-fourMachines.png\"> | \n| :---------------------------------------: | -- |\n\n\n**Further reading**: Detailed comparison of DGL and other Graph alternatives can be found [here](https://arxiv.org/abs/1909.01315).\n\n## DGL Models and Applications\n\n### DGL for research\nOverall there are 30+ models implemented by using DGL:\n- [PyTorch](https://github.com/dmlc/dgl/tree/master/examples/pytorch)\n- [MXNet](https://github.com/dmlc/dgl/tree/master/examples/mxnet)\n- [TensorFlow](https://github.com/dmlc/dgl/tree/master/examples/tensorflow)\n\n### DGL for domain applications\n- [DGL-LifeSci](https://github.com/dmlc/dgl/tree/master/apps/life_sci), previously DGL-Chem\n- [DGL-KE](https://github.com/awslabs/dgl-ke)\n- DGL-RecSys(coming soon)\n\n### DGL for NLP/CV problems\n- [TreeLSTM](https://github.com/dmlc/dgl/tree/master/examples/pytorch/tree_lstm)\n- [GraphWriter](https://github.com/dmlc/dgl/tree/master/examples/pytorch/graphwriter)\n- [Capsule Network](https://github.com/dmlc/dgl/tree/master/examples/pytorch/capsule)\n\nWe are currently in Beta stage.  More features and improvements are coming.\n\n\n## Installation\n\nDGL should work on\n\n* all Linux distributions no earlier than Ubuntu 16.04\n* macOS X\n* Windows 10\n\nDGL requires Python 3.5 or later.\n\nRight now, DGL works on [PyTorch](https://pytorch.org) 1.2.0+, [MXNet](https://mxnet.apache.org) 1.5.1+, and [TensorFlow](https://tensorflow.org) 2.1.0+.\n\n\n### Using anaconda\n\n```\nconda install -c dglteam dgl           # cpu version\nconda install -c dglteam dgl-cuda9.0   # CUDA 9.0\nconda install -c dglteam dgl-cuda9.2   # CUDA 9.2\nconda install -c dglteam dgl-cuda10.0  # CUDA 10.0\nconda install -c dglteam dgl-cuda10.1  # CUDA 10.1\n```\n\n### Using pip\n\n\n|           | Latest Nightly Build Version  | Stable Version          |\n|-----------|-------------------------------|-------------------------|\n| CPU       | `pip install --pre dgl`       | `pip install dgl`       |\n| CUDA 9.0  | `pip install --pre dgl-cu90`  | `pip install dgl-cu90`  |\n| CUDA 9.2  | `pip install --pre dgl-cu92`  | `pip install dgl-cu92`  |\n| CUDA 10.0 | `pip install --pre dgl-cu100` | `pip install dgl-cu100` |\n| CUDA 10.1 | `pip install --pre dgl-cu101` | `pip install dgl-cu101` |\n\n### Built from source code\n\nRefer to the guide [here](https://docs.dgl.ai/install/index.html#install-from-source).\n\n\n## DGL Major Releases\n\n| Releases  | Date   | Features |\n|-----------|--------|-------------------------|\n| v0.4.3    | 03/31/2020 | - TensorFlow support <br> - DGL-KE <br> - DGL-LifeSci <br> - Heterograph sampling APIs (experimental) |\n| v0.4.2      | 01/24/2020 |  - Heterograph support <br> - TensorFlow support (experimental) <br> - MXNet GNN modules <br> | \n| v0.3.1 | 08/23/2019 | - APIs for GNN modules <br> - Model zoo (DGL-Chem) <br> - New installation |\n| v0.2 | 03/09/2019 | - Graph sampling APIs <br> - Speed improvement |\n| v0.1 | 12/07/2018 | - Basic DGL APIs <br> - PyTorch and MXNet support <br> - GNN model examples and tutorials |\n\n## New to Deep Learning and Graph Deep Learning?\n\nCheck out the open source book [*Dive into Deep Learning*](http://gluon.ai/).\n\nFor those who are new to graph neural network, please see the [basic of DGL](https://docs.dgl.ai/tutorials/basics/index.html).\n\nFor audience who are looking for more advanced, realistic, and end-to-end examples, please see [model tutorials](https://docs.dgl.ai/tutorials/models/index.html).\n\n\n## Contributing\n\nPlease let us know if you encounter a bug or have any suggestions by [filing an issue](https://github.com/dmlc/dgl/issues).\n\nWe welcome all contributions from bug fixes to new features and extensions.\n\nWe expect all contributions discussed in the issue tracker and going through PRs.  Please refer to our [contribution guide](https://docs.dgl.ai/contribute.html).\n\n## Cite\n\nIf you use DGL in a scientific publication, we would appreciate citations to the following paper:\n```\n@article{wang2019dgl,\n    title={Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs},\n    url={https://arxiv.org/abs/1909.01315},\n    author={Wang, Minjie and Yu, Lingfan and Zheng, Da and Gan, Quan and Gai, Yu and Ye, Zihao and Li, Mufei and Zhou, Jinjing and Huang, Qi and Ma, Chao and Huang, Ziyue and Guo, Qipeng and Zhang, Hao and Lin, Haibin and Zhao, Junbo and Li, Jinyang and Smola, Alexander J and Zhang, Zheng},\n    journal={ICLR Workshop on Representation Learning on Graphs and Manifolds},\n    year={2019}\n}\n```\n\n## The Team\n\nDGL is developed and maintained by [NYU, NYU Shanghai, AWS Shanghai AI Lab, and AWS MXNet Science Team](https://www.dgl.ai/pages/about.html).\n\n\n## License\n\nDGL uses Apache License 2.0.\n"}, "dill": {"file_name": "uqfoundation/dill/README.md", "raw_text": "dill\n====\nserialize all of python\n\nAbout Dill\n----------\n``dill`` extends python's ``pickle`` module for serializing and de-serializing\npython objects to the majority of the built-in python types. Serialization\nis the process of converting an object to a byte stream, and the inverse\nof which is converting a byte stream back to on python object hierarchy.\n\n``dill`` provides the user the same interface as the ``pickle`` module, and\nalso includes some additional features. In addition to pickling python\nobjects, ``dill`` provides the ability to save the state of an interpreter\nsession in a single command.  Hence, it would be feasable to save a\ninterpreter session, close the interpreter, ship the pickled file to\nanother computer, open a new interpreter, unpickle the session and\nthus continue from the 'saved' state of the original interpreter\nsession.\n\n``dill`` can be used to store python objects to a file, but the primary\nusage is to send python objects across the network as a byte stream.\n``dill`` is quite flexible, and allows arbitrary user defined classes\nand functions to be serialized.  Thus ``dill`` is not intended to be\nsecure against erroneously or maliciously constructed data. It is\nleft to the user to decide whether the data they unpickle is from\na trustworthy source.\n\n``dill`` is part of ``pathos``, a python framework for heterogeneous computing.\n``dill`` is in active development, so any user feedback, bug reports, comments,\nor suggestions are highly appreciated.  A list of known issues is maintained\nat http://trac.mystic.cacr.caltech.edu/project/pathos/query.html, with a public\nticket list at https://github.com/uqfoundation/dill/issues.\n\n\nMajor Features\n--------------\n``dill`` can pickle the following standard types:\n\n* none, type, bool, int, long, float, complex, str, unicode,\n* tuple, list, dict, file, buffer, builtin,\n* both old and new style classes,\n* instances of old and new style classes,\n* set, frozenset, array, functions, exceptions\n\n``dill`` can also pickle more 'exotic' standard types:\n\n* functions with yields, nested functions, lambdas\n* cell, method, unboundmethod, module, code, methodwrapper,\n* dictproxy, methoddescriptor, getsetdescriptor, memberdescriptor,\n* wrapperdescriptor, xrange, slice,\n* notimplemented, ellipsis, quit\n\n``dill`` cannot yet pickle these standard types:\n\n* frame, generator, traceback\n\n``dill`` also provides the capability to:\n\n* save and load python interpreter sessions\n* save and extract the source code from functions and classes\n* interactively diagnose pickling errors\n\n\nCurrent Release\n---------------\nThe latest released version of ``dill`` is available from:\n    https://pypi.org/project/dill\n\n``dill`` is distributed under a 3-clause BSD license.\n\n\nDevelopment Version\n[![Documentation Status](https://readthedocs.org/projects/dill/badge/?version=latest)](https://dill.readthedocs.io/en/latest/?badge=latest)\n[![Travis Build Status](https://img.shields.io/travis/uqfoundation/dill.svg?label=build&logo=travis&branch=master)](https://travis-ci.org/uqfoundation/dill)\n[![codecov](https://codecov.io/gh/uqfoundation/dill/branch/master/graph/badge.svg)](https://codecov.io/gh/uqfoundation/dill)\n-------------------\nYou can get the latest development version with all the shiny new features at:\n    https://github.com/uqfoundation\n\nIf you have a new contribution, please submit a pull request.\n\n\nMore Information\n----------------\nProbably the best way to get started is to look at the documentation at\nhttp://dill.rtfd.io. Also see ``dill.tests`` for a set of scripts that\ndemonstrate how ``dill`` can serialize different python objects. You can\nrun the test suite with ``python -m dill.tests``. The contents of any\npickle file can be examined with ``undill``.  As ``dill`` conforms to\nthe ``pickle`` interface, the examples and documentation found at\nhttp://docs.python.org/library/pickle.html also apply to ``dill``\nif one will ``import dill as pickle``. The source code is also generally\nwell documented, so further questions may be resolved by inspecting the\ncode itself. Please feel free to submit a ticket on github, or ask a\nquestion on stackoverflow (**@Mike McKerns**).\nIf you would like to share how you use ``dill`` in your work, please send\nan email (to **mmckerns at uqfoundation dot org**).\n\n\nCitation\n--------\nIf you use ``dill`` to do research that leads to publication, we ask that you\nacknowledge use of ``dill`` by citing the following in your publication::\n\n    M.M. McKerns, L. Strand, T. Sullivan, A. Fang, M.A.G. Aivazis,\n    \"Building a framework for predictive science\", Proceedings of\n    the 10th Python in Science Conference, 2011;\n    http://arxiv.org/pdf/1202.1056\n\n    Michael McKerns and Michael Aivazis,\n    \"pathos: a framework for heterogeneous computing\", 2010- ;\n    http://trac.mystic.cacr.caltech.edu/project/pathos\n\nPlease see http://trac.mystic.cacr.caltech.edu/project/pathos or\nhttp://arxiv.org/pdf/1202.1056 for further information.\n\n"}, "distributed": {"file_name": "dask/distributed/README.rst", "raw_text": "Distributed\n===========\n\n|Linux Build Status| |Windows Build Status| |Doc Status| |Gitter| |Version Status| |NumFOCUS|\n\nA library for distributed computation.  See documentation_ for more details.\n\n.. _documentation: https://distributed.dask.org\n.. |Linux Build Status| image:: https://travis-ci.org/dask/distributed.svg?branch=master\n   :target: https://travis-ci.org/dask/distributed\n.. |Windows Build Status| image:: https://github.com/dask/distributed/workflows/Windows%20CI/badge.svg?branch=master\n   :target: https://github.com/dask/distributed/actions?query=workflow%3A%22Windows+CI%22\n.. |Doc Status| image:: https://readthedocs.org/projects/distributed/badge/?version=latest\n   :target: https://distributed.dask.org\n   :alt: Documentation Status\n.. |Gitter| image:: https://badges.gitter.im/Join%20Chat.svg\n   :alt: Join the chat at https://gitter.im/dask/dask\n   :target: https://gitter.im/dask/dask?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge\n.. |Version Status| image:: https://img.shields.io/pypi/v/distributed.svg\n   :target: https://pypi.python.org/pypi/distributed/\n.. |NumFOCUS| image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A\n   :target: https://www.numfocus.org/\n"}, "dowhy": {"file_name": "microsoft/dowhy/README.rst", "raw_text": "|BuildStatus|_ |PyPiVersion|_ |PythonSupport|_\n\n.. |PyPiVersion| image:: https://img.shields.io/pypi/v/dowhy.svg\n.. _PyPiVersion: https://pypi.org/project/dowhy/\n\n.. |PythonSupport| image:: https://img.shields.io/pypi/pyversions/dowhy.svg\n.. _PythonSupport: https://pypi.org/project/dowhy/\n\n.. |BuildStatus| image:: https://dev.azure.com/ms/dowhy/_apis/build/status/microsoft.dowhy?branchName=master\n.. _BuildStatus: https://dev.azure.com/ms/dowhy/_build/latest?definitionId=179&branchName=master\n\nDoWhy | Making causal inference easy\n====================================\n\n`Amit Sharma <http://www.amitsharma.in>`_,\n`Emre Kiciman <http://www.kiciman.org>`_\n\n Read the `docs <https://microsoft.github.io/dowhy/>`_ | Try it online! |AzureNotebooks|_ |Binder|_ \n\n.. |AzureNotebooks| image:: https://notebooks.azure.com/launch.svg\n.. _AzureNotebooks: https://notebooks.azure.com/amshar/projects/dowhy/tree/docs/source\n\n.. |Binder| image:: https://mybinder.org/badge_logo.svg\n.. _Binder: https://mybinder.org/v2/gh/microsoft/dowhy/master?filepath=docs%2Fsource%2F\n\n Blog Posts: `Introducing DoWhy <https://www.microsoft.com/en-us/research/blog/dowhy-a-library-for-causal-inference/>`_ | `Using the Do-sampler <https://medium.com/@akelleh/introducing-the-do-sampler-for-causal-inference-a3296ea9e78d>`_\n\nAs computing systems are more frequently and more actively intervening in societally critical domains such as healthcare, education, and governance, it is critical to correctly predict and understand the causal effects of these interventions. Without an A/B test, conventional machine learning methods, built on pattern recognition and correlational analyses, are insufficient for causal reasoning. \n\nMuch like machine learning libraries have done for prediction, **\"DoWhy\" is a Python library that aims to spark causal thinking and analysis**. DoWhy provides a unified interface for causal inference methods and automatically tests many assumptions, thus making inference accessible to non-experts.\n\nFor a quick introduction to causal inference, check out `amit-sharma/causal-inference-tutorial <https://github.com/amit-sharma/causal-inference-tutorial/>`_. We also gave a more comprehensive tutorial at the ACM Knowledge Discovery and Data Mining (`KDD 2018 <http://www.kdd.org/kdd2018/>`_) conference: `causalinference.gitlab.io/kdd-tutorial <http://causalinference.gitlab.io/kdd-tutorial/>`_.\n\nDocumentation for DoWhy is available at `microsoft.github.io/dowhy <https://microsoft.github.io/dowhy/>`_.\n\n.. i here comment toctree::\n.. i here comment   :maxdepth: 4\n.. i here comment   :caption: Contents:\n.. contents:: Contents\n\nThe need for causal inference\n----------------------------------\n\nPredictive models uncover patterns that connect the inputs and outcome in observed data. To intervene, however, we need to estimate the effect of changing an input from its current value, for which no data exists. Such questions, involving estimating a *counterfactual*, are common in decision-making scenarios.\n\n* Will it work?\n    * Does a proposed change to a system improve people's outcomes?\n* Why did it work?\n    * What led to a change in a system's outcome?\n* What should we do?\n    * What changes to a system are likely to improve outcomes for people?\n* What are the overall effects?\n    * How does the system interact with human behavior?\n    * What is the effect of a system's recommendations on people's activity?\n\nAnswering these questions requires causal reasoning. While many methods exist\nfor causal inference, it is hard to compare their assumptions and robustness of results. DoWhy makes three contributions,\n\n1. Provides a principled way of modeling a given problem as a causal graph so\n   that all assumptions are explicit.\n2. Provides a unified interface for many popular causal inference methods, combining the two major frameworks of graphical models and potential outcomes.\n3. Automatically tests for the validity of assumptions if possible and assesses\n   the robustness of the estimate to violations.\n\nInstallation\n-------------\n\n**Requirements**\n\nDoWhy support Python 3+. It requires the following packages:\n\n* numpy\n* scipy\n* scikit-learn\n* pandas\n* networkx  (for analyzing causal graphs)\n* matplotlib (for general plotting)\n* sympy (for rendering symbolic expressions)\n\nInstall the latest release using pip. \n\n.. code:: shell\n   \n   pip install dowhy\n   \nIf you prefer the latest dev version, clone this repository and run the following command from the top-most folder of\nthe repository.\n\n.. code:: shell\n    \n    pip install -e .\n\nIf you face any problems, try installing dependencies manually.\n\n.. code:: shell\n    \n    pip install -r requirements.txt\n\nOptionally, if you wish to input graphs in the dot format, then install pydot (or pygraphviz).\n\n\nFor better-looking graphs, you can optionally install pygraphviz. To proceed,\nfirst install graphviz and then pygraphviz (on Ubuntu and Ubuntu WSL).\n\n.. code:: shell\n\n    sudo apt install graphviz libgraphviz-dev graphviz-dev pkg-config\n    ## from https://github.com/pygraphviz/pygraphviz/issues/71\n    pip install pygraphviz --install-option=\"--include-path=/usr/include/graphviz\" \\\n    --install-option=\"--library-path=/usr/lib/graphviz/\"\n\nKeep in mind that pygraphviz installation can be problematic on the latest versions of Python3. Tested to work with Python 3.5.\n\nSample causal inference analysis in DoWhy\n-------------------------------------------\nMost DoWhy\nanalyses for causal inference take 4 lines to write, assuming a\npandas dataframe df that contains the data:\n\n.. code:: python\n\n    from dowhy import CausalModel\n    import dowhy.datasets\n\n    # Load some sample data\n    data = dowhy.datasets.linear_dataset(\n        beta=10,\n        num_common_causes=5,\n        num_instruments=2,\n        num_samples=10000,\n        treatment_is_binary=True)\n\nDoWhy supports two formats for providing the causal graph: `gml <https://github.com/GunterMueller/UNI_PASSAU_FMI_Graph_Drawing>`_ (preferred) and `dot <http://www.graphviz.org/documentation/>`_. After loading in the data, we use the four main operations in DoWhy: *model*,\n*estimate*, *identify* and *refute*:\n\n.. code:: python\n\n    # Create a causal model from the data and given graph.\n    model = CausalModel(\n        data=data[\"df\"],\n        treatment=data[\"treatment_name\"],\n        outcome=data[\"outcome_name\"],\n        graph=data[\"gml_graph\"])\n\n    # Identify causal effect and return target estimands\n    identified_estimand = model.identify_effect()\n\n    # Estimate the target estimand using a statistical method.\n    estimate = model.estimate_effect(identified_estimand,\n                                     method_name=\"backdoor.propensity_score_matching\")\n\n    # Refute the obtained estimate using multiple robustness checks.\n    refute_results = model.refute_estimate(identified_estimand, estimate,\n                                           method_name=\"random_common_cause\")\n\nDoWhy stresses on the interpretability of its output. At any point in the analysis,\nyou can inspect the untested assumptions, identified estimands (if any) and the\nestimate (if any). Here's a sample output of the linear regression estimator.\n\n.. image:: https://raw.githubusercontent.com/microsoft/dowhy/master/docs/images/regression_output.png\n\nFor detailed code examples, check out the Jupyter notebooks in `docs/source/example_notebooks <https://github.com/microsoft/dowhy/tree/master/docs/source/example_notebooks/>`_, or try them online at `Binder <https://mybinder.org/v2/gh/microsoft/dowhy/master?filepath=docs%2Fsource%2F>`_.\n\n\nA High-level Pandas API\n-----------------------\n\nWe've made an even simpler API for dowhy which is a light layer on top of the standard one. The goal\nwas to make causal analysis much more like regular exploratory analysis. To use this API, simply\nimport :code:`dowhy.api`. This will magically add the :code:`causal` namespace to your\n:code:`pandas.DataFrame` s. Then,\nyou can use the namespace as follows.\n\n.. code:: python\n\n    import dowhy.api\n    import dowhy.datasets\n\n    data = dowhy.datasets.linear_dataset(beta=5,\n        num_common_causes=1,\n        num_instruments = 0,\n        num_samples=1000,\n        treatment_is_binary=True)\n\n    # data['df'] is just a regular pandas.DataFrame\n    data['df'].causal.do(x='v0', # name of treatment variable\n                         variable_types={'v0': 'b', 'y': 'c', 'W0': 'c'},\n                         outcome='y',\n                         common_causes=['W0']).groupby('v0').mean().plot(y='y', kind='bar')\n\n.. image:: https://raw.githubusercontent.com/microsoft/dowhy/master/docs/images/do_barplot.png\n\nFor some methods, the :code:`variable_types` field must be specified. It should be a :code:`dict`, where the keys are\nvariable names, and values are 'o' for ordered discrete, 'u' for un-ordered discrete, 'd' for discrete, or 'c'\nfor continuous.\n\n**Note:If the** :code:`variable_types` **is not specified we make use of the following implicit conversions:**\n::\n   int -> 'c'\n   float -> 'c'\n   binary -> 'b'\n   category -> 'd'\n**Currently we have not added support for time.**\n\nThe :code:`do` method in the causal namespace generates a random sample from $P(outcome|do(X=x))$ of the\nsame length as your data set, and returns this outcome as a new :code:`DataFrame`. You can continue to perform\nthe usual :code:`DataFrame` operations with this sample, and so you can compute statistics and create plots\nfor causal outcomes!\n\nThe :code:`do` method is built on top of the lower-level :code:`dowhy` objects, so can still take a graph and perform\nidentification automatically when you provide a graph instead of :code:`common_causes`.\n\nGraphical Models and Potential Outcomes: Best of both worlds\n------------------------------------------------------------\nDoWhy builds on two of the most powerful frameworks for causal inference:\ngraphical models and potential outcomes. It uses graph-based criteria and\ndo-calculus for modeling assumptions and identifying a non-parametric causal effect.\nFor estimation, it switches to methods based primarily on potential outcomes.\n\nA unifying language for causal inference\n----------------------------------------\n\nDoWhy is based on a simple unifying language for causal inference. Causal\ninference may seem tricky, but almost all methods follow four key steps:\n\n1. Model a causal inference problem using assumptions.\n2. Identify an expression for the causal effect under these assumptions (\"causal estimand\").\n3. Estimate the expression using statistical methods such as matching or instrumental variables.\n4. Finally, verify the validity of the estimate using a variety of robustness checks.\n\nThis workflow can be captured by four key verbs in DoWhy:\n\n- model\n- identify\n- estimate\n- refute\n\nUsing these verbs, DoWhy implements a causal inference engine that can support \na variety of methods. *model* encodes prior knowledge as a formal causal graph, *identify* uses \ngraph-based methods to identify the causal effect, *estimate* uses  \nstatistical methods for estimating the identified estimand, and finally *refute* \ntries to refute the obtained estimate by testing robustness to assumptions.\n\nDoWhy brings three key differences compared to available software for causal inference:\n\n**Explicit identifying assumptions**\n    Assumptions are first-class citizens in DoWhy.\n\n    Each analysis starts with a\n    building a causal model. The assumptions can be viewed graphically or in terms\n    of conditional independence statements. Wherever possible, DoWhy can also\n    automatically test for stated assumptions using observed data.\n\n**Separation between identification and estimation**\n    Identification is the causal problem. Estimation is simply a statistical problem.\n\n    DoWhy\n    respects this boundary and treats them separately. This focuses the causal\n    inference effort on identification, and frees up estimation using any\n    available statistical estimator for a target estimand. In addition, multiple\n    estimation methods can be used for a single identified_estimand and\n    vice-versa.\n\n**Automated robustness checks**\n    What happens when key identifying assumptions may not be satisfied?\n\n    The most critical, and often skipped, part of causal analysis is checking the\n    robustness of an estimate to unverified assumptions. DoWhy makes it easy to\n    automatically run sensitivity and robustness checks on the obtained estimate.\n\nFinally, DoWhy is easily extensible, allowing other implementations of the\nfour verbs to co-exist (we hope to integrate with external\nimplementations in the future). The four verbs are mutually independent, so their\nimplementations can be combined in any way.\n\n\n\nBelow are more details about the current implementation of each of these verbs.\n\nFour steps of causal inference\n------------------------------\n\nI. **Model a causal problem**\n\nDoWhy creates an underlying causal graphical model for each problem. This\nserves to make each causal assumption explicit. This graph need not be\ncomplete---you can provide a partial graph, representing prior\nknowledge about some of the variables. DoWhy automatically considers the rest\nof the variables as potential confounders.\n\nCurrently, DoWhy supports two formats for graph input: `gml <https://github.com/GunterMueller/UNI_PASSAU_FMI_Graph_Drawing>`_ (preferred) and\n`dot <http://www.graphviz.org/documentation/>`_. We strongly suggest to use gml as the input format, as it works well with networkx. You can provide the graph either as a .gml file or as a string. If you prefer to use dot format, you will need to install additional packages (pydot or pygraphviz, see the installation section above). Both .dot files and string format are supported. \n\nWhile not recommended, you can also specify common causes and/or instruments directly\ninstead of providing a graph.\n\n\n.. i comment image:: causal_model.png\n\nII. **Identify a target estimand under the model**\n\nBased on the causal graph, DoWhy finds all possible ways of identifying a desired causal effect based on\nthe graphical model. It uses graph-based criteria and do-calculus to find\npotential ways find expressions that can identify the causal effect.\n\nIII. **Estimate causal effect based on the identified estimand**\n\nDoWhy supports methods based on both back-door criterion and instrumental\nvariables. It also provides a non-parametric permutation test for testing\nthe statistical significance of obtained estimate. \n\nCurrently supported back-door criterion methods.\n\n* Methods based on estimating the treatment assignment\n    * Propensity-based Stratification\n    * Propensity Score Matching\n    * Inverse Propensity Weighting\n\n* Methods based on estimating the response surface\n    * Regression\n\nCurrently supported methods based on instrumental variables.\n\n* Binary Instrument/Wald Estimator\n* Regression discontinuity\n\n\nIV. **Refute the obtained estimate**\n\nHaving access to multiple refutation methods to verify a causal inference is\na key benefit of using DoWhy.\n\nDoWhy supports the following refutation methods.\n\n* Placebo Treatment\n* Irrelevant Additional Confounder\n* Subset validation\n\nCiting this package\n-------------------\nIf you find DoWhy useful for your research work, please cite us as follows:\n\nAmit Sharma, Emre Kiciman, et al. DoWhy: A Python package for causal inference. 2019. https://github.com/microsoft/dowhy\n\nBibtex::\n\n  @misc{dowhy,\n  authors={Sharma, Amit and Kiciman, Emre and others},\n  title={Do{W}hy: {A Python package for causal inference}},\n  howpublished={https://github.com/microsoft/dowhy}\n  year={2019}\n  }\n\n\nRoadmap \n-----------\nThe `projects <https://github.com/microsoft/dowhy/projects>`_ page lists the next steps for DoWhy. If you would like to contribute, have a look at the current projects. If you have a specific request for DoWhy, please raise an issue `here <https://github.com/microsoft/dowhy/issues>`_.\n\nContributing\n-------------\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the `Microsoft Open Source Code of Conduct <https://opensource.microsoft.com/codeofconduct/>`_.\nFor more information see the `Code of Conduct FAQ <https://opensource.microsoft.com/codeofconduct/faq/>`_ or\ncontact `opencode@microsoft.com <mailto:opencode@microsoft.com>`_ with any additional questions or comments.\n"}, "dtale": {"file_name": "man-group/dtale/README.md", "raw_text": "[![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Title.png)](https://github.com/man-group/dtale)\n\n* [Live Demo](http://alphatechadmin.pythonanywhere.com)\n* [Animated US COVID-19 Deaths By State](http://alphatechadmin.pythonanywhere.com/charts/3?chart_type=maps&query=date+%3E+%2720200301%27&agg=raw&map_type=choropleth&loc_mode=USA-states&loc=state_code&map_val=deaths&colorscale=Reds&cpg=false&animate_by=date)\n* [3D Scatter Chart](http://alphatechadmin.pythonanywhere.com/charts/4?chart_type=3d_scatter&query=&x=date&z=Col0&agg=raw&cpg=false&y=%5B%22security_id%22%5D)\n* [Surface Chart](http://alphatechadmin.pythonanywhere.com/charts/4?chart_type=surface&query=&x=date&z=Col0&agg=raw&cpg=false&y=%5B%22security_id%22%5D)\n\n-----------------\n\n[![CircleCI](https://circleci.com/gh/man-group/dtale.svg?style=shield&circle-token=4b67588a87157cc03b484fb96be438f70b5cd151)](https://circleci.com/gh/man-group/dtale)\n[![PyPI](https://img.shields.io/pypi/pyversions/dtale.svg)](https://pypi.python.org/pypi/dtale/)\n[![ReadTheDocs](https://readthedocs.org/projects/dtale/badge)](https://dtale.readthedocs.io)\n[![codecov](https://codecov.io/gh/man-group/dtale/branch/master/graph/badge.svg)](https://codecov.io/gh/man-group/dtale)\n[![Downloads](https://pepy.tech/badge/dtale)](https://pepy.tech/project/dtale)\n\n## What is it?\n\nD-Tale is the combination of a Flask back-end and a React front-end to bring you an easy way to view & analyze Pandas data structures.  It integrates seamlessly with ipython notebooks & python/ipython terminals.  Currently this tool supports such Pandas objects as DataFrame, Series, MultiIndex, DatetimeIndex & RangeIndex.\n\n## Origins\n\nD-Tale was the product of a SAS to Python conversion.  What was originally a perl script wrapper on top of SAS's `insight` function is now a lightweight web client on top of Pandas data structures.\n\n## In The News\n - [Man Institute](https://www.man.com/maninstitute/d-tale) (warning: contains deprecated functionality)\n - [Python Bytes](https://pythonbytes.fm/episodes/show/169/jupyter-notebooks-natively-on-your-ipad)\n - [towards data science](https://towardsdatascience.com/introduction-to-d-tale-5eddd81abe3f)\n\n\n## Tutorials\n - [Pip Install Python YouTube Channel](https://m.youtube.com/watch?v=0RihZNdQc7k&feature=youtu.be)\n - [machine_learning_2019](https://www.youtube.com/watch?v=-egtEUVBy9c)\n\n## Contents\n\n- [Getting Started](#getting-started)\n  - [Python Terminal](#python-terminal)\n  - [Jupyter Notebook](#jupyter-notebook)\n  - [Jupyterhub w/ Kubernetes](https://github.com/man-group/dtale/blob/master/docs/JUPYTERHUB_KUBERNETES.md)\n  - [Google Colab & Kaggle](#google-colab--kaggle)\n  - [R with Reticulate](#r-with-reticulate)\n  - [Command-line](#command-line)\n- [UI](#ui)\n  - [Dimensions/Main Menu](#dimensionsmain-menu)\n  - [Header](#header)\n  - [Main Menu Functions](#main-menu-functions)\n    - [Describe](#describe), [Outlier Detection](#outlier-detection), [Custom Filter](#custom-filter), [Building Columns](#building-columns), [Summarize Data](#summarize-data), [Charts](#charts), [Coverage (Deprecated)](#coverage-deprecated), [Correlations](#correlations), [Heat Map](#heat-map), [Highlight Dtypes](#highlight-dtypes), [Instances](#instances), [Code Exports](#code-exports), [About](#about), [Resize](#resize), [Shutdown](#shutdown)\n  - [Column Menu Functions](#column-menu-functions)\n    - [Filtering](#filtering), [Moving Columns](#moving-columns), [Hiding Columns](#hiding-columns), [Delete](#delete), [Lock](#lock), [Unlock](#unlock), [Sorting](#sorting), [Formats](#formats), [Column Analysis](#column-analysis)\n  - [Menu Functions Depending on Browser Dimensions](#menu-functions-depending-on-browser-dimensions)\n- [For Developers](#for-developers)\n  - [Cloning](#cloning)\n  - [Running Tests](#running-tests)\n  - [Linting](#linting)\n  - [Formatting JS](#formatting-js)\n  - [Docker Development](#docker-development)\n- [Global State/Data Storage](https://github.com/man-group/dtale/blob/master/docs/GLOBAL_STATE.md)\n- [Startup Behavior](#startup-behavior)\n- [Documentation](#documentation)\n- [Requirements](#requirements)\n- [Acknowledgements](#acknowledgements)\n- [License](#license)\n\n## Getting Started\n\n|PyCharm|jupyter|\n|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/gifs/dtale_demo_mini.gif)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/gifs/dtale_ipython.gif)|\n\nInstalling the egg\n\n```bash\n# install dtale egg (important to use the \"--upgrade\" every time you install so it will grab the latest version)\n$ pip install --upgrade dtale\n```\nNow you will have the ability to use D-Tale from the command-line or within a python-enabled terminal\n\n### Python Terminal\nThis comes courtesy of PyCharm\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Python_Terminal.png)\nFeel free to invoke `python` or `ipython` directly and use the commands in the screenshot above and it should work\n\n#### Issues With Windows Firewall\n\nIf you run into issues with viewing D-Tale in your browser on Windows please try making Python public under \"Allowed Apps\" in your Firewall configuration.  Here is a nice article:\n[How to Allow Apps to Communicate Through the Windows Firewall](https://www.howtogeek.com/howto/uncategorized/how-to-create-exceptions-in-windows-vista-firewall/)\n\n#### Additional functions available programatically\n```python\nimport dtale\nimport pandas as pd\n\ndf = pd.DataFrame([dict(a=1,b=2,c=3)])\n\n# Assigning a reference to a running D-Tale process\nd = dtale.show(df)\n\n# Accessing data associated with D-Tale process\ntmp = d.data.copy()\ntmp['d'] = 4\n\n# Altering data associated with D-Tale process\n# FYI: this will clear any front-end settings you have at the time for this process (filter, sorts, formatting)\nd.data = tmp\n\n# Shutting down D-Tale process\nd.kill()\n\n# using Python's `webbrowser` package it will try and open your server's default browser to this process\nd.open_browser()\n\n# There is also some helpful metadata about the process\nd._data_id  # the process's data identifier\nd._url  # the url to access the process\n\nd2 = dtale.get_instance(d._data_id)  # returns a new reference to the instance running at that data_id\n\ndtale.instances()  # prints a list of all ids & urls of running D-Tale sessions\n\n```\n\n#### Duplicate data check\nTo help guard against users loading the same data to D-Tale multiple times and thus eating up precious memory, we have a loose check for duplicate input data.  The check runs the following:\n * Are row & column count the same as a previously loaded piece of data?\n * Are the names and order of columns the same as a previously loaded piece of data?\n\nIf both these conditions are true then you will be presented with an error and a link to the previously loaded data.  Here is an example of how the interaction looks:\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Duplicate_data.png)\n\n\n### Jupyter Notebook\nWithin any jupyter (ipython) notebook executing a cell like this will display a small instance of D-Tale in the output cell.  Here are some examples:\n\n|`dtale.show`|assignment|instance|\n|:------:|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/ipython1.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/ipython2.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/ipython3.png)|\n\nIf you are running ipython<=5.0 then you also have the ability to adjust the size of your output cell for the most recent instance displayed:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/ipython_adjust.png)\n\nOne thing of note is that a lot of the modal popups you see in the standard browser version will now open separate browser windows for spacial convienence:\n\n|Column Menus|Correlations|Describe|Column Analysis|Instances|\n|:------:|:------:|:------:|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Column_menu.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/correlations_popup.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/describe_popup.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/histogram_popup.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/instances_popup.png)|\n\n### JupyterHub w/ Kubernetes\n\nPlease read this [post](https://github.com/man-group/dtale/blob/master/docs/JUPYTERHUB_KUBERNETES.md)\n\n### Google Colab & Kaggle\n\nThese are hosted notebook sites and thanks to the work of [flask_ngrok](https://github.com/gstaff/flask-ngrok) users can run D-Tale within their notebooks.\n\n**DISCLAIMER:** It is import that you set `USE_NGROK` to true when using D-Tale within these two services.  Here is an example:\n\n```\nimport pandas as pd\n\nimport dtale\nimport dtale.app as dtale_app\n\ndtale_app.USE_NGROK = True\n\ndtale.show(pd.DataFrame([1,2,3]))\n```\n\nHere are some video tutorials of each:\n\n|Service|Tutorial|Addtl Notes|\n|:------:|:------:|:------:|\n|Google Colab|[![](http://img.youtube.com/vi/pOYl2M1clIw/0.jpg)](http://www.youtube.com/watch?v=pOYl2M1clIw \"Google Colab\")||\n|Kaggle|[![](http://img.youtube.com/vi/8Il-2HHs2Mg/0.jpg)](http://www.youtube.com/watch?v=8Il-2HHs2Mg \"Kaggle\")|make sure you switch the \"Internet\" toggle to \"On\" under settings of your notebook so you can install the egg from pip|\n\n### R with Reticulate\n\nI was able to get D-Tale running in R using reticulate. Here is an example:\n\n```\nlibrary('reticulate')\ndtale <- import('dtale')\ndf <- read.csv('https://vincentarelbundock.github.io/Rdatasets/csv/boot/acme.csv')\ndtale$show(df, subprocess=FALSE, open_browser=TRUE)\n```\n\nNow the problem with doing this is that D-Tale is not running as a subprocess so it will block your R console and you'll lose out the following functions:\n - manipulating the state of your data from your R console\n - adding more data to D-Tale\n\n`open_browser=TRUE` isn't required and won't work if you don't have a default browser installed on your machine. If you don't use that parameter simply copy & paste the URL that gets printed to your console in the browser of your choice.\n\nI'm going to do some more digging on why R doesn't seem to like using python subprocesses (not sure if it something with how reticulate manages the state of python) and post any findings to this thread.\n\nHere's some helpful links for getting setup:\n\nreticulate\n\ninstalling python packages\n\n### Command-line\nBase CLI options (run `dtale --help` to see all options available)\n\n|Prop     |Description|\n|:--------|:-----------|\n|`--host` |the name of the host you would like to use (most likely not needed since `socket.gethostname()` should figure this out)|\n|`--port` |the port you would like to assign to your D-Tale instance|\n|`--name` |an optional name you can assign to your D-Tale instance (this will be displayed in the `<title>` & Instances popup)|\n|`--debug`|turn on Flask's \"debug\" mode for your D-Tale instance|\n|`--no-reaper`|flag to turn off auto-reaping subprocess (kill D-Tale instances after an hour of inactivity), good for long-running displays |\n|`--open-browser`|flag to automatically open up your server's default browser to your D-Tale instance|\n|`--force`|flag to force D-Tale to try an kill any pre-existing process at the port you've specified so it can use it|\n\nLoading data from [**arctic**(high performance datastore for pandas dataframes)](https://github.com/man-group/arctic) (this requires either installing **arctic** or **dtale[arctic]**)\n```bash\ndtale --arctic-host mongodb://localhost:27027 --arctic-library jdoe.my_lib --arctic-node my_node --arctic-start 20130101 --arctic-end 20161231\n```\nLoading data from **CSV**\n```bash\ndtale --csv-path /home/jdoe/my_csv.csv --csv-parse_dates date\n```\nLoading data from **JSON**\n```bash\ndtale --json-path /home/jdoe/my_json.json --json-parse_dates date\n```\nor\n```bash\ndtale --json-path http://json-endpoint --json-parse_dates date\n```\nLoading data from a **Custom** loader\n- Using the DTALE_CLI_LOADERS environment variable, specify a path to a location containing some python modules\n- Any python module containing the global variables LOADER_KEY & LOADER_PROPS will be picked up as a custom loader\n  - LOADER_KEY: the key that will be associated with your loader.  By default you are given **arctic** & **csv** (if you use one of these are your key it will override these)\n  - LOADER_PROPS: the individual props available to be specified.\n    - For example, with arctic we have host, library, node, start & end.\n    - If you leave this property as an empty list your loader will be treated as a flag.  For example, instead of using all the arctic properties we would simply specify `--arctic` (this wouldn't work well in arctic's case since it depends on all those properties)\n- You will also need to specify a function with the following signature `def find_loader(kwargs)` which returns a function that returns a dataframe or `None`\n- Here is an example of a custom loader:\n```python\nfrom dtale.cli.clickutils import get_loader_options\n\n'''\n  IMPORTANT!!! This global variable is required for building any customized CLI loader.\n  When find loaders on startup it will search for any modules containing the global variable LOADER_KEY.\n'''\nLOADER_KEY = 'testdata'\nLOADER_PROPS = ['rows', 'columns']\n\n\ndef test_data(rows, columns):\n    import pandas as pd\n    import numpy as np\n    import random\n    from past.utils import old_div\n    from pandas.tseries.offsets import Day\n    from dtale.utils import dict_merge\n    import string\n\n    now = pd.Timestamp(pd.Timestamp('now').date())\n    dates = pd.date_range(now - Day(364), now)\n    num_of_securities = max(old_div(rows, len(dates)), 1)  # always have at least one security\n    securities = [\n        dict(security_id=100000 + sec_id, int_val=random.randint(1, 100000000000),\n             str_val=random.choice(string.ascii_letters) * 5)\n        for sec_id in range(num_of_securities)\n    ]\n    data = pd.concat([\n        pd.DataFrame([dict_merge(dict(date=date), sd) for sd in securities])\n        for date in dates\n    ], ignore_index=True)[['date', 'security_id', 'int_val', 'str_val']]\n\n    col_names = ['Col{}'.format(c) for c in range(columns)]\n    return pd.concat([data, pd.DataFrame(np.random.randn(len(data), columns), columns=col_names)], axis=1)\n\n\n# IMPORTANT!!! This function is required for building any customized CLI loader.\ndef find_loader(kwargs):\n    test_data_opts = get_loader_options(LOADER_KEY, kwargs)\n    if len([f for f in test_data_opts.values() if f]):\n        def _testdata_loader():\n            return test_data(int(test_data_opts.get('rows', 1000500)), int(test_data_opts.get('columns', 96)))\n\n        return _testdata_loader\n    return None\n```\nIn this example we simplying building a dataframe with some dummy data based on dimensions specified on the command-line:\n- `--testdata-rows`\n- `--testdata-columns`\n\nHere's how you would use this loader:\n```bash\nDTALE_CLI_LOADERS=./path_to_loaders bash -c 'dtale --testdata-rows 10 --testdata-columns 5'\n```\n\n### Accessing CLI Loaders in Notebook or Console\nI am pleased to announce that all CLI loaders will be available within notebooks & consoles.  Here are some examples:\n- `dtale.show_csv(path='test.csv', parse_dates=['date'])`\n- `dtale.show_json(path='http://json-endpoint', parse_dates=['date'])`\n- `dtale.show_json(path='test.json', parse_dates=['date'])`\n- `dtale.show_arctic(host='host', library='library', node='node', start_date='20200101', end_date='20200101')`\n\n## UI\nOnce you have kicked off your D-Tale session please copy & paste the link on the last line of output in your browser\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Browser1.png)\n\n### Dimensions/Main Menu\nThe information in the upper right-hand corner gives grid dimensions ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Info_cell.png)\n- lower-left => row count\n- upper-right => column count\n- clicking the triangle displays the menu of standard functions (click outside menu to close it)\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Info_menu_small.png)\n\n### Header\n\nThe header gives users an idea of what operations have taken place on your data (sorts, filters, hidden columns).  These values will be persisted across broswer instances.  So if you perform one of these operations and then send a link to one of your colleagues they will see the same thing :)\n\nNotice the \"X\" icon on the right of each display.  Clicking this will remove those operations.\n\nWhen performing multiple of the same operation the description will become too large to display so the display will truncate the description and if users click it they will be presented with a tooltip where you can crop individual operations.  Here are some examples:\n\n|Sorts|Filters|Hidden Columns|\n|-----|-------|--------------|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/header/sorts.PNG)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/header/filters.PNG)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/header/hidden.PNG)|\n\n### Main Menu Functions\n\n#### Describe\nView all the columns & their data types as well as individual details of each column\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe.png)\n\n|Data Type|Display|Notes|\n|--------|:------:|:------:|\n|date|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe_date.png)||\n|string|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe_string.png)|If you have less than or equal to 100 unique values they will be displayed at the bottom of your popup|\n|int|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe_int.png)|Anything with standard numeric classifications (min, max, 25%, 50%, 75%) will have a nice boxplot with the mean (if it exists) displayed as an outlier if you look closely.|\n|float|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe_float.png)||\n\n#### Outlier Detection\nWhen viewing integer & float columns in the [\"Describe\" popup](#describe) you will see in the lower right-hand corner a toggle for Uniques & Outliers.\n\n|Outliers|Filter|\n|--------|------|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/outliers.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/outlier_filter.png)|\n\nIf you click the \"Outliers\" toggle this will load the top 100 outliers in your column based on the following code snippet:\n```python\ns = df[column]\nq1 = s.quantile(0.25)\nq3 = s.quantile(0.75)\niqr = q3 - q1\niqr_lower = q1 - 1.5 * iqr\niqr_upper = q3 + 1.5 * iqr\noutliers = s[(s < iqr_lower) | (s > iqr_upper)]\n```\nIf you click on the \"Apply outlier filter\" link this will add an addtional \"outlier\" filter for this column which can be removed from the [header](#header) or the [custom filter](#custom-filter) shown in picture above to the right.\n\n#### Custom Filter\nApply a custom pandas `query` to your data (link to pandas documentation included in popup)  \n\n|Editing|Result|\n|--------|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Filter_apply.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Post_filter.png)|\n\nYou can also see any outlier or column filters you've applied (which will be included in addition to your custom query) and remove them if you'd like.\n\nContext Variables are user-defined values passed in via the `context_variables` argument to dtale.show(); they can be referenced in filters by prefixing the variable name with '@'.\n\nFor example, here is how you can use context variables in a pandas query:\n```python\nimport pandas as pd\n\ndf = pd.DataFrame([\n  dict(name='Joe', age=7),\n  dict(name='Bob', age=23),\n  dict(name='Ann', age=45),\n  dict(name='Cat', age=88),\n])\ntwo_oldest_ages = df['age'].nlargest(2)\ndf.query('age in @two_oldest_ages')\n```\nAnd here is how you would pass that context variable to D-Tale: `dtale.show(df, context_variables=dict(two_oldest_ages=two_oldest_ages))`\n\nHere's some nice documentation on the performance of [pandas queries](https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html#pandas-eval-performance)\n\n#### Building Columns\n\n[![](http://img.youtube.com/vi/G6wNS9-lG04/0.jpg)](http://www.youtube.com/watch?v=G6wNS9-lG04 \"Build Columns in D-Tale\")\n\nThis video shows you how to build the following:\n - Numeric: adding/subtracting two columns or columns with static values\n - Bins: bucketing values using pandas cut & qcut as well as assigning custom labels\n - Dates: retrieving date properties (hour, weekday, month...) as well as conversions (month end)\n - Random: columns of data type (int, float, string & date) populated with random uniformly distributed values.\n  - Type Conversion: switch columns from one data type to another, fun. :smile:\n\n#### Summarize Data\n\nThis is very powerful functionality which allows users to create a new data from currently loaded data.  The operations currently available are:\n- **Aggregation**: consolidate data by running different aggregations on columns by a specific index\n- **Pivot**: this is simple wrapper around [pandas.Dataframe.pivot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot.html) and [pandas.pivot_table](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html)\n- **Transpose**: transpose your data on a index (be careful dataframes can get very wide if your index has many unique values)\n\n|Function|Data|\n|:------:|:------:|\n|No Reshaping|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/original_data.png)|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/agg_col.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/agg_col_data.png)|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/agg_func.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/agg_func_data.png)|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/pivot.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/pivot_data.png)|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/transpose.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/transpose_data.png)|\n\n[![](http://img.youtube.com/vi/fYsxogXKZ2c/0.jpg)](http://www.youtube.com/watch?v=fYsxogXKZ2c \"Reshaping Tutorial\")\n\n#### Charts\nBuild custom charts based off your data(powered by [plotly/dash](https://github.com/plotly/dash)).\n \n - The Charts will open in a tab because of the fact there is so much functionality offered there you'll probably want to be able to reference the main grid data in the original tab\n - To build a chart you must pick a value for X & Y inputs which effectively drive what data is along the X & Y axes\n   - If you are working with a 3-Dimensional chart (heatmap, 3D Scatter, Surface) you'll need to enter a value for the Z axis as well\n - Once you have entered all the required axes a chart will be built\n - If your data along the x-axis (or combination of x & y in the case of 3D charts) has duplicates you have three options:\n   - Specify a group, which will create series for each group\n   - Specify an aggregation, you can choose from one of the following: Count, First, Last, Mean, Median, Minimum, Maximum, Standard Deviation, Variance, Mean Absolute Deviation, Product of All Items, Sum, Rolling\n     - Specifying a \"Rolling\" aggregation will also require a Window & a Computation (Correlation, Count, Covariance, Kurtosis, Maximum, Mean, Median, Minimum, Skew, Standard Deviation, Sum or Variance)\n     - For heatmaps you will also have access to the \"Correlation\" aggregation since viewing correlation matrices in heatmaps is very useful.  This aggregation is not supported elsewhere\n   - Specify both a group & an aggregation\n - You now have the ability to toggle between different chart types: line, bar, pie, wordcloud, heatmap, 3D scatter & surface\n - If you have specified a group then you have the ability between showing all series in one chart and breaking each series out into its own chart \"Chart per Group\"\n\nHere are some examples:\n\n|Chart Type|Chart|Chart per Group|\n|:------:|:------:|:------:|\n|line|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/line.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/line_pg.png)|\n|bar|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/bar.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/bar_pg.png)|\n|stacked|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/stacked.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/stacked_pg.png)|\n|pie|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/pie.png)||\n|wordcloud|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/wordcloud.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/wordcloud_pg.png)|\n|heatmap|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/heatmap.png)||\n|3D scatter|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/3d_scatter.png)||\n|surface|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/surface.png)||\n|Maps (Scatter GEO)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/scattergeo.png)||\n|Maps (Choropleth)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/choropleth.png)||\n\nY-Axis Toggling\n\nUsers now have the ability to toggle between 3 different behaviors for their y-axis display:\n- *Default*: selecting this option will use the default behavior that comes with plotly for your chart's y-axis\n- *Single*: this allows users to set the range of all series in your chart to be on the same basis as well as making that basis (min/max) editable\n- *Multi*: this allows users to give each series its own y-axis and making that axis' range editable\n\nHere's a quick tutorial: [![](http://img.youtube.com/vi/asblF-rAACY/0.jpg)](http://www.youtube.com/watch?v=asblF-rAACY \"Y-Axis Toggling\")\n\nAnd some screenshots:\n\n|Default|Single|Multi|\n|:------:|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/axis_toggle/default.PNG)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/axis_toggle/single.PNG)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/axis_toggle/multi.PNG)|\n\nWith a bar chart that only has a single Y-Axis you have the ability to sort the bars based on the values for the Y-Axis\n\n|Pre-sort|Post-sort|\n|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/bar_presort.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/bar_postsort.png)|\n\n**Popup Charts**\n\nViewing multiple charts at once and want to separate one out into its own window or simply move one off to the side so you can work on building another for comparison?  Well now you can by clicking the \"Popup\" button :smile:\n\n**Copy Link**\n\nWant to send what you're looking at to someone else?  Simply click the \"Copy Link\" button and it will save a pre-populated chart URL into your clipboard. As long as your D-Tale process is still running when that link is opened you will see your original chart.\n\n**Exporting Charts**\n\nYou can now export your dash charts (with the exception of Wordclouds) to static HTML files which can be emailed to others or saved down to be viewed at a later time.  The best part is that all of the javascript for plotly is embedded in these files so the nice zooming, panning, etc is still available! :boom:\n\n**Exporting CSV**\n\nI've been asked about being able to export the data that is contained within your chart to a CSV for further analysis in tools like Excel.  This button makes that possible.\n\n**OFFLINE CHARTS**\n\nWant to run D-Tale in a jupyter notebook and build a chart that will still be displayed even after your D-Tale process has shutdown?  Now you can!  Here's an example code snippet show how to use it:\n\n```\nimport dtale\n\ndef test_data():\n    import random\n    import pandas as pd\n    import numpy as np\n\n    df = pd.DataFrame([\n        dict(x=i, y=i % 2)\n        for i in range(30)\n    ])\n    rand_data = pd.DataFrame(np.random.randn(len(df), 5), columns=['z{}'.format(j) for j in range(5)])\n    return pd.concat([df, rand_data], axis=1)\n\nd = dtale.show(test_data())\nd.offline_chart(chart_type='bar', x='x', y='z3', agg='sum')\n```\n[![](http://img.youtube.com/vi/DseSmc3fZvc/0.jpg)](http://www.youtube.com/watch?v=DseSmc3fZvc \"Offline Charts Tutorial\")\n\n**Pro Tip: If generating offline charts in jupyter notebooks and you run out of memory please add the following to your command-line when starting jupyter**\n\n`--NotebookApp.iopub_data_rate_limit=1.0e10`\n\n\n**Disclaimer: Long Running Chart Requests**\n\nIf you choose to build a chart that requires a lot of computational resources then it will take some time to run.  Based on the way Flask & plotly/dash interact this will block you from performing any other request until it completes.  There are two courses of action in this situation:\n\n1) Restart your jupyter notebook kernel or python console\n2) Open a new D-Tale session on a different port than the current session.  You can do that with the following command: `dtale.show(df, port=[any open port], force=True)`\n\nIf you miss the legacy (non-plotly/dash) charts, not to worry!  They are still available from the link in the upper-right corner, but on for a limited time...\nHere is the documentation for those: [Legacy Charts](https://github.com/man-group/dtale/blob/master/docs/LEGACY_CHARTS.md)\n\n**Your Feedback is Valuable**\n\nThis is a very powerful feature with many more features that could be offered (linked subplots, different statistical aggregations, etc...) so please submit issues :)\n\n#### Coverage (Deprecated)\n\nIf you have watched the video within the [Man Institute](https://www.man.com/maninstitute/d-tale) blog post you'll notice that there is a \"Coverage\" popup.  This was deprecated with the creation of the \"Charts\" page.  You can create the same coverage chart in that video by choosing the following options in the \"Charts\" page:\n- Type: **Line**\n- X: **date**\n- Y: **security_id**\n- Aggregation: **Count** or **Unique Count**\n\n#### Correlations\nShows a pearson correlation matrix of all numeric columns against all other numeric columns\n  - By default, it will show a grid of pearson correlations (filtering available by using drop-down see 2nd table of screenshots)\n  - If you have a date-type column, you can click an individual cell and see a timeseries of pearson correlations for that column combination\n    - Currently if you have multiple date-type columns you will have the ability to toggle between them by way of a drop-down\n  - Furthermore, you can click on individual points in the timeseries to view the scatter plot of the points going into that correlation\n\n|Matrix|Timeseries|Scatter|\n|------|----------|-------|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_ts.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_scatter.png)|\n\n|Col1 Filtered|Col2 Filtered|Col1 & Col2 Filtered|\n|------|----------|-------|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_col1.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_col2.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_both.png)|\n\nWhen the data being viewed in D-Tale has date or timestamp columns but for each date/timestamp vlaue there is only one row of data the behavior of the Correlations popup is a little different\n  - Instead of a timeseries correlation chart the user is given a rolling correlation chart which can have the window (default: 10) altered\n  - The scatter chart will be created when a user clicks on a point in the rollign correlation chart.  The data displayed in the scatter will be for the ranges of dates involved in the rolling correlation for that date.\n\n|Data|Correlations|\n|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/rolling_corr_data.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/rolling_corr.png)|\n\n#### Heat Map\nThis will hide any non-float or non-int columns (with the exception of the index on the right) and apply a color to the background of each cell.\n\n  - Each float is renormalized to be a value between 0 and 1.0\n  - You have two options for the renormalization\n    - **By Col**: each value is calculated based on the min/max of its column\n    - **Overall**: each value is caluclated by the overall min/max of all the non-hidden float/int columns in the dataset\n  - Each renormalized value is passed to a color scale of red(0) - yellow(0.5) - green(1.0)\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Heatmap.png)\n\nTurn off Heat Map by clicking menu option you previously selected one more time\n\n#### Highlight Dtypes\nThis is a quick way to check and see if your data has been categorized correctly.  By clicking this menu option it will assign a specific background color to each column of a specific data type.\n\n|category|timedelta|float|int|date|string|bool|\n|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n|purple|orange|green|light blue|pink|white|yellow\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/highlight_dtypes.png)\n\n\n#### Code Exports\n*Code Exports* are small snippets of code representing the current state of the grid you're viewing including things like:\n - columns built\n - filtering\n - sorting\n\nOther code exports available are:\n - Column Analysis\n - Correlations (grid, timeseries chart & scatter chart)\n - Describe\n - Charts built using the Chart Builder\n\n [![](http://img.youtube.com/vi/6CkKgpv3d6I/0.jpg)](http://www.youtube.com/watch?v=6CkKgpv3d6I \"Code Export Tutorial\")\n\n|Type|Code Export|\n|:------:|:------:|\n|Main Grid|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/main.png)|\n|Histogram|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/histogram.png)|\n|Describe|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/describe.png)|\n|Correlation Grid|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/main.png)|\n|Correlation Timeseries|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/corr_ts.png)|\n|Correlation Scatter|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/corr_scatter.png)|\n|Charts|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/charts.png)|\n\n\n#### Instances\nThis will give you information about other D-Tale instances are running under your current Python process.\n\nFor example, if you ran the following script:\n```python\nimport pandas as pd\nimport dtale\n\ndtale.show(pd.DataFrame([dict(foo=1, bar=2, biz=3, baz=4, snoopy_D_O_double_gizzle=5)]))\ndtale.show(pd.DataFrame([\n    dict(a=1, b=2, c=3, d=4),\n    dict(a=2, b=3, c=4, d=5),\n    dict(a=3, b=4, c=5, d=6),\n    dict(a=4, b=5, c=6, d=7)\n]))\ndtale.show(pd.DataFrame([range(6), range(6), range(6), range(6), range(6), range(6)]), name=\"foo\")\n```\nThis will make the **Instances** button available in all 3 of these D-Tale instances. Clicking that button while in the first instance invoked above will give you this popup:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Instances.png)\n\nThe grid above contains the following information:\n  - Process: timestamp when the process was started along with the name (if specified in `dtale.show()`)\n  - Rows: number of rows\n  - Columns: number of columns\n  - Column Names: comma-separated string of column names (only first 30 characters, hover for full listing)\n  - Preview: this button is available any of the non-current instances.  Clicking this will bring up left-most 5X5 grid information for that instance\n  - The row highlighted in green signifys the current D-Tale instance\n  - Any other row can be clicked to switch to that D-Tale instance\n\nHere is an example of clicking the \"Preview\" button:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Instances_preview.png)\n\n#### About\nThis will give you information about what version of D-Tale you're running as well as if its out of date to whats on PyPi.\n\n|Up To Date|Out Of Date|\n|--------|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/About-up-to-date.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/About-out-of-date.png)|\n\n#### Resize\nMostly a fail-safe in the event that your columns are no longer lining up. Click this and should fix that\n\n#### Shutdown\nPretty self-explanatory, kills your D-Tale session (there is also an auto-kill process that will kill your D-Tale after an hour of inactivity)\n\n### Column Menu Functions\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Col_menu.png)\n\n#### Filtering\n\n[![](http://img.youtube.com/vi/8zo5ZiI1Yzo/0.jpg)](http://www.youtube.com/watch?v=8zo5ZiI1Yzo \"Column Filtering\")\n\nThese interactive filters come in 3 different types: String, Numeric & Date.  Note that you will still have the ability to apply custom filters from the \"Filter\" popup on the main menu, but it will get applied in addition to any column filters.\n\n|Type|Filter|Data Types|Features|\n|----|------|----------|--------|\n|String|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/filters/string.PNG)|strings & booleans|The ability to select multiple values based on what exists in the column. Notice the \"Show Missing Only\" toggle, this will only show up if your column has nan values|\n|Date|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/filters/dates.PNG)|dates|Specify a range of dates to filter on based on start & end inputs|\n|Numeric|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/filters/numeric.PNG)|ints & floats|For integers the \"=\" will be similar to strings where you can select multiple values based on what exists in the column.  You also have access to other operands: <,>,<=,>=,() - \"Range exclusve\", [] - \"Range inclusive\".|\n\n#### Moving Columns\n\n[![](http://img.youtube.com/vi/We4TH477rRs/0.jpg)](http://www.youtube.com/watch?v=We4TH477rRs \"Moving Columns in D-Tale\")\n\nAll column movements are saved on the server so refreshing your browser won't lose them :ok_hand:\n\n#### Hiding Columns\n\n[![](http://img.youtube.com/vi/ryZT2Lk_YaA/0.jpg)](http://www.youtube.com/watch?v=ryZT2Lk_YaA \"Hide/Unhide Columns in D-Tale\")\n\nAll column movements are saved on the server so refreshing your browser won't lose them :ok_hand:\n\n#### Delete\n\nAs simple as it sounds, click this button to delete this column from your dataframe.  (Warning: not un-doable!)\n\n#### Lock\nAdds your column to \"locked\" columns\n  - \"locked\" means that if you scroll horizontally these columns will stay pinned to the right-hand side\n  - this is handy when you want to keep track of which date or security_id you're looking at\n  - by default, any index columns on the data passed to D-Tale will be locked\n\n#### Unlock\nRemoved column from \"locked\" columns\n\n#### Sorting\nApplies/removes sorting (Ascending/Descending/Clear) to the column selected\n  \n*Important*: as you add sorts they sort added will be added to the end of the multi-sort.  For example:\n\n| Action        | Sort           |\n| ------------- |:--------------:|\n| click \"a\"     |                |\n| sort asc      | a (asc)        |\n| click \"b\"     | a (asc)        |\n| sort desc     | a (asc), b(desc)|\n| click \"a\"     | a (asc), b(desc)|\n| sort None     | b(desc)|\n| sort desc     | b(desc), a(desc)|\n| click \"X\" on sort display | |\n\n#### Formats\nApply simple formats to numeric values in your grid\n\n|Type|Editing|Result|\n|--------|:------:|:------:|\n|Numeric|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Formatting_apply.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Post_formatting.png)|\n|Date|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Formatting_date_apply.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Post_date_formatting.png)|\n|String|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Formatting_string_apply.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Post_string_formatting.png)|\n\nHere's a grid of all the formats available with -123456.789 as input:\n  \n| Format        | Output         |\n| ------------- |:--------------:|\n| Precision (6) | -123456.789000 |\n| Thousands Sep | -123,456.789   |\n| Abbreviate    | -123k          |\n| Exponent      | -1e+5          |\n| BPS           | -1234567890BPS |\n| Red Negatives | <span style=\"color: red;\">-123457</span>|\n\n#### Column Analysis\nBased on the data type of a column different charts will be shown.\n\n| Chart         | Data Types     | Sample |\n|---------------|----------------|--------|\n| Histogram     | Float, Int |![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/histogram.PNG)|\n| Value Counts  | Int, String, Bool, Date, Category|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/value_counts.PNG)|\n| Category      | Float   |![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/category.PNG)|\n\n\n**Histogram** can be displayed in any number of bins (default: 20), simply type a new integer value in the bins input\n\n**Value Count** by default, show the top 100 values ranked by frequency.  If you would like to show the least frequent values simply make your number negative (-10 => 10 least frequent value)\n\n**Value Count w/ Ordinal** you can also apply an ordinal to your **Value Count** chart by selecting a column (of type int or float) and applying an aggregation (default: sum) to it (sum, mean, etc...) this column will be grouped by the column you're analyzing and the value produced by the aggregation will be used to sort your bars and also displayed in a line.  Here's an example:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/value_counts_ordinal.PNG\n)\n\n**Category (Category Breakdown)** when viewing float columns you can also see them broken down by a categorical column (string, date, int, etc...).  This means that when you select a category column this will then display the frequency of each category in a line as well as bars based on the float column you're analyzing grouped by that category and computed by your aggregation (default: mean).\n\n### Menu Functions Depending on Browser Dimensions\nDepending on the dimensions of your browser window the following buttons will not open modals, but rather separate browser windows:  Correlations, Describe & Instances (see images from [Jupyter Notebook](#jupyter-notebook), also Charts will always open in a separate browser window)\n\n## For Developers\n\n### Cloning\n\nClone the code (git clone ssh://git@github.com:manahl/dtale.git), then start the backend server:\n\n```bash\n$ git clone ssh://git@github.com:manahl/dtale.git\n# install the dependencies\n$ python setup.py develop\n# start the server\n$ python dtale --csv-path /home/jdoe/my_csv.csv --csv-parse_dates date\n```\n\nYou can also run dtale from PyDev directly.\n\nYou will also want to import javascript dependencies and build the source:\n\n``` bash\n$ npm install\n# 1) a persistent server that serves the latest JS:\n$ npm run watch\n# 2) or one-off build:\n$ npm run build\n```\n\n### Running tests\n\nThe usual npm test command works:\n\n```\n$ npm test\n```\n\nYou can run individual test files:\n\n```\n$ TEST=static/__tests__/dtale/DataViewer-base-test.jsx npm run test-file\n```\n\n### Linting\n\nYou can lint all the JS and CSS to confirm there's nothing obviously wrong with\nit:\n\n``` bash\n$ npm run lint -s\n```\n\nYou can also lint individual JS files:\n\n``` bash\n$ npm run lint-js-file -s -- static/dtale/DataViewer.jsx\n```\n\n### Formatting JS\n\nYou can auto-format code as follows:\n\n``` bash\n$ npm run format\n```\n\n### Docker Development\n\nYou can build python 27-3 & run D-Tale as follows:\n```bash\n$ yarn run build\n$ docker-compose build dtale_2_7\n$ docker run -it --network host dtale_2_7:latest\n$ python\n>>> import pandas as pd\n>>> df = pd.DataFrame([dict(a=1,b=2,c=3)])\n>>> import dtale\n>>> dtale.show(df)\n```\nThen view your D-Tale instance in your browser using the link that gets printed\n\nYou can build python 36-1 & run D-Tale as follows:\n```bash\n$ yarn run build\n$ docker-compose build dtale_3_6\n$ docker run -it --network host dtale_3_6:latest\n$ python\n>>> import pandas as pd\n>>> df = pd.DataFrame([dict(a=1,b=2,c=3)])\n>>> import dtale\n>>> dtale.show(df)\n```\nThen view your D-Tale instance in your browser using the link that gets printed\n\n\n## Global State/Data Storage\n\nIf D-Tale is running in an environment with multiple python processes (ex: on a web server running [gunicorn](https://github.com/benoitc/gunicorn)) it will most likely encounter issues with inconsistent state.  Developers can fix this by configuring the system D-Tale uses for storing data.  Detailed documentation is available here: [Data Storage and managing Global State](https://github.com/man-group/dtale/blob/master/docs/GLOBAL_STATE.md)\n\n\n## Startup Behavior\n\nHere's a little background on how the `dtale.show()` function works:\n - by default it will look for ports between 40000 & 49000, but you can change that range by specifying the environment variables DTALE_MIN_PORT & DTALE_MAX_PORT\n - think of sessions as python consoles or jupyter notebooks\n\n1) Session 1 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40000|1|http://localhost:40000/dtale/main/1|\n\n2) Session 1 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40000|1,2|http://localhost:40000/dtale/main/[1,2]|\n\n2) Session 2 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40000|1,2|http://localhost:40000/dtale/main/[1,2]|\n|2|40001|1|http://localhost:40001/dtale/main/1|\n\n3) Session 1 executes `dtale.show(df, port=40001, force=True)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|1,2,3|http://localhost:40001/dtale/main/[1,2,3]|\n\n4) Session 3 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|1,2,3|http://localhost:40001/dtale/main/[1,2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n\n5) Session 2 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|1,2,3|http://localhost:40001/dtale/main/[1,2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n|2|40002|1|http://localhost:40002/dtale/main/1|\n\n6) Session 4 executes `dtale.show(df, port=8080)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|1,2,3|http://localhost:40001/dtale/main/[1,2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n|2|40002|1|http://localhost:40002/dtale/main/1|\n|4|8080|1|http://localhost:8080/dtale/main/1|\n\n7) Session 1 executes `dtale.get_instance(1).kill()` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|2,3|http://localhost:40001/dtale/main/[2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n|2|40002|1|http://localhost:40002/dtale/main/1|\n|4|8080|1|http://localhost:8080/dtale/main/1|\n\n7) Session 5 sets DTALE_MIN_RANGE to 30000 and DTALE_MAX_RANGE 39000 and executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data ID(s)|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|2,3|http://localhost:40001/dtale/main/[2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n|2|40002|1|http://localhost:40002/dtale/main/1|\n|4|8080|1|http://localhost:8080/dtale/main/1|\n|5|30000|1|http://localhost:30000/dtale/main/1|\n\n## Documentation\n\nHave a look at the [detailed documentation](https://dtale.readthedocs.io).\n\n## Requirements\n\nD-Tale works with:\n  \n  * Back-end\n    * arctic [extra]\n    * dash\n    * dash_daq\n    * Flask\n    * Flask-Compress\n    * Pandas\n    * plotly\n    * scipy\n    * six\n  * Front-end\n    * react-virtualized\n    * chart.js\n\n## Acknowledgements\n\nD-Tale has been under active development at [Man Numeric](http://www.numeric.com/) since 2019.\n\nOriginal concept and implementation: [Andrew Schonfeld](https://github.com/aschonfeld)\n\nContributors:\n\n * [Phillip Dupuis](https://github.com/phillipdupuis)\n * [Fernando Saravia Rajal](https://github.com/fersarr)\n * [Dominik Christ](https://github.com/DominikMChrist)\n * [Reza Moshksar](https://github.com/reza1615)\n * [Chris Boddy](https://github.com/cboddy)\n * [Jason Holden](https://github.com/jasonkholden)\n * [Tom Taylor](https://github.com/TomTaylorLondon)\n * [Wilfred Hughes](https://github.com/Wilfred)\n * Mike Kelly\n * [Vincent Riemer](https://github.com/vincentriemer)\n * [Youssef Habchi](http://youssef-habchi.com/) - title font\n * ... and many others ...\n\nContributions welcome!\n\n## License\n\nD-Tale is licensed under the GNU LGPL v2.1.  A copy of which is included in [LICENSE](LICENSE)\n"}, "dvc": {"file_name": "iterative/dvc/README.rst", "raw_text": "|Banner|\n\n`Website <https://dvc.org>`_\n\u2022 `Docs <https://dvc.org/doc>`_\n\u2022 `Blog <http://blog.dataversioncontrol.com>`_\n\u2022 `Twitter <https://twitter.com/DVCorg>`_\n\u2022 `Chat (Community & Support) <https://dvc.org/chat>`_\n\u2022 `Tutorial <https://dvc.org/doc/get-started>`_\n\u2022 `Mailing List <https://sweedom.us10.list-manage.com/subscribe/post?u=a08bf93caae4063c4e6a351f6&id=24c0ecc49a>`_\n\n|Release| |CI| |Maintainability| |Coverage| |Donate| |DOI|\n\n|Snap| |Choco| |Brew| |Conda| |PyPI| |Packages|\n\n|\n\n**Data Version Control** or **DVC** is an **open-source** tool for data science and machine\nlearning projects. Key features:\n\n#. Simple **command line** Git-like experience. Does not require installing and maintaining\n   any databases. Does not depend on any proprietary online services.\n\n#. Management and versioning of **datasets** and **machine learning models**. Data is saved in\n   S3, Google cloud, Azure, Alibaba cloud, SSH server, HDFS, or even local HDD RAID.\n\n#. Makes projects **reproducible** and **shareable**; helping to answer questions about how\n   a model was built.\n\n#. Helps manage experiments with Git tags/branches and **metrics** tracking.\n\n**DVC** aims to replace spreadsheet and document sharing tools (such as Excel or Google Docs)\nwhich are being used frequently as both knowledge repositories and team ledgers.\nDVC also replaces both ad-hoc scripts to track, move, and deploy different model versions;\nas well as ad-hoc data file suffixes and prefixes.\n\n.. contents:: **Contents**\n  :backlinks: none\n\nHow DVC works\n=============\n\nWe encourage you to read our `Get Started <https://dvc.org/doc/get-started>`_ guide to better understand what DVC\nis and how it can fit your scenarios.\n\nThe easiest (but not perfect!) *analogy* to describe it: DVC is Git (or Git-LFS to be precise) & Makefiles\nmade right and tailored specifically for ML and Data Science scenarios.\n\n#. ``Git/Git-LFS`` part - DVC helps store and share data artifacts and models, connecting them with a Git repository.\n#. ``Makefile``\\ s part - DVC describes how one data or model artifact was built from other data and code.\n\nDVC usually runs along with Git. Git is used as usual to store and version code (including DVC meta-files). DVC helps\nto store data and model files seamlessly out of Git, while preserving almost the same user experience as if they\nwere stored in Git itself. To store and share the data cache, DVC supports multiple remotes - any cloud (S3, Azure,\nGoogle Cloud, etc) or any on-premise network storage (via SSH, for example).\n\n|Flowchart|\n\nThe DVC pipelines (computational graph) feature connects code and data together. It is possible to explicitly\nspecify all steps required to produce a model: input dependencies including data, commands to run,\nand output information to be saved. See the quick start section below or\nthe `Get Started <https://dvc.org/doc/get-started>`_ tutorial to learn more.\n\nQuick start\n===========\n\nPlease read `Get Started <https://dvc.org/doc/get-started>`_ guide for a full version. Common workflow commands include:\n\n+-----------------------------------+-------------------------------------------------------------------+\n| Step                              | Command                                                           |\n+===================================+===================================================================+\n| Track data                        | | ``$ git add train.py``                                          |\n|                                   | | ``$ dvc add images.zip``                                        |\n+-----------------------------------+-------------------------------------------------------------------+\n| Connect code and data by commands | | ``$ dvc run -d images.zip -o images/ unzip -q images.zip``      |\n|                                   | | ``$ dvc run -d images/ -d train.py -o model.p python train.py`` |\n+-----------------------------------+-------------------------------------------------------------------+\n| Make changes and reproduce        | | ``$ vi train.py``                                               |\n|                                   | | ``$ dvc repro model.p.dvc``                                     |\n+-----------------------------------+-------------------------------------------------------------------+\n| Share code                        | | ``$ git add .``                                                 |\n|                                   | | ``$ git commit -m 'The baseline model'``                        |\n|                                   | | ``$ git push``                                                  |\n+-----------------------------------+-------------------------------------------------------------------+\n| Share data and ML models          | | ``$ dvc remote add myremote -d s3://mybucket/image_cnn``        |\n|                                   | | ``$ dvc push``                                                  |\n+-----------------------------------+-------------------------------------------------------------------+\n\nInstallation\n============\n\nThere are four options to install DVC: ``pip``, Homebrew, Conda (Anaconda) or an OS-specific package.\nFull instructions are `available here <https://dvc.org/doc/get-started/install>`_.\n\nSnap (Snapcraft/Linux)\n----------------------\n\n|Snap|\n\n.. code-block:: bash\n\n   snap install dvc --classic\n\nThis corresponds to the latest tagged release.\nAdd ``--edge`` for the latest ``master`` version.\n\nChoco (Chocolatey/Windows)\n--------------------------\n\n|Choco|\n\n.. code-block:: bash\n\n   choco install dvc\n\nBrew (Homebrew/Mac OS)\n----------------------\n\n|Brew|\n\n.. code-block:: bash\n\n   brew install dvc\n\nConda (Anaconda)\n----------------\n\n|Conda|\n\n.. code-block:: bash\n\n   conda install -c conda-forge dvc\n\nCurrently, this includes support for Python versions 2.7, 3.6 and 3.7.\n\npip (PyPI)\n----------\n\n|PyPI|\n\n.. code-block:: bash\n\n   pip install dvc\n\nDepending on the remote storage type you plan to use to keep and share your data, you might need to specify\none of the optional dependencies: ``s3``, ``gs``, ``azure``, ``oss``, ``ssh``. Or ``all`` to include them all.\nThe command should look like this: ``pip install dvc[s3]`` (in this case AWS S3 dependencies such as ``boto3``\nwill be installed automatically).\n\nTo install the development version, run:\n\n.. code-block:: bash\n\n   pip install git+git://github.com/iterative/dvc\n\nPackage\n-------\n\n|Packages|\n\nSelf-contained packages for Linux, Windows, and Mac are available. The latest version of the packages\ncan be found on the GitHub `releases page <https://github.com/iterative/dvc/releases>`_.\n\nUbuntu / Debian (deb)\n^^^^^^^^^^^^^^^^^^^^^\n.. code-block:: bash\n\n   sudo wget https://dvc.org/deb/dvc.list -O /etc/apt/sources.list.d/dvc.list\n   sudo apt-get update\n   sudo apt-get install dvc\n\nFedora / CentOS (rpm)\n^^^^^^^^^^^^^^^^^^^^^\n.. code-block:: bash\n\n   sudo wget https://dvc.org/rpm/dvc.repo -O /etc/yum.repos.d/dvc.repo\n   sudo yum update\n   sudo yum install dvc\n\nComparison to related technologies\n==================================\n\n#. `Git-annex <https://git-annex.branchable.com/>`_ - DVC uses the idea of storing the content of large files (which should\n   not be in a Git repository) in a local key-value store, and uses file hardlinks/symlinks instead of\n   copying/duplicating files.\n\n#. `Git-LFS <https://git-lfs.github.com/>`_ - DVC is compatible with any remote storage (S3, Google Cloud, Azure, SSH,\n   etc). DVC also uses reflinks or hardlinks to avoid copy operations on checkouts; thus handling large data files\n   much more efficiently.\n\n#. *Makefile* (and analogues including ad-hoc scripts) - DVC tracks dependencies (in a directed acyclic graph).\n\n#. `Workflow Management Systems <https://en.wikipedia.org/wiki/Workflow_management_system>`_ - DVC is a workflow\n   management system designed specifically to manage machine learning experiments. DVC is built on top of Git.\n\n#. `DAGsHub <https://dagshub.com/>`_ - This is a Github equivalent for DVC. Pushing Git+DVC based repositories to DAGsHub will produce in a high level project dashboard; including DVC pipelines and metrics visualizations, as well as links to any DVC-managed files present in cloud storage.\n\nContributing\n============\n\n|Maintainability| |Donate|\n\nContributions are welcome! Please see our `Contributing Guide <https://dvc.org/doc/user-guide/contributing/core>`_ for more\ndetails.\n\n.. image:: https://sourcerer.io/fame/efiop/iterative/dvc/images/0\n   :target: https://sourcerer.io/fame/efiop/iterative/dvc/links/0\n   :alt: 0\n\n.. image:: https://sourcerer.io/fame/efiop/iterative/dvc/images/1\n   :target: https://sourcerer.io/fame/efiop/iterative/dvc/links/1\n   :alt: 1\n\n.. image:: https://sourcerer.io/fame/efiop/iterative/dvc/images/2\n   :target: https://sourcerer.io/fame/efiop/iterative/dvc/links/2\n   :alt: 2\n\n.. image:: https://sourcerer.io/fame/efiop/iterative/dvc/images/3\n   :target: https://sourcerer.io/fame/efiop/iterative/dvc/links/3\n   :alt: 3\n\n.. image:: https://sourcerer.io/fame/efiop/iterative/dvc/images/4\n   :target: https://sourcerer.io/fame/efiop/iterative/dvc/links/4\n   :alt: 4\n\n.. image:: https://sourcerer.io/fame/efiop/iterative/dvc/images/5\n   :target: https://sourcerer.io/fame/efiop/iterative/dvc/links/5\n   :alt: 5\n\n.. image:: https://sourcerer.io/fame/efiop/iterative/dvc/images/6\n   :target: https://sourcerer.io/fame/efiop/iterative/dvc/links/6\n   :alt: 6\n\n.. image:: https://sourcerer.io/fame/efiop/iterative/dvc/images/7\n   :target: https://sourcerer.io/fame/efiop/iterative/dvc/links/7\n   :alt: 7\n\nMailing List\n============\n\nWant to stay up to date? Want to help improve DVC by participating in our occasional polls? Subscribe to our `mailing list <https://sweedom.us10.list-manage.com/subscribe/post?u=a08bf93caae4063c4e6a351f6&id=24c0ecc49a>`_. No spam, really low traffic.\n\nCopyright\n=========\n\nThis project is distributed under the Apache license version 2.0 (see the LICENSE file in the project root).\n\nBy submitting a pull request to this project, you agree to license your contribution under the Apache license version\n2.0 to this project.\n\nCitation\n========\n\n|DOI|\n\nIterative, *DVC: Data Version Control - Git for Data & Models* (2020)\n`DOI:10.5281/zenodo.012345 <https://doi.org/10.5281/zenodo.3677553>`_.\n\n.. |Banner| image:: https://dvc.org/img/logo-github-readme.png\n   :target: https://dvc.org\n   :alt: DVC logo\n\n.. |Release| image:: https://img.shields.io/badge/release-ok-brightgreen\n   :target: https://travis-ci.com/iterative/dvc/branches\n   :alt: Release\n\n.. |CI| image:: https://img.shields.io/travis/com/iterative/dvc/master?label=dev&logo=travis\n   :target: https://travis-ci.com/iterative/dvc/builds\n   :alt: Travis dev branch\n\n.. |Maintainability| image:: https://codeclimate.com/github/iterative/dvc/badges/gpa.svg\n   :target: https://codeclimate.com/github/iterative/dvc\n   :alt: Code Climate\n\n.. |Coverage| image:: https://codecov.io/gh/iterative/dvc/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/iterative/dvc\n   :alt: Codecov\n\n.. |Donate| image:: https://img.shields.io/badge/patreon-donate-green.svg?logo=patreon\n   :target: https://www.patreon.com/DVCorg/overview\n   :alt: Donate\n\n.. |Snap| image:: https://img.shields.io/badge/snap-install-82BEA0.svg?logo=snapcraft\n   :target: https://snapcraft.io/dvc\n   :alt: Snapcraft\n\n.. |Choco| image:: https://img.shields.io/chocolatey/v/dvc?label=choco\n   :target: https://chocolatey.org/packages/dvc\n   :alt: Chocolatey\n\n.. |Brew| image:: https://img.shields.io/homebrew/v/dvc?label=brew\n   :target: https://formulae.brew.sh/formula/dvc\n   :alt: Homebrew\n\n.. |Conda| image:: https://img.shields.io/conda/v/conda-forge/dvc.svg?label=conda&logo=conda-forge\n   :target: https://anaconda.org/conda-forge/dvc\n   :alt: Conda-forge\n\n.. |PyPI| image:: https://img.shields.io/pypi/v/dvc.svg?label=pip&logo=PyPI&logoColor=white\n   :target: https://pypi.org/project/dvc\n   :alt: PyPI\n\n.. |Packages| image:: https://img.shields.io/github/v/release/iterative/dvc?label=deb|pkg|rpm|exe&logo=GitHub\n   :target: https://github.com/iterative/dvc/releases/latest\n   :alt: deb|pkg|rpm|exe\n\n.. |DOI| image:: https://img.shields.io/badge/DOI-10.5281/zenodo.3677553-blue.svg\n   :target: https://doi.org/10.5281/zenodo.3677553\n   :alt: DOI\n\n.. |Flowchart| image:: https://dvc.org/img/flow.gif\n   :target: https://dvc.org/img/flow.gif\n   :alt: how_dvc_works\n"}, "eli5": {"file_name": "TeamHG-Memex/eli5/README.rst", "raw_text": "====\nELI5\n====\n\n.. image:: https://img.shields.io/pypi/v/eli5.svg\n   :target: https://pypi.python.org/pypi/eli5\n   :alt: PyPI Version\n\n.. image:: https://travis-ci.org/TeamHG-Memex/eli5.svg?branch=master\n   :target: https://travis-ci.org/TeamHG-Memex/eli5\n   :alt: Build Status\n\n.. image:: https://codecov.io/github/TeamHG-Memex/eli5/coverage.svg?branch=master\n   :target: https://codecov.io/github/TeamHG-Memex/eli5?branch=master\n   :alt: Code Coverage\n\n.. image:: https://readthedocs.org/projects/eli5/badge/?version=latest\n   :target: https://eli5.readthedocs.io/en/latest/?badge=latest\n   :alt: Documentation\n\n\nELI5 is a Python package which helps to debug machine learning\nclassifiers and explain their predictions.\n\n.. image:: https://raw.githubusercontent.com/TeamHG-Memex/eli5/master/docs/source/static/word-highlight.png\n   :alt: explain_prediction for text data\n\n.. image:: https://raw.githubusercontent.com/TeamHG-Memex/eli5/master/docs/source/static/gradcam-catdog.png\n   :alt: explain_prediction for image data\n\nIt provides support for the following machine learning frameworks and packages:\n\n* scikit-learn_. Currently ELI5 allows to explain weights and predictions\n  of scikit-learn linear classifiers and regressors, print decision trees\n  as text or as SVG, show feature importances and explain predictions\n  of decision trees and tree-based ensembles. ELI5 understands text\n  processing utilities from scikit-learn and can highlight text data\n  accordingly. Pipeline and FeatureUnion are supported.\n  It also allows to debug scikit-learn pipelines which contain\n  HashingVectorizer, by undoing hashing.\n\n* Keras_ - explain predictions of image classifiers via Grad-CAM visualizations.\n\n* xgboost_ - show feature importances and explain predictions of XGBClassifier,\n  XGBRegressor and xgboost.Booster.\n\n* LightGBM_ - show feature importances and explain predictions of\n  LGBMClassifier and LGBMRegressor.\n\n* CatBoost_ - show feature importances of CatBoostClassifier,\n  CatBoostRegressor and catboost.CatBoost.\n\n* lightning_ - explain weights and predictions of lightning classifiers and\n  regressors.\n\n* sklearn-crfsuite_. ELI5 allows to check weights of sklearn_crfsuite.CRF\n  models.\n\n\nELI5 also implements several algorithms for inspecting black-box models\n(see `Inspecting Black-Box Estimators`_):\n\n* TextExplainer_ allows to explain predictions\n  of any text classifier using LIME_ algorithm (Ribeiro et al., 2016).\n  There are utilities for using LIME with non-text data and arbitrary black-box\n  classifiers as well, but this feature is currently experimental.\n* `Permutation importance`_ method can be used to compute feature importances\n  for black box estimators.\n\nExplanation and formatting are separated; you can get text-based explanation\nto display in console, HTML version embeddable in an IPython notebook\nor web dashboards, a ``pandas.DataFrame`` object if you want to process\nresults further, or JSON version which allows to implement custom rendering\nand formatting on a client.\n\n.. _lightning: https://github.com/scikit-learn-contrib/lightning\n.. _scikit-learn: https://github.com/scikit-learn/scikit-learn\n.. _sklearn-crfsuite: https://github.com/TeamHG-Memex/sklearn-crfsuite\n.. _LIME: https://eli5.readthedocs.io/en/latest/blackbox/lime.html\n.. _TextExplainer: https://eli5.readthedocs.io/en/latest/tutorials/black-box-text-classifiers.html\n.. _xgboost: https://github.com/dmlc/xgboost\n.. _LightGBM: https://github.com/Microsoft/LightGBM\n.. _Catboost: https://github.com/catboost/catboost\n.. _Keras: https://keras.io/\n.. _Permutation importance: https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html\n.. _Inspecting Black-Box Estimators: https://eli5.readthedocs.io/en/latest/blackbox/index.html\n\nLicense is MIT.\n\nCheck `docs <https://eli5.readthedocs.io/>`_ for more.\n\n----\n\n.. image:: https://hyperiongray.s3.amazonaws.com/define-hg.svg\n\t:target: https://www.hyperiongray.com/?pk_campaign=github&pk_kwd=eli5\n\t:alt: define hyperiongray\n"}, "fastai": {"file_name": "fastai/fastai/README.md", "raw_text": "[![Build Status](https://dev.azure.com/fastdotai/fastai/_apis/build/status/fastai.fastai)](https://dev.azure.com/fastdotai/fastai/_build/latest?definitionId=1)\n[![pypi fastai version](https://img.shields.io/pypi/v/fastai.svg)](https://pypi.python.org/pypi/fastai)\n[![Conda fastai version](https://img.shields.io/conda/v/fastai/fastai.svg)](https://anaconda.org/fastai/fastai)\n\n[![Anaconda-Server Badge](https://anaconda.org/fastai/fastai/badges/platforms.svg)](https://anaconda.org/fastai/fastai)\n[![fastai python compatibility](https://img.shields.io/pypi/pyversions/fastai.svg)](https://pypi.python.org/pypi/fastai)\n[![fastai license](https://img.shields.io/pypi/l/fastai.svg)](https://pypi.python.org/pypi/fastai)\n\n# fastai\n\nThe fastai library simplifies training fast and accurate neural nets using modern best practices. See the [fastai website](https://docs.fast.ai) to get started. The library is based on research into deep learning best practices undertaken at [fast.ai](http://www.fast.ai), and includes \\\"out of the box\\\" support for [`vision`](https://docs.fast.ai/vision.html#vision), [`text`](https://docs.fast.ai/text.html#text), [`tabular`](https://docs.fast.ai/tabular.html#tabular), and [`collab`](https://docs.fast.ai/collab.html#collab) (collaborative filtering) models. For brief examples, see the [examples](https://github.com/fastai/fastai/tree/master/examples) folder; detailed examples are provided in the full [documentation](https://docs.fast.ai/). For instance, here's how to train an MNIST model using [resnet18](https://arxiv.org/abs/1512.03385) (from the [vision example](https://github.com/fastai/fastai/blob/master/examples/vision.ipynb)):\n\n```python\nfrom fastai.vision import *\npath = untar_data(MNIST_PATH)\ndata = image_data_from_folder(path)\nlearn = cnn_learner(data, models.resnet18, metrics=accuracy)\nlearn.fit(1)\n```\n\n## Note for [course.fast.ai](http://course.fast.ai) students\n\nThis document is written for `fastai v1`, which we use for the current version the [course.fast.ai](http://course.fast.ai) deep learning courses. If you're following along with a course at [course18.fast.ai](http://course18.fast.ai) (i.e. the machine learning course, which isn't updated for v1) you need to use `fastai 0.7`;  please follow the installation instructions [here](https://forums.fast.ai/t/fastai-v0-install-issues-thread/24652).\n\n## Installation\n\n**NB:** *fastai v1 currently supports Linux only, and requires **PyTorch v1** and **Python 3.6** or later. Windows support is at an experimental stage: it should work fine but it's much slower and less well tested. Since Macs don't currently have good Nvidia GPU support, we do not currently prioritize Mac development.*\n\n`fastai-1.x` can be installed with either `conda` or `pip` package managers and also from source. At the moment you can't just run *install*, since you first need to get the correct `pytorch` version installed - thus to get `fastai-1.x` installed choose one of the installation recipes below using your favorite python package manager. Note that **PyTorch v1** and **Python 3.6** are the minimal version requirements.\n\nIt's highly recommended you install `fastai` and its dependencies in a virtual environment ([`conda`](https://conda.io/docs/user-guide/tasks/manage-environments.html) or others), so that you don't interfere with system-wide python packages. It's not that you must, but if you experience problems with any dependency packages, please consider using a fresh virtual environment just for `fastai`.\n\nStarting with pytorch-1.x you no longer need to install a special pytorch-cpu version. Instead use the normal pytorch and it works with and without GPU. But [you can install the cpu build too](https://docs.fast.ai/install.html#cpu-build).\n\nIf you experience installation problems, please read about [installation issues](https://github.com/fastai/fastai/blob/master/README.md#installation-issues).\n\nIf you are planning on using `fastai` in the jupyter notebook environment, make sure to also install the corresponding [packages](https://docs.fast.ai/install.html#jupyter-notebook-dependencies).\n\nMore advanced installation issues, such as installing only partial dependencies are covered in a dedicated [installation doc](https://docs.fast.ai/install.html).\n\n### Conda Install\n\n```bash\nconda install -c pytorch -c fastai fastai\n```\n\nThis will install the `pytorch` build with the latest `cudatoolkit` version. If you need a higher or lower `CUDA XX` build (e.g. CUDA 9.0), following the instructions [here](https://pytorch.org/get-started/locally/), to install the desired `pytorch` build.\n\nNote that JPEG decoding can be a bottleneck, particularly if you have a fast GPU. You can optionally install an optimized JPEG decoder as follows (Linux):\n\n```bash\nconda uninstall --force jpeg libtiff -y\nconda install -c conda-forge libjpeg-turbo pillow==6.0.0\nCC=\"cc -mavx2\" pip install --no-cache-dir -U --force-reinstall --no-binary :all: --compile pillow-simd\n```\nIf you only care about faster JPEG decompression, it can be `pillow` or `pillow-simd` in the last command above, the latter speeds up other image processing operations. For the full story see [Pillow-SIMD](https://docs.fast.ai/performance.html#faster-image-processing).\n\n### PyPI Install\n\n```bash\npip install fastai\n```\n\nBy default pip will install the latest `pytorch` with the latest `cudatoolkit`. If your hardware doesn't support the latest `cudatoolkit`, follow the instructions [here](https://pytorch.org/get-started/locally/), to install a `pytorch` build that fits your hardware.\n\n### Bug Fix Install\n\nIf a bug fix was made in git and you can't wait till a new release is made, you can install the bleeding edge version of `fastai` with:\n\n```\npip install git+https://github.com/fastai/fastai.git\n```\n\n### Developer Install\n\nThe following instructions will result in a [pip editable install](https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs), so that you can `git pull` at any time and your environment will automatically get the updates:\n\n```bash\ngit clone https://github.com/fastai/fastai\ncd fastai\ntools/run-after-git-clone\npip install -e \".[dev]\"\n```\n\nNext, you can test that the build works by starting the jupyter notebook:\n\n```bash\njupyter notebook\n```\nand executing an example notebook. For example load `examples/tabular.ipynb` and run it.\n\nPlease refer to [CONTRIBUTING.md](https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md) and [Notes For Developers](https://docs.fast.ai/dev/develop.html) for more details on how to contribute to the `fastai` project.\n\n\n\n\n### Building From Source\n\nIf for any reason you can't use the prepackaged packages and have to build from source, this section is for you.\n\n1. To build `pytorch` from source follow the [complete instructions](https://github.com/pytorch/pytorch#from-source). Remember to first install CUDA, CuDNN, and other required libraries as suggested - everything will be very slow without those libraries built into `pytorch`.\n\n2. Next, you will also need to build `torchvision` from source:\n\n   ```bash\n   git clone https://github.com/pytorch/vision\n   cd vision\n   python setup.py install\n   ```\n\n3. When both `pytorch` and `torchvision` are installed, first test that you can load each of these libraries:\n\n   ```bash\n   import torch\n   import torchvision\n   ```\n\n   to validate that they were installed correctly\n\n   Finally, proceed with `fastai` installation as normal, either through prepackaged pip or conda builds or installing from source (\"the developer install\") as explained in the sections above.\n\n\n\n## Installation Issues\n\nIf the installation process fails, first make sure [your system is supported](https://github.com/fastai/fastai/blob/master/README.md#is-my-system-supported). And if the problem is still not addressed, please refer to the [troubleshooting document](https://docs.fast.ai/troubleshoot.html).\n\nIf you encounter installation problems with conda, make sure you have the latest `conda` client (`conda install` will do an update too):\n```bash\nconda install conda\n```\n\n### Is My System Supported?\n\n1. Python: You need to have python 3.6 or higher\n\n2. CPU or GPU\n\n   The `pytorch` binary package comes with its own CUDA, CuDNN, NCCL, MKL, and other libraries so you don't have to install system-wide NVIDIA's CUDA and related libraries if you don't need them for something else. If you have them installed already it doesn't matter which NVIDIA's CUDA version library you have installed system-wide. Your system could have CUDA 9.0 libraries, and you can still use `pytorch` build with CUDA 10.0 libraries without any problem, since the `pytorch` binary package is self-contained.\n\n   The only requirement is that you have installed and configured the NVIDIA driver correctly. Usually you can test that by running `nvidia-smi`. While it's possible that this application is not available on your system, it's very likely that if it doesn't work, then you don't have your NVIDIA drivers configured properly. And remember that a reboot is always required after installing NVIDIA drivers.\n\n3. Operating System:\n\n   Since fastai-1.0 relies on pytorch-1.0, you need to be able to install pytorch-1.0 first.\n\n   As of this moment pytorch.org's 1.0 version supports:\n\n    | Platform | GPU    | CPU    |\n    |----------|--------|--------|\n    | linux    | binary | binary |\n    | mac      | source | binary |\n    | windows  | binary | binary |\n\n   Legend: `binary` = can be installed directly, `source` = needs to be built from source.\n\n   If there is no `pytorch` preview conda or pip package available for your system, you may still be able to [build it from source](https://pytorch.org/get-started/locally/).\n\n4. How do you know which pytorch cuda version build to choose?\n\n   It depends on the version of the installed NVIDIA driver. Here are the requirements for CUDA versions supported by pre-built `pytorch` releases:\n\n    | CUDA Toolkit | NVIDIA (Linux x86_64) |\n    |--------------|-----------------------|\n    | CUDA 10.0    | >= 410.00             |\n    | CUDA 9.0     | >= 384.81             |\n    | CUDA 8.0     | >= 367.48             |\n\n   So if your NVIDIA driver is less than 384, then you can only use CUDA 8.0. Of course, you can upgrade your drivers to more recent ones if your card supports it.\n\n   You can find a complete table with all variations [here](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html).\n\n   If you use NVIDIA driver 410+, you most likely want to install the `cudatoolkit=10.0` pytorch variant, via:\n   ```bash\n   conda install -c pytorch pytorch cudatoolkit=10.0\n   ```\n   or if you need a lower version, use one of:\n   ```bash\n   conda install -c pytorch pytorch cudatoolkit=8.0\n   conda install -c pytorch pytorch cudatoolkit=9.0\n   ```\n   For other options refer to the complete list of [the available pytorch variants](https://pytorch.org/get-started/locally/).\n\n## Updates\n\nIn order to update your environment, simply install `fastai` in exactly the same way you did the initial installation.\n\nTop level files `environment.yml` and `environment-cpu.yml` belong to the old fastai (0.7). `conda env update` is no longer the way to update your `fastai-1.x` environment. These files remain because the fastai course-v2 video instructions rely on this setup. Eventually, once fastai course-v3 p1 and p2 will be completed, they will probably be moved to where they belong - under `old/`.\n\n## Contribution guidelines\n\nIf you want to contribute to `fastai`, be sure to review the [contribution guidelines](https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md). This project adheres to fastai's [code of conduct](https://github.com/fastai/fastai/blob/master/CODE-OF-CONDUCT.md). By participating, you are expected to uphold this code.\n\nWe use GitHub issues for tracking requests and bugs, so please see [fastai forum](https://forums.fast.ai/) for general questions and discussion.\n\nThe fastai project strives to abide by generally accepted best practices in open-source software development:\n\n## History\n\nA detailed history of changes can be found [here](https://github.com/fastai/fastai/blob/master/CHANGES.md).\n\n## Copyright\n\nCopyright 2017 onwards, fast.ai, Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this project's files except in compliance with the License. A copy of the License is provided in the LICENSE file in this repository.\n"}, "fiber": {"file_name": "uber/fiber/README.md", "raw_text": "<p align=\"right\">\n  <a href=\"https://travis-ci.com/uber/fiber\">\n      <img src=\"https://travis-ci.com/uber/fiber.svg?token=BxMzxQEDDtTBPG9151kk&branch=master\" alt=\"build\" />\n  </a>\n</p>\n\n<img src=\"docs/img/fiber_logo.png\" alt=\"drawing\" width=\"550\"/>\n\n[**Project Home**](https://uber.github.io/fiber/) &nbsp;\n[**Blog**](https://uber.github.io/fiber/introduction/) &nbsp;\n[**Documents**](https://uber.github.io/fiber/getting-started/) &nbsp;\n[**Paper**](https://arxiv.org/abs/2003.11164) &nbsp;\n[**Media Coverage**](https://venturebeat.com/2020/03/26/uber-details-fiber-a-framework-for-distributed-ai-model-training/)\n\n# Fiber\n\n### Distributed Computing for AI Made Simple\n\n*This project is experimental and the APIs are not considered stable.*\n\nFiber is a Python distributed computing library for modern computer clusters.\n\n* It is easy to use. Fiber allows you to write programs that run on a computer cluster level without the need to dive into the details of computer cluster.\n* It is easy to learn. Fiber provides the same API as Python's standard [multiprocessing](https://docs.python.org/3.6/library/multiprocessing.html) library that you are familiar with. If you know how to use multiprocessing, you can program a computer cluster with Fiber.\n* It is fast. Fiber's communication backbone is built on top of [Nanomsg](https://nanomsg.org/) which is a high-performance asynchronous messaging library to allow fast and reliable communication.\n* It doesn't need deployment. You run it as the same way as running a normal application on a computer cluster and Fiber handles the rest for you.\n* It it reliable. Fiber has built-in error handling when you are running a pool of workers. Users can focus on writing the actual application code instead of dealing with crashed workers.\n\nOriginally, it was developed to power large scale parallel scientific computation projects like [POET](https://eng.uber.com/poet-open-ended-deep-learning/) and it has been used to power similar projects within Uber.\n\n\n## Installation\n\n```\npip install fiber\n```\n\nCheck [here](https://uber.github.io/fiber/installation/) for details.\n\n## Quick Start\n\n\n### Hello Fiber\nTo use Fiber, simply import it in your code and it works very similar to multiprocessing.\n\n```python\nimport fiber\n\nif __name__ == '__main__':\n    fiber.Process(target=print, args=('Hello, Fiber!',)).start()\n```\n\nNote that `if __name__ == '__main__':` is necessary because Fiber uses *spawn* method to start new processes. Check [here](https://stackoverflow.com/questions/50781216/in-python-multiprocessing-process-do-we-have-to-use-name-main) for details.\n\nLet's take look at another more complex example:\n\n### Estimating Pi\n\n\n```python\nimport fiber\nimport random\n\n@fiber.meta(cpu=1)\ndef inside(p):\n    x, y = random.random(), random.random()\n    return x * x + y * y < 1\n\ndef main():\n    NUM_SAMPLES = int(1e6)\n    pool = fiber.Pool(processes=4)\n    count = sum(pool.map(inside, range(0, NUM_SAMPLES)))\n    print(\"Pi is roughly {}\".format(4.0 * count / NUM_SAMPLES))\n\nif __name__ == '__main__':\n    main()\n```\n\n\nFiber implements most of multiprocessing's API including `Process`, `SimpleQueue`, `Pool`, `Pipe`, `Manager` and it has its own extension to the multiprocessing's API to make it easy to compose large scale distributed applications. For the detailed API guild, check out [here](https://uber.github.io/fiber/process/).\n\n### Running on a Kubernetes cluster\n\nFiber also has native support for computer clusters. To run the above example on Kubernetes, fiber provided a convenient command line tool to manage the workflow.\n\nAssume you have a working docker environment locally and have finished configuring [Google Cloud SDK](https://cloud.google.com/sdk/docs/quickstarts). Both `gcloud` and `kubectl` are available locally. Then you can start by writing a Dockerfile which describes the running environment.  An example Dockerfile looks like this:\n\n```dockerfile\n# example.docker\nFROM python:3.6-buster\nADD examples/pi_estimation.py /root/pi_estimation.py\nRUN pip install fiber\n```\n**Build an image and launch your job**\n\n```\nfiber run -a python3 /root/pi_estimation.py\n```\n\nThis command will look for local Dockerfile and build a docker image and push it to your Google Container Registry . It then launches the main job which contains your code and runs the command `python3 /root/pi_estimation.py` inside your job. Once the main job is running, it will start 4 subsequent jobs on the cluster and each of them is a Pool worker.\n\n\n## Supported platforms\n\n* Operating system: Linux\n* Python: 3.6+\n* Supported cluster management systems:\n\t* Kubernetes (Tested with Google Kubernetes Engine on Google cloud)\n\nWe are interested in supporting other cluster management systems like [Slurm](https://slurm.schedmd.com/), if you want to contribute to it please let us know.\n\n\nCheck [here](https://uber.github.io/fiber/platforms/) for details.\n\n## Documentation\n\nThe documentation, including method/API references, can be found [here](https://uber.github.io/fiber/getting-started/).\n\n\n## Testing\n\nInstall test dependencies. You'll also need to make sure [docker](https://docs.docker.com/install/) is available on the testing machine.\n\n```bash\n$ pip install -e .[test]\n```\n\nRun tests\n\n```bash\n$ make test\n```\n\n## Contributing\nPlease read our [code of conduct](CODE_OF_CONDUCT.md) before you contribute! You can find details for submitting pull requests in the [CONTRIBUTING.md](CONTRIBUTING.md) file. Issue [template](https://help.github.com/articles/about-issue-and-pull-request-templates/).\n\n## Versioning\nWe document versions and changes in our changelog - see the [CHANGELOG.md](CHANGELOG.md) file for details.\n\n## License\nThis project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.\n\n## Cite Fiber\n\n```\n@misc{zhi2020fiber,\n    title={Fiber: A Platform for Efficient Development and Distributed Training for Reinforcement Learning and Population-Based Methods},\n    author={Jiale Zhi and Rui Wang and Jeff Clune and Kenneth O. Stanley},\n    year={2020},\n    eprint={2003.11164},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n```\n\n## Acknowledgments\n* Special thanks to Piero Molino for designing the logo for Fiber\n"}, "fiscalyear": {"file_name": "adamjstewart/fiscalyear/README.rst", "raw_text": ".. image:: https://travis-ci.org/adamjstewart/fiscalyear.svg?branch=master\n   :target: https://travis-ci.org/adamjstewart/fiscalyear\n\n.. image:: https://codecov.io/gh/adamjstewart/fiscalyear/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/adamjstewart/fiscalyear\n\n.. image:: https://readthedocs.org/projects/fiscalyear/badge/?version=latest\n   :target: https://fiscalyear.readthedocs.io\n\n.. image:: https://badge.fury.io/py/fiscalyear.svg\n   :target: https://pypi.org/project/fiscalyear/\n\n.. image:: https://anaconda.org/conda-forge/fiscalyear/badges/version.svg\n   :target: https://anaconda.org/conda-forge/fiscalyear\n\n\nOverview\n========\n\n`fiscalyear <https://github.com/adamjstewart/fiscalyear>`_ is a small, lightweight Python module providing helpful utilities for managing the fiscal calendar. It is designed as an extension of the built-in `datetime <https://docs.python.org/3/library/datetime.html>`_ and `calendar <https://docs.python.org/3/library/calendar.html>`_ modules, adding the ability to query the fiscal year and fiscal quarter of a date or datetime object.\n\n\nBasic Usage\n===========\n\n``fiscalyear`` provides several useful classes.\n\nFiscalYear\n----------\n\nThe ``FiscalYear`` class provides an object for storing information about the start and end of a particular fiscal year.\n\n.. code-block:: python\n\n   >>> from fiscalyear import *\n   >>> a = FiscalYear(2017)\n   >>> a.start\n   FiscalDateTime(2016, 10, 1, 0, 0)\n   >>> a.end\n   FiscalDateTime(2017, 9, 30, 23, 59, 59)\n\nYou can also get the current ``FiscalYear`` with:\n\n.. code-block:: python\n\n   >>> FiscalYear.current()\n   FiscalYear(2018)\n\nFiscalQuarter\n-------------\n\nThe ``FiscalYear`` class also allows you to query information about a specific quarter.\n\n.. code-block:: python\n\n   >>> a.q3.start\n   FiscalDateTime(2017, 4, 1, 0, 0)\n   >>> a.q3.end\n   FiscalDateTime(2017, 6, 30, 23, 59, 59)\n\n\nThese objects represent the standalone ``FiscalQuarter`` class.\n\n.. code-block:: python\n\n   >>> b = FiscalQuarter(2017, 3)\n   >>> b.start\n   FiscalDateTime(2017, 4, 1, 0, 0)\n   >>> b.end\n   FiscalDateTime(2017, 6, 30, 23, 59, 59)\n   >>> a.q3 == b\n   True\n   >>> b in a\n   True\n\nYou can also get the current ``FiscalQuarter`` with:\n\n.. code-block:: python\n\n   >>> FiscalQuarter.current()\n   FiscalQuarter(2018, 2)\n\nFiscalDateTime\n--------------\n\nThe start and end of each quarter are stored as instances of the ``FiscalDateTime`` class. This class provides all of the same features as the ``datetime`` class, with the addition of the ability to query the fiscal year and quarter.\n\n.. code-block:: python\n\n   >>> c = FiscalDateTime.now()\n   >>> c\n   FiscalDateTime(2017, 4, 8, 20, 30, 31, 105323)\n   >>> c.fiscal_year\n   2017\n   >>> c.quarter\n   3\n   >>> c.next_quarter\n   FiscalQuarter(2017, 4)\n\n\nFiscalDate\n----------\n\nIf you don't care about the time component of the ``FiscalDateTime`` class, the ``FiscalDate`` class is right for you.\n\n.. code-block:: python\n\n   >>> d = FiscalDate.today()\n   >>> d\n   FiscalDate(2017, 4, 8)\n   >>> d.fiscal_year\n   2017\n   >>> d.prev_fiscal_year\n   FiscalYear(2016)\n\n\nInstallation\n============\n\n``fiscalyear`` has no dependencies, making it simple and easy to install. The recommended way to install ``fiscalyear`` is with ``pip``.\n\n.. code-block:: console\n\n   $ pip install fiscalyear\n\n\nFor alternate installation methods, see the `Installation Documentation <http://fiscalyear.readthedocs.io/en/latest/installation.html>`_.\n\n\nDocumentation\n=============\n\nDocumentation is hosted on `Read the Docs <http://fiscalyear.readthedocs.io/en/latest/index.html>`_.\n"}, "fbprophet": {"file_name": "facebook/prophet/README.md", "raw_text": "# Prophet: Automatic Forecasting Procedure\n\n[![Build Status](https://travis-ci.com/facebook/prophet.svg?branch=master)](https://travis-ci.com/facebook/prophet)\n[![Pypi_Version](https://img.shields.io/pypi/v/fbprophet.svg)](https://pypi.python.org/pypi/fbprophet)\n[![Conda_Version](https://anaconda.org/conda-forge/fbprophet/badges/version.svg)](https://anaconda.org/conda-forge/fbprophet/)\n\nProphet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n\nProphet is [open source software](https://code.facebook.com/projects/) released by Facebook's [Core Data Science team](https://research.fb.com/category/data-science/). It is available for download on [CRAN](https://cran.r-project.org/package=prophet) and [PyPI](https://pypi.python.org/pypi/fbprophet/).\n\n## Important links\n\n- Homepage: https://facebook.github.io/prophet/\n- HTML documentation: https://facebook.github.io/prophet/docs/quick_start.html\n- Issue tracker: https://github.com/facebook/prophet/issues\n- Source code repository: https://github.com/facebook/prophet\n- Prophet R package: https://cran.r-project.org/package=prophet\n- Prophet Python package: https://pypi.python.org/pypi/fbprophet/\n- Release blogpost: https://research.fb.com/prophet-forecasting-at-scale/\n- Prophet paper: Sean J. Taylor, Benjamin Letham (2018) Forecasting at scale. The American Statistician 72(1):37-45 (https://peerj.com/preprints/3190.pdf).\n\n## Installation in R\n\nProphet is a [CRAN package](https://cran.r-project.org/package=prophet) so you can use `install.packages`. For OSX, be sure to specify a source install:\n\n```\n# R\n> install.packages('prophet', type=\"source\")\n```\n\nAfter installation, you can [get started!](https://facebook.github.io/prophet/docs/quick_start.html#r-api)\n\n### Windows\n\nOn Windows, R requires a compiler so you'll need to [follow the instructions](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started) provided by `rstan`. The key step is installing [Rtools](http://cran.r-project.org/bin/windows/Rtools/) before attempting to install the package.\n\nIf you have custom Stan compiler settings, install from source rather than the CRAN binary.\n\n## Installation in Python\n\nProphet is on PyPI, so you can use pip to install it:\n\n```\n# bash\n$ pip install fbprophet\n```\n\nThe default dependency that Prophet has is `pystan`. PyStan has its own [installation instructions](http://pystan.readthedocs.io/en/latest/installation_beginner.html). Install pystan with pip before using pip to install fbprophet.\n\nYou can also choose a (more experimental) alternative stan backend called `cmdstanpy`. It requires the [CmdStan](https://mc-stan.org/users/interfaces/cmdstan) command line interface and you will have to specify the environment variable `STAN_BACKEND` pointing to it, for example:\n\n```\n# bash\n$ CMDSTAN=/tmp/cmdstan-2.22.1 STAN_BACKEND=CMDSTANPY pip install fbprophet\n```\n\nNote that the `CMDSTAN` variable is directly related to `cmdstanpy` module and can be omitted if your CmdStan binaries are in your `$PATH`.\n\nIt is also possible to install Prophet with two backends:\n\n```\n# bash\n$ CMDSTAN=/tmp/cmdstan-2.22.1 STAN_BACKEND=PYSTAN,CMDSTANPY pip install fbprophet\n```\n\nAfter installation, you can [get started!](https://facebook.github.io/prophet/docs/quick_start.html#python-api)\n\nIf you upgrade the version of PyStan installed on your system, you may need to reinstall fbprophet ([see here](https://github.com/facebook/prophet/issues/324)).\n\n### Anaconda\n\nUse `conda install gcc` to set up gcc. The easiest way to install Prophet is through conda-forge: `conda install -c conda-forge fbprophet`.\n\n### Windows\n\nOn Windows, PyStan requires a compiler so you'll need to [follow the instructions](http://pystan.readthedocs.io/en/latest/windows.html). The easiest way to install Prophet in Windows is in Anaconda.\n\n### Linux\n\nMake sure compilers (gcc, g++, build-essential) and Python development tools (python-dev, python3-dev) are installed. In Red Hat systems, install the packages gcc64 and gcc64-c++. If you are using a VM, be aware that you will need at least 4GB of memory to install fbprophet, and at least 2GB of memory to use fbprophet.\n\n## Changelog\n\n### Version 0.6 (2020.03.03)\n\n- Fix bugs related to upstream changes in `holidays` and `pandas` packages.\n- Compile model during first use, not during install (to comply with CRAN policy)\n- `cmdstanpy` backend now available in Python\n\n### Version 0.5 (2019.05.14)\n\n- Conditional seasonalities\n- Improved cross validation estimates\n- Plotly plot in Python\n- Bugfixes\n\n### Version 0.4 (2018.12.18)\n\n- Added holidays functionality\n- Bugfixes\n\n### Version 0.3 (2018.06.01)\n\n- Multiplicative seasonality\n- Cross validation error metrics and visualizations\n- Parameter to set range of potential changepoints\n- Unified Stan model for both trend types\n- Improved future trend uncertainty for sub-daily data\n- Bugfixes\n\n### Version 0.2.1 (2017.11.08)\n\n- Bugfixes\n\n### Version 0.2 (2017.09.02)\n\n- Forecasting with sub-daily data\n- Daily seasonality, and custom seasonalities\n- Extra regressors\n- Access to posterior predictive samples\n- Cross-validation function\n- Saturating minimums\n- Bugfixes\n\n### Version 0.1.1 (2017.04.17)\n\n- Bugfixes\n- New options for detecting yearly and weekly seasonality (now the default)\n\n### Version 0.1 (2017.02.23)\n\n- Initial release\n\n## License\n\nProphet is licensed under the [MIT license](LICENSE.md).\n"}, "Flask": {"file_name": "pallets/flask/README.rst", "raw_text": "Flask\n=====\n\nFlask is a lightweight `WSGI`_ web application framework. It is designed\nto make getting started quick and easy, with the ability to scale up to\ncomplex applications. It began as a simple wrapper around `Werkzeug`_\nand `Jinja`_ and has become one of the most popular Python web\napplication frameworks.\n\nFlask offers suggestions, but doesn't enforce any dependencies or\nproject layout. It is up to the developer to choose the tools and\nlibraries they want to use. There are many extensions provided by the\ncommunity that make adding new functionality easy.\n\n\nInstalling\n----------\n\nInstall and update using `pip`_:\n\n.. code-block:: text\n\n    pip install -U Flask\n\n\nA Simple Example\n----------------\n\n.. code-block:: python\n\n    from flask import Flask\n\n    app = Flask(__name__)\n\n    @app.route(\"/\")\n    def hello():\n        return \"Hello, World!\"\n\n.. code-block:: text\n\n    $ env FLASK_APP=hello.py flask run\n     * Serving Flask app \"hello\"\n     * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n\n\nContributing\n------------\n\nFor guidance on setting up a development environment and how to make a\ncontribution to Flask, see the `contributing guidelines`_.\n\n.. _contributing guidelines: https://github.com/pallets/flask/blob/master/CONTRIBUTING.rst\n\n\nDonate\n------\n\nThe Pallets organization develops and supports Flask and the libraries\nit uses. In order to grow the community of contributors and users, and\nallow the maintainers to devote more time to the projects, `please\ndonate today`_.\n\n.. _please donate today: https://psfmember.org/civicrm/contribute/transact?reset=1&id=20\n\n\nLinks\n-----\n\n* Website: https://palletsprojects.com/p/flask/\n* Documentation: https://flask.palletsprojects.com/\n* Releases: https://pypi.org/project/Flask/\n* Code: https://github.com/pallets/flask\n* Issue tracker: https://github.com/pallets/flask/issues\n* Test status: https://dev.azure.com/pallets/flask/_build\n* Official chat: https://discord.gg/t6rrQZH\n\n.. _WSGI: https://wsgi.readthedocs.io\n.. _Werkzeug: https://www.palletsprojects.com/p/werkzeug/\n.. _Jinja: https://www.palletsprojects.com/p/jinja/\n.. _pip: https://pip.pypa.io/en/stable/quickstart/\n"}, "gensim": {"file_name": "RaRe-Technologies/gensim/README.md", "raw_text": "gensim \u2013 Topic Modelling in Python\n==================================\n\n[![Build Status](https://travis-ci.org/RaRe-Technologies/gensim.svg?branch=develop)](https://travis-ci.org/RaRe-Technologies/gensim)\n[![GitHub release](https://img.shields.io/github/release/rare-technologies/gensim.svg?maxAge=3600)](https://github.com/RaRe-Technologies/gensim/releases)\n[![Conda-forge Build](https://anaconda.org/conda-forge/gensim/badges/version.svg)](https://anaconda.org/conda-forge/gensim)\n[![Wheel](https://img.shields.io/pypi/wheel/gensim.svg)](https://pypi.python.org/pypi/gensim)\n[![DOI](https://zenodo.org/badge/DOI/10.13140/2.1.2393.1847.svg)](https://doi.org/10.13140/2.1.2393.1847)\n[![Mailing List](https://img.shields.io/badge/-Mailing%20List-brightgreen.svg)](https://groups.google.com/forum/#!forum/gensim)\n[![Gitter](https://img.shields.io/badge/gitter-join%20chat%20%E2%86%92-09a3d5.svg)](https://gitter.im/RaRe-Technologies/gensim)\n[![Follow](https://img.shields.io/twitter/follow/gensim_py.svg?style=social&label=Follow)](https://twitter.com/gensim_py)\n\nGensim is a Python library for *topic modelling*, *document indexing*\nand *similarity retrieval* with large corpora. Target audience is the\n*natural language processing* (NLP) and *information retrieval* (IR)\ncommunity.\n\n<!--\n## :pizza: Hacktoberfest 2019 :beer:\n\nWe are accepting PRs for Hacktoberfest!\nSee [here](HACKTOBERFEST.md) for details.\n-->\n\nFeatures\n--------\n\n-   All algorithms are **memory-independent** w.r.t. the corpus size\n    (can process input larger than RAM, streamed, out-of-core),\n-   **Intuitive interfaces**\n    -   easy to plug in your own input corpus/datastream (trivial\n        streaming API)\n    -   easy to extend with other Vector Space algorithms (trivial\n        transformation API)\n-   Efficient multicore implementations of popular algorithms, such as\n    online **Latent Semantic Analysis (LSA/LSI/SVD)**, **Latent\n    Dirichlet Allocation (LDA)**, **Random Projections (RP)**,\n    **Hierarchical Dirichlet Process (HDP)** or **word2vec deep\n    learning**.\n-   **Distributed computing**: can run *Latent Semantic Analysis* and\n    *Latent Dirichlet Allocation* on a cluster of computers.\n-   Extensive [documentation and Jupyter Notebook tutorials].\n\nIf this feature list left you scratching your head, you can first read\nmore about the [Vector Space Model] and [unsupervised document analysis]\non Wikipedia.\n\nSupport\n------------\n\nAsk open-ended or research questions on the [Gensim Mailing List](https://groups.google.com/forum/#!forum/gensim).\n\nRaise bugs on [Github](https://github.com/RaRe-Technologies/gensim/blob/develop/CONTRIBUTING.md) but **make sure you follow the [issue template](https://github.com/RaRe-Technologies/gensim/blob/develop/ISSUE_TEMPLATE.md)**. Issues that are not bugs or fail to follow the issue template will be closed without inspection.\n\nInstallation\n------------\n\nThis software depends on [NumPy and Scipy], two Python packages for\nscientific computing. You must have them installed prior to installing\ngensim.\n\nIt is also recommended you install a fast BLAS library before installing\nNumPy. This is optional, but using an optimized BLAS such as [ATLAS] or\n[OpenBLAS] is known to improve performance by as much as an order of\nmagnitude. On OS X, NumPy picks up the BLAS that comes with it\nautomatically, so you don\u2019t need to do anything special.\n\nThe simple way to install gensim is:\n\n    pip install -U gensim\n\nOr, if you have instead downloaded and unzipped the [source tar.gz]\npackage, you\u2019d run:\n\n    python setup.py test\n    python setup.py install\n\nFor alternative modes of installation (without root privileges,\ndevelopment installation, optional install features), see the\n[documentation].\n\nThis version has been tested under Python 2.7, 3.5 and 3.6. Gensim\u2019s github repo is hooked\nagainst [Travis CI for automated testing] on every commit push and pull\nrequest. Support for Python 2.6, 3.3 and 3.4 was dropped in gensim 1.0.0. Install gensim 0.13.4 if you *must* use Python 2.6, 3.3 or 3.4. Support for Python 2.5 was dropped in gensim 0.10.0; install gensim 0.9.1 if you *must* use Python 2.5). \n\nHow come gensim is so fast and memory efficient? Isn\u2019t it pure Python, and isn\u2019t Python slow and greedy?\n--------------------------------------------------------------------------------------------------------\n\nMany scientific algorithms can be expressed in terms of large matrix\noperations (see the BLAS note above). Gensim taps into these low-level\nBLAS libraries, by means of its dependency on NumPy. So while\ngensim-the-top-level-code is pure Python, it actually executes highly\noptimized Fortran/C under the hood, including multithreading (if your\nBLAS is so configured).\n\nMemory-wise, gensim makes heavy use of Python\u2019s built-in generators and\niterators for streamed data processing. Memory efficiency was one of\ngensim\u2019s [design goals], and is a central feature of gensim, rather than\nsomething bolted on as an afterthought.\n\nDocumentation\n-------------\n\n-   [QuickStart]\n-   [Tutorials]\n-   [Official API Documentation]\n\n  [QuickStart]: https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html\n  [Tutorials]: https://radimrehurek.com/gensim/auto_examples/\n  [Official Documentation and Walkthrough]: http://radimrehurek.com/gensim/\n  [Official API Documentation]: http://radimrehurek.com/gensim/apiref.html\n  \n---------\n\nAdopters\n--------\n\n| Company | Logo | Industry | Use of Gensim |\n|---------|------|----------|---------------|                          \n| [RARE Technologies](http://rare-technologies.com) | ![rare](docs/src/readme_images/rare.png) | ML & NLP consulting | Creators of Gensim \u2013\u00a0this is us! |\n| [Amazon](http://www.amazon.com/) |  ![amazon](docs/src/readme_images/amazon.png) | Retail |  Document similarity. |\n| [National Institutes of Health](https://github.com/NIHOPA/pipeline_word2vec) | ![nih](docs/src/readme_images/nih.png) | Health | Processing grants and publications with word2vec. |\n| [Cisco Security](http://www.cisco.com/c/en/us/products/security/index.html) | ![cisco](docs/src/readme_images/cisco.png) | Security |  Large-scale fraud detection. |\n| [Mindseye](http://www.mindseyesolutions.com/) | ![mindseye](docs/src/readme_images/mindseye.png) | Legal | Similarities in legal documents. |\n| [Channel 4](http://www.channel4.com/) | ![channel4](docs/src/readme_images/channel4.png) | Media | Recommendation engine. |\n| [Talentpair](http://talentpair.com) | ![talent-pair](docs/src/readme_images/talent-pair.png) | HR | Candidate matching in high-touch recruiting. |\n| [Juju](http://www.juju.com/)  | ![juju](docs/src/readme_images/juju.png) | HR | Provide non-obvious related job suggestions. |\n| [Tailwind](https://www.tailwindapp.com/) | ![tailwind](docs/src/readme_images/tailwind.png) | Media | Post interesting and relevant content to Pinterest. |\n| [Issuu](https://issuu.com/) | ![issuu](docs/src/readme_images/issuu.png) | Media | Gensim's LDA module lies at the very core of the analysis we perform on each uploaded publication to figure out what it's all about. |\n| [Search Metrics](http://www.searchmetrics.com/) | ![search-metrics](docs/src/readme_images/search-metrics.png) | Content Marketing | Gensim word2vec used for entity disambiguation in Search Engine Optimisation. |\n| [12K Research](https://12k.co/) | ![12k](docs/src/readme_images/12k.png)| Media |   Document similarity analysis on media articles. |\n| [Stillwater Supercomputing](http://www.stillwater-sc.com/) | ![stillwater](docs/src/readme_images/stillwater.png) | Hardware | Document comprehension and association with word2vec. |\n| [SiteGround](https://www.siteground.com/) |  ![siteground](docs/src/readme_images/siteground.png) | Web hosting | An ensemble search engine which uses different embeddings models and similarities, including word2vec, WMD, and LDA. |\n| [Capital One](https://www.capitalone.com/) | ![capitalone](docs/src/readme_images/capitalone.png) | Finance | Topic modeling for customer complaints exploration. |\n\n-------\n\nCiting gensim\n------------\n\nWhen [citing gensim in academic papers and theses], please use this\nBibTeX entry:\n\n    @inproceedings{rehurek_lrec,\n          title = {{Software Framework for Topic Modelling with Large Corpora}},\n          author = {Radim {\\v R}eh{\\r u}{\\v r}ek and Petr Sojka},\n          booktitle = {{Proceedings of the LREC 2010 Workshop on New\n               Challenges for NLP Frameworks}},\n          pages = {45--50},\n          year = 2010,\n          month = May,\n          day = 22,\n          publisher = {ELRA},\n          address = {Valletta, Malta},\n          note={\\url{http://is.muni.cz/publication/884893/en}},\n          language={English}\n    }\n\n  [citing gensim in academic papers and theses]: https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vG_kV0AAAAJ&citation_for_view=9vG_kV0AAAAJ:NaGl4SEjCO4C\n\n  [Travis CI for automated testing]: https://travis-ci.org/RaRe-Technologies/gensim\n  [design goals]: http://radimrehurek.com/gensim/about.html\n  [RaRe Technologies]: http://rare-technologies.com/wp-content/uploads/2016/02/rare_image_only.png%20=10x20\n  [rare\\_tech]: //rare-technologies.com\n  [Talentpair]: https://avatars3.githubusercontent.com/u/8418395?v=3&s=100\n  [citing gensim in academic papers and theses]: https://scholar.google.cz/citations?view_op=view_citation&hl=en&user=9vG_kV0AAAAJ&citation_for_view=9vG_kV0AAAAJ:u-x6o8ySG0sC\n\n  \n  \n  [documentation and Jupyter Notebook tutorials]: https://github.com/RaRe-Technologies/gensim/#documentation\n  [Vector Space Model]: http://en.wikipedia.org/wiki/Vector_space_model\n  [unsupervised document analysis]: http://en.wikipedia.org/wiki/Latent_semantic_indexing\n  [NumPy and Scipy]: http://www.scipy.org/Download\n  [ATLAS]: http://math-atlas.sourceforge.net/\n  [OpenBLAS]: http://xianyi.github.io/OpenBLAS/\n  [source tar.gz]: http://pypi.python.org/pypi/gensim\n  [documentation]: http://radimrehurek.com/gensim/install.html\n"}, "graphviz": {"file_name": "xflr6/graphviz/README.rst", "raw_text": "Graphviz\n========\n\n|PyPI version| |License| |Supported Python| |Format|\n\n|Travis| |Codecov| |Readthedocs-stable| |Readthedocs-latest|\n\nThis package facilitates the creation and rendering of graph descriptions in\nthe DOT_ language of the Graphviz_ graph drawing software (`master repo`_) from\nPython.\n\nCreate a graph object, assemble the graph by adding nodes and edges, and\nretrieve its DOT source code string. Save the source code to a file and render\nit with the Graphviz installation of your system.\n\nUse the ``view`` option/method to directly inspect the resulting (PDF, PNG,\nSVG, etc.) file with its default application. Graphs can also be rendered\nand displayed within `Jupyter notebooks`_ (formerly known as\n`IPython notebooks`_, example_) as well as the `Jupyter Qt Console`_.\n\n\nLinks\n-----\n\n- GitHub: https://github.com/xflr6/graphviz\n- PyPI: https://pypi.org/project/graphviz/\n- Documentation: https://graphviz.readthedocs.io\n- Changelog: https://graphviz.readthedocs.io/en/latest/changelog.html\n- Issue Tracker: https://github.com/xflr6/graphviz/issues\n- Download: https://pypi.org/project/graphviz/#files\n\n\nInstallation\n------------\n\nThis package runs under Python 2.7, and 3.5+, use pip_ to install:\n\n.. code:: bash\n\n    $ pip install graphviz\n\nTo render the generated DOT source code, you also need to install Graphviz\n(`download page`_).\n\nMake sure that the directory containing the ``dot`` executable is on your\nsystems' path.\n\n\nQuickstart\n----------\n\nCreate a graph object:\n\n.. code:: python\n\n    >>> from graphviz import Digraph\n\n    >>> dot = Digraph(comment='The Round Table')\n\n    >>> dot  #doctest: +ELLIPSIS\n    <graphviz.dot.Digraph object at 0x...>\n\nAdd nodes and edges:\n\n.. code:: python\n\n    >>> dot.node('A', 'King Arthur')\n    >>> dot.node('B', 'Sir Bedevere the Wise')\n    >>> dot.node('L', 'Sir Lancelot the Brave')\n\n    >>> dot.edges(['AB', 'AL'])\n    >>> dot.edge('B', 'L', constraint='false')\n\nCheck the generated source code:\n\n.. code:: python\n\n    >>> print(dot.source)  # doctest: +NORMALIZE_WHITESPACE\n    // The Round Table\n    digraph {\n        A [label=\"King Arthur\"]\n        B [label=\"Sir Bedevere the Wise\"]\n        L [label=\"Sir Lancelot the Brave\"]\n        A -> B\n        A -> L\n        B -> L [constraint=false]\n    }\n\nSave and render the source code, optionally view the result:\n\n.. code:: python\n\n    >>> dot.render('test-output/round-table.gv', view=True)  # doctest: +SKIP\n    'test-output/round-table.gv.pdf'\n\n.. image:: https://raw.github.com/xflr6/graphviz/master/docs/round-table.png\n    :align: center\n\n\nSee also\n--------\n\n- pygraphviz_ |--| full-blown interface wrapping the Graphviz C library with SWIG\n- graphviz-python_ |--| official Python bindings (documentation_)\n- pydot_ |--| stable pure-Python approach, requires pyparsing\n\n\nLicense\n-------\n\nThis package is distributed under the `MIT license`_.\n\n\n.. _pip: https://pip.readthedocs.io\n.. _Graphviz:  https://www.graphviz.org\n.. _master repo: https://gitlab.com/graphviz/graphviz/\n.. _download page: https://www.graphviz.org/download/\n.. _DOT: https://www.graphviz.org/doc/info/lang.html\n.. _Jupyter notebooks: https://jupyter.org\n.. _IPython notebooks: https://ipython.org/notebook.html\n.. _example: https://nbviewer.jupyter.org/github/xflr6/graphviz/blob/master/examples/notebook.ipynb\n.. _Jupyter Qt Console: https://qtconsole.readthedocs.io\n\n.. _pygraphviz: https://pypi.org/project/pygraphviz/\n.. _graphviz-python: https://pypi.org/project/graphviz-python/\n.. _documentation: https://www.graphviz.org/pdf/gv.3python.pdf\n.. _pydot: https://pypi.org/project/pydot/\n\n.. _MIT license: https://opensource.org/licenses/MIT\n\n\n.. |--| unicode:: U+2013\n\n\n.. |PyPI version| image:: https://img.shields.io/pypi/v/graphviz.svg\n    :target: https://pypi.org/project/graphviz/\n    :alt: Latest PyPI Version\n.. |License| image:: https://img.shields.io/pypi/l/graphviz.svg\n    :target: https://pypi.org/project/graphviz/\n    :alt: License\n.. |Supported Python| image:: https://img.shields.io/pypi/pyversions/graphviz.svg\n    :target: https://pypi.org/project/graphviz/\n    :alt: Supported Python Versions\n.. |Format| image:: https://img.shields.io/pypi/format/graphviz.svg\n    :target: https://pypi.org/project/graphviz/\n    :alt: Format\n\n.. |Travis| image:: https://img.shields.io/travis/xflr6/graphviz.svg\n    :target: https://travis-ci.org/xflr6/graphviz\n    :alt: Travis\n.. |Codecov| image:: https://codecov.io/gh/xflr6/graphviz/branch/master/graph/badge.svg\n    :target: https://codecov.io/gh/xflr6/graphviz\n    :alt: Codecov\n.. |Readthedocs-stable| image:: https://readthedocs.org/projects/graphviz/badge/?version=stable\n    :target: https://graphviz.readthedocs.io/en/stable/?badge=stable\n    :alt: Readthedocs stable\n.. |Readthedocs-latest| image:: https://readthedocs.org/projects/graphviz/badge/?version=latest\n    :target: https://graphviz.readthedocs.io/en/latest/?badge=latest\n    :alt: Readthedocs latest"}, "gspread-pandas": {"file_name": "aiguofer/gspread-pandas/README.rst", "raw_text": "===============\nGetting Started\n===============\n\n.. image:: https://img.shields.io/pypi/v/gspread-pandas.svg\n        :target: https://pypi.python.org/pypi/gspread-pandas\n        :alt: PyPI Version\n\n.. image:: https://img.shields.io/travis/aiguofer/gspread-pandas.svg\n        :target: https://travis-ci.org/aiguofer/gspread-pandas\n        :alt: Travis-CI Build Status\n\n.. image:: https://readthedocs.org/projects/gspread-pandas/badge/?version=latest\n        :target: https://gspread-pandas.readthedocs.io/en/latest/?badge=latest\n        :alt: Documentation Status\n\nauthor: Diego Fernandez\n\nLinks:\n\n-  `Documentation <http://gspread-pandas.readthedocs.io/>`_\n-  `Source code <https://github.com/aiguofer/gspread-pandas>`_\n-  `Short video tutorial <https://youtu.be/2yIcNYzfzPw>`_\n\n.. attention:: Upgrading from < 2.0\n\n    If you are upgrading, the ``user`` is now an optional param that\n    uses ``default`` as the default. If you're a single user, you might\n    want to re-name your credentials to ``default`` so you can stop\n    specifying it:\n\n    .. code-block:: console\n\n        mv ~/.config/gspread_pandas/creds{<old_name>,default}\n\nOverview\n========\n\nA package to easily open an instance of a Google spreadsheet and\ninteract with worksheets through Pandas DataFrames. It enables you to\neasily pull data from Google spreadsheets into DataFrames as well as\npush data into spreadsheets from DataFrames. It leverages\n`gspread <https://github.com/burnash/gspread/>`__ in the backend for\nmost of the heavylifting, but it has a lot of added functionality\nto handle things specific to working with DataFrames as well as\nsome extra nice to have features.\n\nThe target audience are Data Analysts and Data Scientists, but it can also\nbe used by Data Engineers or anyone trying to automate workflows with Google\nSheets and Pandas.\n\nSome key goals/features:\n\n-  Be easy to use interactively, with good docstrings and auto-completion\n-  Nicely handle headers and indexes (including multi-level headers and merged cells)\n-  Run on Jupyter, headless server, and/or scripts\n-  Allow storing different user credentials or using Service Accounts\n-  Automatically handle token refreshes\n-  Enable handling of frozen rows and columns\n-  Enable filling in all merged cells when pulling data\n-  Nicely handle large data sets and auto-retries\n-  Enable creation of filters\n-  Handle retries when exceeding 100 second user quota\n-  When pushing DataFrames with MultiIndex columns, allow merging or flattening headers\n-  Ability to handle Spreadsheet permissions\n-  Ability to specify ``ValueInputOption`` and ``ValueRenderOption`` for specific columns\n\nInstallation / Usage\n====================\n\nTo install use pip:\n\n.. code-block:: console\n\n    $ pip install gspread-pandas\n\nOr clone the repo:\n\n.. code-block:: console\n\n    $ git clone https://github.com/aiguofer/gspread-pandas.git\n    $ python setup.py install\n\nBefore using, you will need to download Google client credentials for\nyour app.\n\nClient Credentials\n------------------\n\nTo allow a script to use Google Drive API we need to authenticate our\nself towards Google. To do so, we need to create a project, describing\nthe tool and generate credentials. Please use your web browser and go to\n`Google console <https://console.developers.google.com/>`__ and :\n\n-  Choose **Create Project** in popup menu on the top.\n-  A dialog box appears, so give your project a name and click on\n   **Create** button.\n-  On the left-side menu click on **API Manager**.\n-  A table of available APIs is shown. Switch **Drive API** and click on\n   **Enable API** button. Do the same for **Sheets API**. Other APIs might\n   be switched off, for our purpose.\n-  On the left-side menu click on **Credentials**.\n-  In section **OAuth consent screen** select your email address and\n   give your product a name. Then click on **Save** button.\n-  In section **Credentials** click on **Add credentials** and switch\n   **OAuth client ID** (if you want to use your own account or enable\n   the use of multiple accounts) or **Service account key** (if you prefer\n   to have a service account interacting with spreadsheets).\n-  If you select **OAuth client ID**:\n\n   -  Select **Application type** item as **Other** and give it a name.\n   -  Click on **Create** button.\n   -  Click on **Download JSON** icon on the right side of created\n      **OAuth client IDs** and store the downloaded file on your file system.\n-  If you select **Service account key**\n\n   -  Click on **Service account** dropdown and select **New service account**\n   -  Give it a **Service account name** and ignore the **Role** dropdown\n      (unless you know you need this for something else, it's not necessary for\n      working with spreadsheets)\n   -  Note the **Service account ID** as you might need to give that user\n      permission to interact with your spreadsheets\n   -  Leave **Key type** as **JSON**\n   -  Click **Create** and store the downloaded file on your file system.\n-  Please be aware, the file contains your private credentials, so take\n   care of the file in the same way you care of your private SSH key;\n   Move the downloaded JSON to ``~/.config/gspread_pandas/google_secret.json``\n   (or you can configure the directory and file name by directly calling\n   ``gspread_pandas.conf.get_config``\n\n\nThanks to similar project\n`df2gspread <https://github.com/maybelinot/df2gspread>`__ for this great\ndescription of how to get the client credentials.\n\nYou can read more about it in the `configuration docs\n<https://gspread-pandas.readthedocs.io/en/latest/configuration.html>`__\nincluding how to change the default behavior.\n\nExample\n=======\n\n.. code:: python\n\n    from __future__ import print_function\n    import pandas as pd\n    from gspread_pandas import Spread, Client\n\n    file_name = \"http://stats.idre.ucla.edu/stat/data/binary.csv\"\n    df = pd.read_csv(file_name)\n\n    # 'Example Spreadsheet' needs to already exist and your user must have access to it\n    spread = Spread('Example Spreadsheet')\n    # This will ask to authenticate if you haven't done so before\n\n    # Display available worksheets\n    spread.sheets\n\n    # Save DataFrame to worksheet 'New Test Sheet', create it first if it doesn't exist\n    spread.df_to_sheet(df, index=False, sheet='New Test Sheet', start='A2', replace=True)\n    spread.update_cells('A1', 'A1', ['Created by:', spread.email])\n    print(spread)\n    # <gspread_pandas.client.Spread - User: '<example_user>@gmail.com', Spread: 'Example Spreadsheet', Sheet: 'New Test Sheet'>\n\n    # You can now first instanciate a Client separately and query folders and\n    # instanciate other Spread objects by passing in the Client\n    client = Client()\n    # Assumming you have a dir called 'example dir' with sheets in it\n    available_sheets = client.find_spreadsheet_files_in_folders('example dir')\n    spreads = []\n    for sheet in available_sheets.get('example dir', []):\n        spreads.append(Spread(sheet['id'], client=client))\n\nTroubleshooting\n===============\n\nSSL Error\n---------\n\nIf you're getting an SSL related error or can't seem to be able to open existing\nspreadsheets that you have access to, you might be running into an issue caused by\n``certifi``. This has mainly been experienced on RHEL and CentOS running Python 2.7.\nYou can read more about it in `issue 223\n<https://github.com/burnash/gspread/issues/223>`_\nand `issue 354 <https://github.com/burnash/gspread/issues/354>`_ but, in short, the\nsolution is to either install a specific version of ``certifi`` that works for you,\nor remove it altogether.\n\n.. code-block:: console\n\n   pip install certifi==2015.4.28\n\nor\n\n.. code-block:: console\n\n   pip uninstall certifi\n\nEOFError in Rodeo\n-----------------\n\nIf you're trying to use ``gspread_pandas`` from within\n`Rodeo <https://www.yhat.com/products/rodeo>`_ you might get an\n``EOFError: EOF when reading a line`` error when trying to pass in the verification\ncode. The workaround for this is to first verify your account in a regular shell.\nSince you're just doing this to get your Oauth token, the spreadsheet doesn't need\nto be valid. Just run this in shell:\n\n.. code:: python\n\n   python -c \"from gspread_pandas import Spread; Spread('<user_key>','')\"\n\nThen follow the instructions to create and store the OAuth creds.\n"}, "h5py": {"file_name": "h5py/h5py/README.rst", "raw_text": ".. image:: https://travis-ci.org/h5py/h5py.png\n   :target: https://travis-ci.org/h5py/h5py\n.. image:: https://ci.appveyor.com/api/projects/status/h3iajp4d1myotprc/branch/master?svg=true\n   :target: https://ci.appveyor.com/project/h5py/h5py/branch/master\n.. image:: https://dev.azure.com/h5pyappveyor/h5py/_apis/build/status/h5py.h5py?branchName=master\n   :target: https://dev.azure.com/h5pyappveyor/h5py/_build/latest?definitionId=1&branchName=master\n\nHDF5 for Python\n===============\n`h5py` is a thin, pythonic wrapper around the `HDF5 <https://support.hdfgroup.org/HDF5/>`_, which runs on Python 3 (3.6+).\n\nWebsites\n--------\n\n* Main website: https://www.h5py.org\n* Source code: https://github.com/h5py/h5py\n* Mailing list: https://groups.google.com/d/forum/h5py\n\nInstallation\n------------\n\nPre-build `h5py` can either be installed via your Python Distribution (e.g.\n`Continuum Anaconda`_, `Enthought Canopy`_) or from `PyPI`_ via `pip`_.\n`h5py` is also distributed in many Linux Distributions (e.g. Ubuntu, Fedora),\nand in the MacOS package managers `Homebrew <https://brew.sh/>`_,\n`Macports <https://www.macports.org/>`_, or `Fink <http://finkproject.org/>`_.\n\nMore detailed installation instructions, including how to install `h5py` with\nMPI support, can be found at: https://docs.h5py.org/en/latest/build.html.\n\n\nReporting bugs\n--------------\n\nOpen a bug at https://github.com/h5py/h5py/issues.  For general questions, ask\non the list (https://groups.google.com/d/forum/h5py).\n\n.. _`Continuum Anaconda`: http://continuum.io/downloads\n.. _`Enthought Canopy`: https://www.enthought.com/products/canopy/\n.. _`PyPI`: https://pypi.org/project/h5py/\n.. _`pip`: https://pip.pypa.io/en/stable/\n"}, "horovod": {"file_name": "horovod/horovod/README.rst", "raw_text": ".. raw:: html\n\n    <p align=\"center\"><img src=\"https://user-images.githubusercontent.com/16640218/34506318-84d0c06c-efe0-11e7-8831-0425772ed8f2.png\" alt=\"Logo\" width=\"200\"/></p>\n    <br/>\n\nHorovod\n=======\n\n.. image:: https://badge.buildkite.com/6f976bc161c69d9960fc00de01b69deb6199b25680a09e5e26.svg?branch=master\n   :target: https://buildkite.com/horovod/horovod\n   :alt: Build Status\n\n.. image:: https://readthedocs.org/projects/horovod/badge/?version=latest\n   :target: https://horovod.readthedocs.io/en/latest/\n   :alt: Documentation Status\n\n.. image:: https://img.shields.io/badge/License-Apache%202.0-blue.svg\n   :target: https://img.shields.io/badge/License-Apache%202.0-blue.svg\n   :alt: License\n\n.. image:: https://app.fossa.com/api/projects/git%2Bgithub.com%2Fhorovod%2Fhorovod.svg?type=shield\n   :target: https://app.fossa.com/projects/git%2Bgithub.com%2Fhorovod%2Fhorovod?ref=badge_shield\n   :alt: FOSSA Status\n\n.. image:: https://bestpractices.coreinfrastructure.org/projects/2373/badge\n   :target: https://bestpractices.coreinfrastructure.org/projects/2373\n   :alt: CII Best Practices\n\n.. image:: https://pepy.tech/badge/horovod\n   :target: https://pepy.tech/project/horovod\n   :alt: Downloads\n\n.. inclusion-marker-start-do-not-remove\n\n|\n\nHorovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.\nThe goal of Horovod is to make distributed deep learning fast and easy to use.\n\n\n.. raw:: html\n\n   <p><img src=\"https://raw.githubusercontent.com/lfai/artwork/master/lfai/horizontal/color/lfai-color.png\" alt=\"LF AI\" width=\"200\"/></p>\n\n\nHorovod is hosted by the `LF AI Foundation <https://lfdl.io>`_ (LF AI). If you are a company that is deeply\ncommitted to using open source technologies in artificial intelligence, machine, and deep learning, and want to support\nthe communities of open source projects in these domains, consider joining the LF AI Foundation. For details\nabout who's involved and how Horovod plays a role, read the LF AI `announcement <https://lfdl.io/press/2018/12/13/lf-deep-learning-welcomes-horovod-distributed-training-framework-as-newest-project/>`_.\n\n|\n\n.. contents::\n\n\nThe full documentation and an API reference are published at https://horovod.readthedocs.io/en/latest.\n\n|\n\nWhy Horovod?\n------------\nThe primary motivation for this project is to make it easy to take a single-GPU training script and successfully scale\nit to train across many GPUs in parallel. This has two aspects:\n\n1. How much modification does one have to make to a program to make it distributed, and how easy is it to run it?\n2. How much faster would it run in distributed mode?\n\nInternally at Uber we found the MPI model to be much more straightforward and require far less code changes than previous\nsolutions such as Distributed TensorFlow with parameter servers. Once a training script has been written for scale with\nHorovod, it can run on a single-GPU, multiple-GPUs, or even multiple hosts without any further code changes.\nSee the `Usage <#usage>`__ section for more details.\n\nIn addition to being easy to use, Horovod is fast. Below is a chart representing the benchmark that was done on 128\nservers with 4 Pascal GPUs each connected by RoCE-capable 25 Gbit/s network:\n\n.. image:: https://user-images.githubusercontent.com/16640218/38965607-bf5c46ca-4332-11e8-895a-b9c137e86013.png\n   :alt: 512-GPU Benchmark\n\nHorovod achieves 90% scaling efficiency for both Inception V3 and ResNet-101, and 68% scaling efficiency for VGG-16.\nSee `Benchmarks <docs/benchmarks.rst>`_ to find out how to reproduce these numbers.\n\nWhile installing MPI and NCCL itself may seem like an extra hassle, it only needs to be done once by the team dealing\nwith infrastructure, while everyone else in the company who builds the models can enjoy the simplicity of training them at\nscale.\n\n\nInstall\n-------\nTo install Horovod:\n\n1. Install `Open MPI <https://www.open-mpi.org/>`_ or another MPI implementation. Learn how to install Open MPI `on this page <https://www.open-mpi.org/faq/?category=building#easy-build>`_.\n\n   **Note**: Open MPI 3.1.3 has an issue that may cause hangs. The recommended fix is to downgrade to Open MPI 3.1.2 or upgrade to Open MPI 4.0.0.\n\n.. raw:: html\n\n    <p/>\n\n2. If you've installed TensorFlow from `PyPI <https://pypi.org/project/tensorflow>`__, make sure that the ``g++-4.8.5`` or ``g++-4.9`` is installed.\n\n   If you've installed PyTorch from `PyPI <https://pypi.org/project/torch>`__, make sure that the ``g++-4.9`` or above is installed.\n\n   If you've installed either package from `Conda <https://conda.io>`_, make sure that the ``gxx_linux-64`` Conda package is installed.\n\n.. raw:: html\n\n    <p/>\n\n3. Install the ``horovod`` pip package.\n\n   To run on CPUs:\n\n   .. code-block:: bash\n\n      $ pip install horovod\n\n   To run on GPUs with NCCL:\n\n   .. code-block:: bash\n\n      $ HOROVOD_GPU_ALLREDUCE=NCCL HOROVOD_GPU_BROADCAST=NCCL pip install horovod\n\nThis basic installation is good for laptops and for getting to know Horovod.\n\nFor more details on installing Horovod with GPU support, read `Horovod on GPU <docs/gpus.rst>`_.\n\nFor the full list of Horovod installation options, read the `Installation Guide <docs/install.rst>`_.\n\nIf you want to use Docker, read `Horovod in Docker <docs/docker.rst>`_.\n\nTo compile Horovod from source, follow the instructions in the `Contributor Guide <docs/contributors.rst>`_.\n\n\nConcepts\n--------\nHorovod core principles are based on `MPI <http://mpi-forum.org/>`_ concepts such as *size*, *rank*,\n*local rank*, **allreduce**, **allgather** and, *broadcast*. See `this page <docs/concepts.rst>`_ for more details.\n\nSupported frameworks\n--------------------\nSee these pages for Horovod examples and best practices:\n\n- `Horovod with TensorFlow <docs/tensorflow.rst>`_\n- `Horovod with Keras <docs/keras.rst>`_\n- `Horovod with PyTorch <docs/pytorch.rst>`_\n- `Horovod with MXNet <docs/mxnet.rst>`_\n\nUsage\n-----\n\nTo use Horovod, make the following additions to your program:\n\n1. Run ``hvd.init()`` to initialize Horovod.\n\n.. raw:: html\n\n    <p/>\n\n2. Pin each GPU to a single process to avoid resource contention.\n\n   With the typical setup of one GPU per process, set this to *local rank*. The first process on\n   the server will be allocated the first GPU, the second process will be allocated the second GPU, and so forth.\n\n.. raw:: html\n\n    <p/>\n\n\n3. Scale the learning rate by the number of workers.\n\n   Effective batch size in synchronous distributed training is scaled by the number of workers.\n   An increase in learning rate compensates for the increased batch size.\n\n.. raw:: html\n\n    <p/>\n\n\n4. Wrap the optimizer in ``hvd.DistributedOptimizer``.\n\n   The distributed optimizer delegates gradient computation to the original optimizer, averages gradients using **allreduce** or **allgather**, and then applies those averaged gradients.\n\n.. raw:: html\n\n    <p/>\n\n\n5. Broadcast the initial variable states from rank 0 to all other processes.\n\n   This is necessary to ensure consistent initialization of all workers when training is started with random weights or restored from a checkpoint.\n\n.. raw:: html\n\n    <p/>\n\n\n6. Modify your code to save checkpoints only on worker 0 to prevent other workers from corrupting them.\n\n.. raw:: html\n\n    <p/>\n\n\nExample using TensorFlow v1 (see the `examples <https://github.com/horovod/horovod/blob/master/examples/>`_ directory for full training examples):\n\n.. code-block:: python\n\n    import tensorflow as tf\n    import horovod.tensorflow as hvd\n\n\n    # Initialize Horovod\n    hvd.init()\n\n    # Pin GPU to be used to process local rank (one GPU per process)\n    config = tf.ConfigProto()\n    config.gpu_options.visible_device_list = str(hvd.local_rank())\n\n    # Build model...\n    loss = ...\n    opt = tf.train.AdagradOptimizer(0.01 * hvd.size())\n\n    # Add Horovod Distributed Optimizer\n    opt = hvd.DistributedOptimizer(opt)\n\n    # Add hook to broadcast variables from rank 0 to all other processes during\n    # initialization.\n    hooks = [hvd.BroadcastGlobalVariablesHook(0)]\n\n    # Make training operation\n    train_op = opt.minimize(loss)\n\n    # Save checkpoints only on worker 0 to prevent other workers from corrupting them.\n    checkpoint_dir = '/tmp/train_logs' if hvd.rank() == 0 else None\n\n    # The MonitoredTrainingSession takes care of session initialization,\n    # restoring from a checkpoint, saving to a checkpoint, and closing when done\n    # or an error occurs.\n    with tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,\n                                           config=config,\n                                           hooks=hooks) as mon_sess:\n      while not mon_sess.should_stop():\n        # Perform synchronous training.\n        mon_sess.run(train_op)\n\n\nRunning Horovod\n---------------\nThe example commands below show how to run distributed training.\nSee `Run Horovod <docs/running.rst>`_ for more details, including RoCE/InfiniBand tweaks and tips for dealing with hangs.\n\n1. To run on a machine with 4 GPUs:\n\n   .. code-block:: bash\n\n        $ horovodrun -np 4 -H localhost:4 python train.py\n\n2. To run on 4 machines with 4 GPUs each:\n\n   .. code-block:: bash\n\n       $ horovodrun -np 16 -H server1:4,server2:4,server3:4,server4:4 python train.py\n\n3. To run using Open MPI without the ``horovodrun`` wrapper, see `Running Horovod with Open MPI <docs/mpirun.rst>`_.\n\n4. To run in Docker, see `Horovod in Docker <docs/docker.rst>`_.\n\n5. To run in Kubernetes, see `Kubeflow <https://github.com/kubeflow/examples/tree/master/demos/yelp_demo/ks_app/vendor/kubeflow/mpi-job>`_, `MPI Operator <https://github.com/kubeflow/mpi-operator/>`_, `Helm Chart <https://github.com/kubernetes/charts/tree/master/stable/horovod/>`_, and `FfDL <https://github.com/IBM/FfDL/tree/master/etc/examples/horovod/>`_.\n\n6. To run in Spark, see `Spark <docs/spark.rst>`_.\n\n7. To run in Singularity, see `Singularity <https://github.com/sylabs/examples/tree/master/machinelearning/horovod>`_.\n\n8. To run in a LSF HPC cluster (e.g. Summit), see `LSF <docs/lsf.rst>`_.\n\nGloo\n----\n`Gloo <https://github.com/facebookincubator/gloo>`_ is an open source collective communications library developed by Facebook.\n\nGloo comes included with Horovod, and allows users to run Horovod without requiring MPI to be installed. Gloo support only requires\nthat you have `CMake <https://cmake.org/>`_ installed, and is only supported on Linux at this time.\n\nFor environments that have support both MPI and Gloo, you can choose to use Gloo at runtime by passing the ``--gloo`` argument to ``horovodrun``:\n\n.. code-block:: bash\n\n     $ horovodrun --gloo -np 2 python train.py\n\nGloo support is still early in its development, and more features are coming soon.\n\nmpi4py\n------\nHorovod supports mixing and matching Horovod collectives with other MPI libraries, such as `mpi4py <https://mpi4py.scipy.org>`_,\nprovided that the MPI was built with multi-threading support.\n\nYou can check for MPI multi-threading support by querying the ``hvd.mpi_threads_supported()`` function.\n\n.. code-block:: python\n\n    import horovod.tensorflow as hvd\n\n    # Initialize Horovod\n    hvd.init()\n\n    # Verify that MPI multi-threading is supported.\n    assert hvd.mpi_threads_supported()\n\n    from mpi4py import MPI\n    assert hvd.size() == MPI.COMM_WORLD.Get_size()\n\nYou can also initialize Horovod with an `mpi4py` sub-communicator, in which case each sub-communicator\nwill run an independent Horovod training.\n\n.. code-block:: python\n\n    from mpi4py import MPI\n    import horovod.tensorflow as hvd\n\n    # Split COMM_WORLD into subcommunicators\n    subcomm = MPI.COMM_WORLD.Split(color=MPI.COMM_WORLD.rank % 2,\n                                   key=MPI.COMM_WORLD.rank)\n\n    # Initialize Horovod\n    hvd.init(comm=subcomm)\n\n    print('COMM_WORLD rank: %d, Horovod rank: %d' % (MPI.COMM_WORLD.rank, hvd.rank()))\n\n\nInference\n---------\nLearn how to optimize your model for inference and remove Horovod operations from the graph `here <docs/inference.rst>`_.\n\n\nTensor Fusion\n-------------\nOne of the unique things about Horovod is its ability to interleave communication and computation coupled with the ability\nto batch small **allreduce** operations, which results in improved performance. We call this batching feature Tensor Fusion.\n\nSee `here <docs/tensor-fusion.rst>`__ for full details and tweaking instructions.\n\n\nHorovod Timeline\n----------------\nHorovod has the ability to record the timeline of its activity, called Horovod Timeline.\n\n.. image:: https://user-images.githubusercontent.com/16640218/29735271-9e148da0-89ac-11e7-9ae0-11d7a099ac89.png\n   :alt: Horovod Timeline\n\nUse Horovod timeline to analyze Horovod performance.\nSee `here <docs/timeline.rst>`__ for full details and usage instructions.\n\n\nAutomated Performance Tuning\n----------------------------\nSelecting the right values to efficiently make use of Tensor Fusion and other advanced Horovod features can involve\na good amount of trial and error. We provide a system to automate this performance optimization process called\n**autotuning**, which you can enable with a single command line argument to ``horovodrun``.\n\nSee `here <docs/autotune.rst>`__ for full details and usage instructions.\n\n\nGuides\n------\n1. Run distributed training in Microsoft Azure using `Batch AI and Horovod <https://github.com/Azure/BatchAI/tree/master/recipes/Horovod>`_.\n\nSend us links to any user guides you want to publish on this site\n\nTroubleshooting\n---------------\nSee `Troubleshooting <docs/troubleshooting.rst>`_ and submit a `ticket <https://github.com/horovod/horovod/issues/new>`_\nif you can't find an answer.\n\n\nCitation\n--------\nPlease cite Horovod in your publications if it helps your research:\n\n::\n\n    @article{sergeev2018horovod,\n      Author = {Alexander Sergeev and Mike Del Balso},\n      Journal = {arXiv preprint arXiv:1802.05799},\n      Title = {Horovod: fast and easy distributed deep learning in {TensorFlow}},\n      Year = {2018}\n    }\n\n\nPublications\n------------\n1. Sergeev, A., Del Balso, M. (2017) *Meet Horovod: Uber\u2019s Open Source Distributed Deep Learning Framework for TensorFlow*.\nRetrieved from `https://eng.uber.com/horovod/ <https://eng.uber.com/horovod/>`_\n\n2. Sergeev, A. (2017) *Horovod - Distributed TensorFlow Made Easy*. Retrieved from\n`https://www.slideshare.net/AlexanderSergeev4/horovod-distributed-tensorflow-made-easy <https://www.slideshare.net/AlexanderSergeev4/horovod-distributed-tensorflow-made-easy>`_\n\n3. Sergeev, A., Del Balso, M. (2018) *Horovod: fast and easy distributed deep learning in TensorFlow*. Retrieved from\n`arXiv:1802.05799 <https://arxiv.org/abs/1802.05799>`_\n\n\nReferences\n----------\nThe Horovod source code was based off the Baidu `tensorflow-allreduce <https://github.com/baidu-research/tensorflow-allreduce>`_\nrepository written by Andrew Gibiansky and Joel Hestness. Their original work is described in the article\n`Bringing HPC Techniques to Deep Learning <http://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/>`_.\n\nMailing lists\n-------------\nSubscribe to `Horovod Announce <https://lists.lfai.foundation/g/horovod-announce>`_ and \n`Horovod Technical-Discuss <https://lists.lfai.foundation/g/horovod-technical-discuss>`_ to stay up to date.\n\n\n.. inclusion-marker-end-do-not-remove\n   Place contents above here if they should also appear in read-the-docs.\n   Contents below are already part of the read-the-docs table of contents.\n"}, "hpbandster": {"file_name": "automl/HpBandSter/README.md", "raw_text": "# HpBandSter [![Build Status](https://travis-ci.org/automl/HpBandSter.svg?branch=master)](https://travis-ci.org/automl/HpBandSter)  [![codecov](https://codecov.io/gh/automl/HpBandSter/branch/master/graph/badge.svg)](https://codecov.io/gh/automl/HpBandSter)\na distributed Hyperband implementation on Steroids\n\nThis python 3 package is a framework for distributed hyperparameter optimization.\nIt started out as a simple implementation of [Hyperband (Li et al. 2017)](http://jmlr.org/papers/v18/16-558.html), and contains\nan implementation of [BOHB (Falkner et al. 2018)](http://proceedings.mlr.press/v80/falkner18a.html)\n\n## How to install\n\nWe try to keep the package on PyPI up to date. So you should be able to install it via:\n```\npip install hpbandster\n```\nIf you want to develop on the code you could install it via:\n\n```\npython3 setup.py develop --user\n```\n\n## Documentation\n\nThe documentation is hosted on github pages: [https://automl.github.io/HpBandSter/](https://automl.github.io/HpBandSter/)\nIt contains a quickstart guide with worked out examples to get you started in different circumstances.\nCheck it out if you are interest in applying one of the implemented optimizers to your problem.\n\nWe have also written a [blogpost](https://www.automl.org/blog_bohb/) showcasing the results from our ICML paper.\n"}, "interpret": {"file_name": "interpretml/interpret/README.md", "raw_text": "\n\n# InterpretML - Alpha Release\n\n![License](https://img.shields.io/github/license/microsoft/interpret.svg?style=flat-square)\n![Python Version](https://img.shields.io/pypi/pyversions/interpret.svg?style=flat-square)\n![Package Version](https://img.shields.io/pypi/v/interpret.svg?style=flat-square)\n![Build Status](https://img.shields.io/azure-devops/build/ms/interpret/293/master.svg?style=flat-square)\n![Coverage](https://img.shields.io/azure-devops/coverage/ms/interpret/293/master.svg?style=flat-square)\n![Maintenance](https://img.shields.io/maintenance/yes/2020?style=flat-square)\n\n<br/>\n\n> ### In the beginning machines learned in darkness, and data scientists struggled in the void to explain them.\n>\n> ### Let there be light.\n\n<br/>\n\nInterpretML is an open-source python package for training interpretable machine learning models and explaining blackbox systems. Interpretability is essential for:\n- Model debugging - Why did my model make this mistake?\n- Detecting bias - Does my model discriminate?\n- Human-AI cooperation - How can I understand and trust the model's decisions?\n- Regulatory compliance - Does my model satisfy legal requirements?\n- High-risk applications - Healthcare, finance, judicial, ...\n\nHistorically, the most interpretable machine learning models were not very accurate, and the most accurate models were not very interpretable. Microsoft Research has developed an algorithm called the Explainable Boosting Machine (EBM)<sup>[*](#ebm-footnote)</sup> which has both high accuracy and interpretability. EBM uses modern machine learning techniques like bagging and gradient boosting to breathe new life into traditional GAMs (Generalized Additive Models). This makes them as accurate as random forests and gradient boosted trees, and also enhances their intelligibility and editability.\n\n<br/>\n\n[*Notebook for reproducing table*](https://nbviewer.jupyter.org/github/interpretml/interpret/blob/master/benchmarks/EBM%20Classification%20Comparison.ipynb)\n\n| Dataset/AUROC | Domain  | Logistic Regression | Random Forest | XGBoost        | Explainable Boosting Machine |\n|---------------|---------|:-------------------:|:-------------:|:--------------:|:----------------------------:|\n| Adult Income  | Finance | .907\u00b1.003           | .903\u00b1.002     | .922\u00b1.002      | **_.928\u00b1.002_**              |\n| Heart Disease | Medical | .895\u00b1.030           | .890\u00b1.008     | .870\u00b1.014      | **_.916\u00b1.010_**              |\n| Breast Cancer | Medical | **_.995\u00b1.005_**     | .992\u00b1.009     | **_.995\u00b1.006_**| **_.995\u00b1.006_**              |\n| Telecom Churn | Business| .804\u00b1.015           | .824\u00b1.002     | .850\u00b1.006      | **_.851\u00b1.005_**              |\n| Credit Fraud  | Security| .979\u00b1.002           | .950\u00b1.007     | **_.981\u00b1.003_**| .975\u00b1.005                    |\n\n<br/>\n\nIn addition to EBM, InterpretML also supports methods like LIME, SHAP, linear models, partial dependence, decision trees and rule lists.  The package makes it easy to compare and contrast models to find the best one for your needs.\n\n<a name=\"ebm-footnote\">*</a> *EBM is a fast implementation of GA<sup>2</sup>M. Details on the algorithm can be found [here](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/KDD2015FinalDraftIntelligibleModels4HealthCare_igt143e-caruanaA.pdf).*\n\n---\n\n## Installation\n\nPython 3.5+ | Linux, Mac OS X, Windows\n```sh\npip install -U interpret\n```\n\n## Getting Started\n\nLet's fit an Explainable Boosting Machine\n\n```python\nfrom interpret.glassbox import ExplainableBoostingClassifier\n\nebm = ExplainableBoostingClassifier()\nebm.fit(X_train, y_train)\n\n# EBM supports pandas dataframes, numpy arrays, and handles \"string\" data natively.\n```\n\nUnderstand the model\n```python\nfrom interpret import show\n\nebm_global = ebm.explain_global()\nshow(ebm_global)\n```\n![Global Explanation Image](examples/python/assets/readme_ebm_global_specific.PNG?raw=true)\n\n<br/>\n\nUnderstand individual predictions\n```python\nebm_local = ebm.explain_local(X_test, y_test)\nshow(ebm_local)\n```\n![Local Explanation Image](examples/python/assets/readme_ebm_local_specific.PNG?raw=true)\n\n<br/>\n\nAnd if you have multiple models, compare them\n```python\nshow([logistic_regression, decision_tree])\n```\n![Dashboard Image](examples/python/assets/readme_dashboard.PNG?raw=true)\n\n<br/>\n\n## Example Notebooks\n\n- [Interpretable machine learning models for binary classification](https://nbviewer.jupyter.org/github/interpretml/interpret/blob/master/examples/python/notebooks/Interpretable%20Classification%20Methods.ipynb)\n- [Interpretable machine learning models for regression](https://nbviewer.jupyter.org/github/interpretml/interpret/blob/master/examples/python/notebooks/Interpretable%20Regression%20Methods.ipynb)\n- [Blackbox interpretability for binary classification](https://nbviewer.jupyter.org/github/interpretml/interpret/blob/master/examples/python/notebooks/Explaining%20Blackbox%20Classifiers.ipynb)\n- [Blackbox interpretability for regression](https://nbviewer.jupyter.org/github/interpretml/interpret/blob/master/examples/python/notebooks/Explaining%20Blackbox%20Regressors.ipynb)\n\n## Roadmap\n\nCurrently we're working on:\n- R language interface (R is currently a WIP. Basic EBM classification can be done via the ebm_classify & ebm_predict_proba functions, but the predictions are a bit less accurate than in python. No plotting included yet, but other R plotting tools can do a basic job visualizing EBM models)\n- Missing Values Support\n- Improved Categorical Encoding\n- Interaction effect purification (see citations for details)\n\n...and lots more! Get in touch to find out more.\n\n## Contributing\n\nIf you are interested contributing directly to the code base, please see [CONTRIBUTING.md](./CONTRIBUTING.md).\n\n## Acknowledgements\n\nInterpretML was originally created by (equal contributions): Samuel Jenkins, Harsha Nori, Paul Koch, and Rich Caruana\n\nMany people have supported us along the way. Check out [ACKNOWLEDGEMENTS.md](./ACKNOWLEDGEMENTS.md)!\n\nWe also build on top of many great packages. Please check them out!\n\n[plotly](https://github.com/plotly/plotly.py) |\n[dash](https://github.com/plotly/dash) | \n[scikit-learn](https://github.com/scikit-learn/scikit-learn) | \n[lime](https://github.com/marcotcr/lime) |\n[shap](https://github.com/slundberg/shap) |\n[salib](https://github.com/SALib/SALib) |\n[skope-rules](https://github.com/scikit-learn-contrib/skope-rules) |\n[treeinterpreter](https://github.com/andosa/treeinterpreter) |\n[gevent](https://github.com/gevent/gevent) | \n[joblib](https://github.com/joblib/joblib) |\n[pytest](https://github.com/pytest-dev/pytest) | \n[jupyter](https://github.com/jupyter/notebook) \n\n\n## Citations\n\n<br/>\n<details open>\n  <summary><strong>InterpretML</strong></summary>\n  <hr/>\n  <details open>\n    <summary>\n      <em>\"InterpretML: A Unified Framework for Machine Learning Interpretability\" (H. Nori, S. Jenkins, P. Koch, and R.\n        Caruana 2019)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{nori2019interpretml,\n  title={InterpretML: A Unified Framework for Machine Learning Interpretability},\n  author={Nori, Harsha and Jenkins, Samuel and Koch, Paul and Caruana, Rich},\n  journal={arXiv preprint arXiv:1909.09223},\n  year={2019}\n}\n</pre>\n    <a href=\"https://arxiv.org/pdf/1909.09223.pdf\">Paper link</a>\n  </details>\n  <hr/>\n</details>\n\n<details>\n  <summary><strong>Explainable Boosting</strong></summary>\n  <hr/>\n  <details>\n    <summary>\n      <em>\"Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission\" (R. Caruana,\n        Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad 2015)</em>\n    </summary>\n    <br/>\n    <pre>\n@inproceedings{caruana2015intelligible,\n  title={Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission},\n  author={Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},\n  booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\n  pages={1721--1730},\n  year={2015},\n  organization={ACM}\n}\n</pre>\n    <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/KDD2015FinalDraftIntelligibleModels4HealthCare_igt143e-caruanaA.pdf\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Accurate intelligible models with pairwise interactions\" (Y. Lou, R. Caruana, J. Gehrke, and G. Hooker\n        2013)</em>\n    </summary>\n    <br/>\n    <pre>\n@inproceedings{lou2013accurate,\n  title={Accurate intelligible models with pairwise interactions},\n  author={Lou, Yin and Caruana, Rich and Gehrke, Johannes and Hooker, Giles},\n  booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},\n  pages={623--631},\n  year={2013},\n  organization={ACM}\n}\n</pre>\n    <a href=\"http://www.cs.cornell.edu/~yinlou/papers/lou-kdd13.pdf\">Paper link</a>\n  </details>\n  <details>\n    <summary>\n      <em>\"Intelligible models for classification and regression\" (Y. Lou, R. Caruana, and J. Gehrke 2012)</em>\n    </summary>\n    <br/>\n    <pre>\n@inproceedings{lou2012intelligible,\n  title={Intelligible models for classification and regression},\n  author={Lou, Yin and Caruana, Rich and Gehrke, Johannes},\n  booktitle={Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining},\n  pages={150--158},\n  year={2012},\n  organization={ACM}\n}\n\n</pre>\n    <a href=\"https://www.cs.cornell.edu/~yinlou/papers/lou-kdd12.pdf\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Axiomatic Interpretability for Multiclass Additive Models\" (X. Zhang, S. Tan, P. Koch, Y. Lou, U. Chajewska, and R. Caruana 2019)</em>\n    </summary>\n    <br/>\n    <pre>\n@inproceedings{zhang2019axiomatic,\n  title={Axiomatic Interpretability for Multiclass Additive Models},\n  author={Zhang, Xuezhou and Tan, Sarah and Koch, Paul and Lou, Yin and Chajewska, Urszula and Caruana, Rich},\n  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining},\n  pages={226--234},\n  year={2019},\n  organization={ACM}\n}\n</pre>\n    <a href=\"https://arxiv.org/pdf/1810.09092.pdf\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Distill-and-compare: auditing black-box models using transparent model distillation\" (S. Tan, R. Caruana, G. Hooker, and Y. Lou 2018)</em>\n    </summary>\n    <br/>\n    <pre>\n@inproceedings{tan2018distill,\n  title={Distill-and-compare: auditing black-box models using transparent model distillation},\n  author={Tan, Sarah and Caruana, Rich and Hooker, Giles and Lou, Yin},\n  booktitle={Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},\n  pages={303--310},\n  year={2018},\n  organization={ACM}\n}\n</pre>\n    <a href=\"https://arxiv.org/pdf/1710.06169\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models\" (B. Lengerich, S. Tan, C. Chang, G. Hooker, R. Caruana 2019)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{lengerich2019purifying,\n  title={Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models},\n  author={Lengerich, Benjamin and Tan, Sarah and Chang, Chun-Hao and Hooker, Giles and Caruana, Rich},\n  journal={arXiv preprint arXiv:1911.04974},\n  year={2019}\n}\n</pre>\n    <a href=\"https://arxiv.org/pdf/1911.04974.pdf\">Paper link</a>\n  </details>\n\n  <hr/>\n</details>\n\n<details>\n  <summary><strong>LIME</strong></summary>\n  <hr/>\n  <details>\n    <summary>\n      <em>\"Why should i trust you?: Explaining the predictions of any classifier\" (M. T. Ribeiro, S. Singh, and C. Guestrin 2016)</em>\n    </summary>\n    <br/>\n    <pre>\n@inproceedings{ribeiro2016should,\n  title={Why should i trust you?: Explaining the predictions of any classifier},\n  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},\n  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},\n  pages={1135--1144},\n  year={2016},\n  organization={ACM}\n}\n</pre>\n    <a href=\"https://arxiv.org/pdf/1602.04938.pdf\">Paper link</a>\n  </details>\n  <hr/>\n</details>\n\n<details>\n  <summary><strong>SHAP</strong></summary>\n  <hr/>\n  <details>\n    <summary>\n      <em>\"A Unified Approach to Interpreting Model Predictions\" (S. M. Lundberg and S.-I. Lee 2017)</em>\n    </summary>\n    <br/>\n    <pre>\n@incollection{NIPS2017_7062,\n title = {A Unified Approach to Interpreting Model Predictions},\n author = {Lundberg, Scott M and Lee, Su-In},\n booktitle = {Advances in Neural Information Processing Systems 30},\n editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {4765--4774},\n year = {2017},\n publisher = {Curran Associates, Inc.},\n url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}\n}\n</pre>\n    <a href=\"http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf\">Paper link</a>\n  </details>\n  <details>\n    <summary>\n      <em>\"Consistent individualized feature attribution for tree ensembles\" (Lundberg, Scott M and Erion, Gabriel G and Lee, Su-In 2018)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{lundberg2018consistent,\n  title={Consistent individualized feature attribution for tree ensembles},\n  author={Lundberg, Scott M and Erion, Gabriel G and Lee, Su-In},\n  journal={arXiv preprint arXiv:1802.03888},\n  year={2018}\n}\n</pre>\n    <a href=\"https://arxiv.org/pdf/1802.03888\">Paper link</a>\n  </details>\n  <details>\n    <summary>\n      <em>\"Explainable machine-learning predictions for the prevention of hypoxaemia during surgery\" (S. M. Lundberg et al. 2018)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{lundberg2018explainable,\n  title={Explainable machine-learning predictions for the prevention of hypoxaemia during surgery},\n  author={Lundberg, Scott M and Nair, Bala and Vavilala, Monica S and Horibe, Mayumi and Eisses, Michael J and Adams, Trevor and Liston, David E and Low, Daniel King-Wai and Newman, Shu-Fang and Kim, Jerry and others},\n  journal={Nature Biomedical Engineering},\n  volume={2},\n  number={10},\n  pages={749},\n  year={2018},\n  publisher={Nature Publishing Group}\n}\n</pre>\n    <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6467492/pdf/nihms-1505578.pdf\">Paper link</a>\n  </details>\n  <hr/>\n</details>\n\n<details>\n  <summary><strong>Sensitivity Analysis</strong></summary>\n  <hr/>\n  <details>\n    <summary>\n      <em>\"SALib: An open-source Python library for Sensitivity Analysis\" (J. D. Herman and W. Usher 2017)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{herman2017salib,\n  title={SALib: An open-source Python library for Sensitivity Analysis.},\n  author={Herman, Jonathan D and Usher, Will},\n  journal={J. Open Source Software},\n  volume={2},\n  number={9},\n  pages={97},\n  year={2017}\n}\n</pre>\n    <a href=\"https://www.researchgate.net/profile/Will_Usher/publication/312204236_SALib_An_open-source_Python_library_for_Sensitivity_Analysis/links/5ac732d64585151e80a39547/SALib-An-open-source-Python-library-for-Sensitivity-Analysis.pdf?origin=publication_detail\">Paper link</a>\n  </details>\n  <details>\n    <summary>\n      <em>\"Factorial sampling plans for preliminary computational experiments\" (M. D. Morris 1991)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{morris1991factorial,\n  title={},\n  author={Morris, Max D},\n  journal={Technometrics},\n  volume={33},\n  number={2},\n  pages={161--174},\n  year={1991},\n  publisher={Taylor \\& Francis Group}\n}\n</pre>\n    <a href=\"https://abe.ufl.edu/Faculty/jjones/ABE_5646/2010/Morris.1991%20SA%20paper.pdf\">Paper link</a>\n  </details>\n  <hr/>\n</details>\n\n<details>\n  <summary><strong>Partial Dependence</strong></summary>\n  <hr/>\n  <details>\n    <summary>\n      <em>\"Greedy function approximation: a gradient boosting machine\" (J. H. Friedman 2001)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{friedman2001greedy,\n  title={Greedy function approximation: a gradient boosting machine},\n  author={Friedman, Jerome H},\n  journal={Annals of statistics},\n  pages={1189--1232},\n  year={2001},\n  publisher={JSTOR}\n}\n    </pre>\n    <a href=\"https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451\">Paper link</a>\n  </details>\n  <hr/>\n</details>\n\n\n\n<details>\n  <summary><strong>Open Source Software</strong></summary>\n  <hr/>\n  <details>\n    <summary>\n      <em>\"Scikit-learn: Machine learning in Python\" (F. Pedregosa et al. 2011)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{pedregosa2011scikit,\n  title={Scikit-learn: Machine learning in Python},\n  author={Pedregosa, Fabian and Varoquaux, Ga{\\\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},\n  journal={Journal of machine learning research},\n  volume={12},\n  number={Oct},\n  pages={2825--2830},\n  year={2011}\n}\n</pre>\n    <a href=\"http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf\">Paper link</a>\n  </details>\n<details>\n    <summary>\n      <em>\"Collaborative data science\" (Plotly Technologies Inc. 2015)</em>\n    </summary>\n    <br/>\n    <pre>\n@online{plotly, \n  author = {Plotly Technologies Inc.}, \n  title = {Collaborative data science}, \n  publisher = {Plotly Technologies Inc.}, \n  address = {Montreal, QC}, \n  year = {2015}, \n  url = {https://plot.ly} }\n  </pre>\n    <a href=\"https://plot.ly\">Link</a>\n</details>\n<details>\n    <summary>\n      <em>\"Joblib: running python function as pipeline jobs\" (G. Varoquaux and O. Grisel 2009)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{varoquaux2009joblib,\n  title={Joblib: running python function as pipeline jobs},\n  author={Varoquaux, Ga{\\\"e}l and Grisel, O},\n  journal={packages. python. org/joblib},\n  year={2009}\n}\n  </pre>\n    <a href=\"https://joblib.readthedocs.io/en/latest/\">Link</a>\n</details>\n  \n  \n  <hr/>\n</details>\n\n\n## External links\n\n- [A gentle introduction to GA2Ms, a white box model](https://blog.fiddler.ai/2019/06/a-gentle-introduction-to-ga2ms-a-white-box-model)\n- [On Model Explainability: From LIME, SHAP, to Explainable Boosting](https://everdark.github.io/k9/notebooks/ml/model_explain/model_explain.nb.html)\n- [Benchmarking and MLI experiments on the Adult dataset](https://github.com/sayakpaul/Benchmarking-and-MLI-experiments-on-the-Adult-dataset/blob/master/Benchmarking_experiments_on_the_Adult_dataset_and_interpretability.ipynb)\n- [Dealing with Imbalanced Data (Mortgage loans defaults)](https://mikewlange.github.io/ImbalancedData-/index.html)\n- [Kaggle PGA Tour analysis by GAM](https://www.kaggle.com/juyamagu/pga-tour-analysis-by-gam)\n- [Interpretable Prediction of Goals in Soccer](http://statsbomb.com/wp-content/uploads/2019/10/decroos-interpretability-statsbomb.pdf)\n- [Explaining Model Pipelines With InterpretML](https://medium.com/@mariusvadeika/explaining-model-pipelines-with-interpretml-a9214f75400b)\n- [Explain Your Model with Microsoft\u2019s InterpretML](https://medium.com/@Dataman.ai/explain-your-model-with-microsofts-interpretml-5daab1d693b4)\n\n## Contact us\n\nThere are multiple ways to get in touch:\n- Email us at interpret@microsoft.com\n- Or, feel free to raise a GitHub issue\n\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n\n> ### If a tree fell in your random forest, would anyone notice?\n"}, "jellyfish": {"file_name": "jamesturk/jellyfish/README.rst", "raw_text": "=========\njellyfish\n=========\n\n.. image:: https://travis-ci.com/jamesturk/jellyfish.svg?branch=master\n    :target: https://travis-ci.com/jamesturk/jellyfish\n\n.. image:: https://coveralls.io/repos/jamesturk/jellyfish/badge.png?branch=master\n    :target: https://coveralls.io/r/jamesturk/jellyfish\n\n.. image:: https://img.shields.io/pypi/v/jellyfish.svg\n    :target: https://pypi.python.org/pypi/jellyfish\n\n.. image:: https://readthedocs.org/projects/jellyfish/badge/?version=latest\n    :target: https://readthedocs.org/projects/jellyfish/?badge=latest\n    :alt: Documentation Status\n\n.. image:: https://ci.appveyor.com/api/projects/status/9xeyl1f5sd5pl40h?svg=true\n    :target: https://ci.appveyor.com/project/jamesturk/jellyfish/\n\nJellyfish is a python library for doing approximate and phonetic matching of strings.\n\nWritten by James Turk <dev@jamesturk.net> and Michael Stephens.\n\nSee https://github.com/jamesturk/jellyfish/graphs/contributors for contributors.\n\nSee http://jellyfish.readthedocs.io for documentation.\n\nSource is available at http://github.com/jamesturk/jellyfish.\n\n**Jellyfish >= 0.7 only supports Python 3, if you need Python 2 please use 0.6.x.**\n\nIncluded Algorithms\n===================\n\nString comparison:\n\n* Levenshtein Distance\n* Damerau-Levenshtein Distance\n* Jaro Distance\n* Jaro-Winkler Distance\n* Match Rating Approach Comparison\n* Hamming Distance\n\nPhonetic encoding:\n\n* American Soundex\n* Metaphone\n* NYSIIS (New York State Identification and Intelligence System)\n* Match Rating Codex\n\nExample Usage\n=============\n\n>>> import jellyfish\n>>> jellyfish.levenshtein_distance(u'jellyfish', u'smellyfish')\n2\n>>> jellyfish.jaro_distance(u'jellyfish', u'smellyfish')\n0.89629629629629637\n>>> jellyfish.damerau_levenshtein_distance(u'jellyfish', u'jellyfihs')\n1\n\n>>> jellyfish.metaphone(u'Jellyfish')\n'JLFX'\n>>> jellyfish.soundex(u'Jellyfish')\n'J412'\n>>> jellyfish.nysiis(u'Jellyfish')\n'JALYF'\n>>> jellyfish.match_rating_codex(u'Jellyfish')\n'JLLFSH'\n\nRunning Tests\n=============\n\nIf you are interested in contributing to Jellyfish, you may want to\nrun tests locally. Jellyfish uses tox_ to run tests, which you can\nsetup and run as follows::\n\n  pip install tox\n  # cd jellyfish/\n  tox\n\n.. _tox: https://tox.readthedocs.io/en/latest/\n"}, "jax": {"file_name": "google/jax/README.md", "raw_text": "<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png\" alt=\"logo\"></img>\n</div>\n\n# JAX: Autograd and XLA [![Test status](https://travis-ci.org/google/jax.svg?branch=master)](https://travis-ci.org/google/jax)\n\n[**Quickstart**](#quickstart-colab-in-the-cloud)\n| [**Transformations**](#transformations)\n| [**Install guide**](#installation)\n| [**Change logs**](https://jax.readthedocs.io/en/latest/CHANGELOG.html)\n| [**Reference docs**](https://jax.readthedocs.io/en/latest/)\n| [**Code search**](https://cs.opensource.google/jax/jax)\n\n**Announcement:** JAX has dropped Python 2 support, and requires Python 3.6 or newer. See [docs/CHANGELOG.rst](https://jax.readthedocs.io/en/latest/CHANGELOG.html).\n\n## What is JAX?\n\nJAX is [Autograd](https://github.com/hips/autograd) and\n[XLA](https://www.tensorflow.org/xla),\nbrought together for high-performance machine learning research.\n\nWith its updated version of [Autograd](https://github.com/hips/autograd),\nJAX can automatically differentiate native\nPython and NumPy functions. It can differentiate through loops, branches,\nrecursion, and closures, and it can take derivatives of derivatives of\nderivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)\nvia [`grad`](#automatic-differentiation-with-grad) as well as forward-mode differentiation,\nand the two can be composed arbitrarily to any order.\n\nWhat\u2019s new is that JAX uses\n[XLA](https://www.tensorflow.org/xla)\nto compile and run your NumPy programs on GPUs and TPUs. Compilation happens\nunder the hood by default, with library calls getting just-in-time compiled and\nexecuted. But JAX also lets you just-in-time compile your own Python functions\ninto XLA-optimized kernels using a one-function API,\n[`jit`](#compilation-with-jit). Compilation and automatic differentiation can be\ncomposed arbitrarily, so you can express sophisticated algorithms and get\nmaximal performance without leaving Python. You can even program multiple GPUs\nor TPU cores at once using [`pmap`](#spmd-programming-with-pmap), and\ndifferentiate through the whole thing.\n\nDig a little deeper, and you'll see that JAX is really an extensible system for\n[composable function transformations](#transformations). Both\n[`grad`](#automatic-differentiation-with-grad) and [`jit`](#compilation-with-jit)\nare instances of such transformations. Others are\n[`vmap`](#auto-vectorization-with-vmap) for automatic vectorization and\n[`pmap`](#spmd-programming-with-pmap) for single-program multiple-data (SPMD)\nparallel programming of multiple accelerators, with more to come.\n\nThis is a research project, not an official Google product. Expect bugs and\n[sharp edges](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html).\nPlease help by trying it out, [reporting\nbugs](https://github.com/google/jax/issues), and letting us know what you\nthink!\n\n```python\nimport jax.numpy as np\nfrom jax import grad, jit, vmap\n\ndef predict(params, inputs):\n  for W, b in params:\n    outputs = np.dot(inputs, W) + b\n    inputs = np.tanh(outputs)\n  return outputs\n\ndef logprob_fun(params, inputs, targets):\n  preds = predict(params, inputs)\n  return np.sum((preds - targets)**2)\n\ngrad_fun = jit(grad(logprob_fun))  # compiled gradient evaluation function\nperex_grads = jit(vmap(grad_fun, in_axes=(None, 0, 0)))  # fast per-example grads\n```\n\n### Contents\n* [Quickstart: Colab in the Cloud](#quickstart-colab-in-the-cloud)\n* [Transformations](#transformations)\n* [Current gotchas](#current-gotchas)\n* [Installation](#installation)\n* [Citing JAX](#citing-jax)\n* [Reference documentation](#reference-documentation)\n\n## Quickstart: Colab in the Cloud\nJump right in using a notebook in your browser, connected to a Google Cloud GPU.\nHere are some starter notebooks:\n- [The basics: NumPy on accelerators, `grad` for differentiation, `jit` for compilation, and `vmap` for vectorization](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)\n- [Training a Simple Neural Network, with TensorFlow Dataset Data Loading](https://colab.research.google.com/github/google/jax/blob/master/docs/notebooks/neural_network_with_tfds_data.ipynb)\n\n**JAX now runs on Cloud TPUs.** To try out the preview, see the [Cloud TPU\nColabs](https://github.com/google/jax/tree/master/cloud_tpu_colabs).\n\nFor a deeper dive into JAX:\n- [The Autodiff Cookbook, Part 1: easy and powerful automatic differentiation in JAX](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)\n- [Common gotchas and sharp edges](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html)\n- See the [full list of\nnotebooks](https://github.com/google/jax/tree/master/docs/notebooks).\n\nYou can also take a look at [the mini-libraries in\n`jax.experimental`](https://github.com/google/jax/tree/master/jax/experimental/README.md),\nlike [`stax` for building neural\nnetworks](https://github.com/google/jax/tree/master/jax/experimental/README.md#neural-net-building-with-stax)\nand [`optimizers` for first-order stochastic\noptimization](https://github.com/google/jax/tree/master/jax/experimental/README.md#first-order-optimization),\nor the [examples](https://github.com/google/jax/tree/master/examples).\n\n## Transformations\n\nAt its core, JAX is an extensible system for transforming numerical functions.\nHere are four of primary interest: `grad`, `jit`, `vmap`, and `pmap`.\n\n### Automatic differentiation with `grad`\n\nJAX has roughly the same API as [Autograd](https://github.com/hips/autograd).\nThe most popular function is\n[`grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)\nfor reverse-mode gradients:\n\n```python\nfrom jax import grad\nimport jax.numpy as np\n\ndef tanh(x):  # Define a function\n  y = np.exp(-2.0 * x)\n  return (1.0 - y) / (1.0 + y)\n\ngrad_tanh = grad(tanh)  # Obtain its gradient function\nprint(grad_tanh(1.0))   # Evaluate it at x = 1.0\n# prints 0.4199743\n```\n\nYou can differentiate to any order with `grad`.\n\n```python\nprint(grad(grad(grad(tanh)))(1.0))\n# prints 0.62162673\n```\n\nFor more advanced autodiff, you can use\n[`jax.vjp`](https://jax.readthedocs.io/en/latest/jax.html#jax.vjp) for\nreverse-mode vector-Jacobian products and\n[`jax.jvp`](https://jax.readthedocs.io/en/latest/jax.html#jax.jvp) for\nforward-mode Jacobian-vector products. The two can be composed arbitrarily with\none another, and with other JAX transformations. Here's one way to compose those\nto make a function that efficiently computes [full Hessian\nmatrices](https://jax.readthedocs.io/en/latest/jax.html#jax.hessian):\n\n```python\nfrom jax import jit, jacfwd, jacrev\n\ndef hessian(fun):\n  return jit(jacfwd(jacrev(fun)))\n```\n\nAs with [Autograd](https://github.com/hips/autograd), you're free to use\ndifferentiation with Python control structures:\n\n```python\ndef abs_val(x):\n  if x > 0:\n    return x\n  else:\n    return -x\n\nabs_val_grad = grad(abs_val)\nprint(abs_val_grad(1.0))   # prints 1.0\nprint(abs_val_grad(-1.0))  # prints -1.0 (abs_val is re-evaluated)\n```\n\nSee the [reference docs on automatic\ndifferentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)\nand the [JAX Autodiff\nCookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)\nfor more.\n\n### Compilation with `jit`\n\nYou can use XLA to compile your functions end-to-end with\n[`jit`](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit),\nused either as an `@jit` decorator or as a higher-order function.\n\n```python\nimport jax.numpy as np\nfrom jax import jit\n\ndef slow_f(x):\n  # Element-wise ops see a large benefit from fusion\n  return x * x + x * 2.0\n\nx = np.ones((5000, 5000))\nfast_f = jit(slow_f)\n%timeit -n10 -r3 fast_f(x)  # ~ 4.5 ms / loop on Titan X\n%timeit -n10 -r3 slow_f(x)  # ~ 14.5 ms / loop (also on GPU via JAX)\n```\n\nYou can mix `jit` and `grad` and any other JAX transformation however you like.\n\nUsing `jit` puts constraints on the kind of Python control flow\nthe function can use; see\nthe [Gotchas\nNotebook](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#python-control-flow-+-JIT)\nfor more.\n\n### Auto-vectorization with `vmap`\n\n[`vmap`](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap) is\nthe vectorizing map.\nIt has the familiar semantics of mapping a function along array axes, but\ninstead of keeping the loop on the outside, it pushes the loop down into a\nfunction\u2019s primitive operations for better performance.\n\nUsing `vmap` can save you from having to carry around batch dimensions in your\ncode. For example, consider this simple *unbatched* neural network prediction\nfunction:\n\n```python\ndef predict(params, input_vec):\n  assert input_vec.ndim == 1\n  for W, b in params:\n    output_vec = np.dot(W, input_vec) + b  # `input_vec` on the right-hand side!\n    input_vec = np.tanh(output_vec)\n  return output_vec\n```\n\nWe often instead write `np.dot(inputs, W)` to allow for a batch dimension on the\nleft side of `inputs`, but we\u2019ve written this particular prediction function to\napply only to single input vectors. If we wanted to apply this function to a\nbatch of inputs at once, semantically we could just write\n\n```python\nfrom functools import partial\npredictions = np.stack(list(map(partial(predict, params), input_batch)))\n```\n\nBut pushing one example through the network at a time would be slow! It\u2019s better\nto vectorize the computation, so that at every layer we\u2019re doing matrix-matrix\nmultiplies rather than matrix-vector multiplies.\n\nThe `vmap` function does that transformation for us. That is, if we write\n\n```python\nfrom jax import vmap\npredictions = vmap(partial(predict, params))(input_batch)\n# or, alternatively\npredictions = vmap(predict, in_axes=(None, 0))(params, input_batch)\n```\n\nthen the `vmap` function will push the outer loop inside the function, and our\nmachine will end up executing matrix-matrix multiplications exactly as if we\u2019d\ndone the batching by hand.\n\nIt\u2019s easy enough to manually batch a simple neural network without `vmap`, but\nin other cases manual vectorization can be impractical or impossible. Take the\nproblem of efficiently computing per-example gradients: that is, for a fixed set\nof parameters, we want to compute the gradient of our loss function evaluated\nseparately at each example in a batch. With `vmap`, it\u2019s easy:\n\n```python\nper_example_gradients = vmap(partial(grad(loss), params))(inputs, targets)\n```\n\nOf course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other\nJAX transformation! We use `vmap` with both forward- and reverse-mode automatic\ndifferentiation for fast Jacobian and Hessian matrix calculations in\n`jax.jacfwd`, `jax.jacrev`, and `jax.hessian`.\n\n### SPMD programming with `pmap`\n\nFor parallel programming of multiple accelerators, like multiple GPUs, use\n[`pmap`](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap).\nWith `pmap` you write single-program multiple-data (SPMD) programs, including\nfast parallel collective communication operations. Applying `pmap` will mean\nthat the function you write is compiled by XLA (similarly to `jit`), then\nreplicated and executed in parallel accross devices.\n\nHere's an example on an 8-GPU machine:\n\n```python\nfrom jax import random, pmap\nimport jax.numpy as np\n\n# Create 8 random 5000 x 6000 matrices, one per GPU\nkeys = random.split(random.PRNGKey(0), 8)\nmats = pmap(lambda key: random.normal(key, (5000, 6000)))(keys)\n\n# Run a local matmul on each device in parallel (no data transfer)\nresult = pmap(lambda x: np.dot(x, x.T))(mats)  # result.shape is (8, 5000, 5000)\n\n# Compute the mean on each device in parallel and print the result\nprint(pmap(np.mean)(result))\n# prints [1.1566595 1.1805978 ... 1.2321935 1.2015157]\n```\n\nIn addition to expressing pure maps, you can use fast [collective communication\noperations](https://jax.readthedocs.io/en/latest/jax.lax.html#parallel-operators)\nbetween devices:\n\n```python\nfrom functools import partial\nfrom jax import lax\n\n@partial(pmap, axis_name='i')\ndef normalize(x):\n  return x / lax.psum(x, 'i')\n\nprint(normalize(np.arange(4.)))\n# prints [0.         0.16666667 0.33333334 0.5       ]\n```\n\nYou can even [nest `pmap` functions](https://colab.sandbox.google.com/github/google/jax/blob/master/cloud_tpu_colabs/Pmap_Cookbook.ipynb#scrollTo=MdRscR5MONuN) for more\nsophisticated communication patterns.\n\nIt all composes, so you're free to differentiate through parallel computations:\n\n```python\nfrom jax import grad\n\n@pmap\ndef f(x):\n  y = np.sin(x)\n  @pmap\n  def g(z):\n    return np.cos(z) * np.tan(y.sum()) * np.tanh(x).sum()\n  return grad(lambda w: np.sum(g(w)))(x)\n\nprint(f(x))\n# [[ 0.        , -0.7170853 ],\n#  [-3.1085174 , -0.4824318 ],\n#  [10.366636  , 13.135289  ],\n#  [ 0.22163185, -0.52112055]]\n\nprint(grad(lambda x: np.sum(f(x)))(x))\n# [[ -3.2369726,  -1.6356447],\n#  [  4.7572474,  11.606951 ],\n#  [-98.524414 ,  42.76499  ],\n#  [ -1.6007166,  -1.2568436]]\n```\n\nWhen reverse-mode differentiating a `pmap` function (e.g. with `grad`), the\nbackward pass of the computation is parallelized just like the forward pass.\n\nSee the [SPMD\nCookbook](https://colab.sandbox.google.com/github/google/jax/blob/master/cloud_tpu_colabs/Pmap_Cookbook.ipynb)\nand the [SPMD MNIST classifier from scratch\nexample](https://github.com/google/jax/blob/master/examples/spmd_mnist_classifier_fromscratch.py)\nfor more.\n\n## Current gotchas\n\nFor a more thorough survey of current gotchas, with examples and explanations,\nwe highly recommend reading the [Gotchas\nNotebook](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html).\nSome standouts:\n\n1. JAX transformations only work on [pure functions](https://en.wikipedia.org/wiki/Pure_function), which don't have side-effects and respect [referential transparency](https://en.wikipedia.org/wiki/Referential_transparency) (i.e. object identity testing with `is` isn't preserved). If you use a JAX transformation on an impure Python function, you might see an error like `Exception: Can't lift Traced...`  or `Exception: Different traces at same level`.\n1. [In-place mutating updates of\n   arrays](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-In-Place-Updates), like `x[i] += y`, aren't supported, but [there are functional alternatives](https://jax.readthedocs.io/en/latest/jax.ops.html). Under a `jit`, those functional alternatives will reuse buffers in-place automatically.\n1. [Random numbers are\n   different](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Random-Numbers), but for [good reasons](https://github.com/google/jax/blob/master/design_notes/prng.md).\n1. If you're looking for [convolution\n   operators](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Convolutions),\n   they're in the `jax.lax` package.\n1. JAX enforces single-precision (32-bit, e.g. `float32`) values by default, and\n   [to enable\n   double-precision](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#Double-(64bit)-precision)\n   (64-bit, e.g. `float64`) one needs to set the `jax_enable_x64` variable at\n   startup (or set the environment variable `JAX_ENABLE_X64=True`).\n1. Some of NumPy's dtype promotion semantics involving a mix of Python scalars\n   and NumPy types aren't preserved, namely `np.add(1, np.array([2],\n   np.float32)).dtype` is `float64` rather than `float32`.\n1. Some transformations, like `jit`, [constrain how you can use Python control\n   flow](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Control-Flow).\n   You'll always get loud errors if something goes wrong. You might have to use\n   [`jit`'s `static_argnums`\n   parameter](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit),\n   [structured control flow\n   primitives](https://jax.readthedocs.io/en/latest/jax.lax.html#control-flow-operators)\n   like\n   [`lax.scan`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html#jax.lax.scan),\n   or just use `jit` on smaller subfunctions.\n\n## Installation\n\nJAX is written in pure Python, but it depends on XLA, which needs to be\ninstalled as the `jaxlib` package. Use the following instructions to install a\nbinary package with `pip`, or to build JAX from source.\n\nWe support installing or building `jaxlib` on Linux (Ubuntu 16.04 or later) and\nmacOS (10.12 or later) platforms, but not yet Windows. We're not currently\nworking on Windows support, but contributions are welcome\n(see [#438](https://github.com/google/jax/issues/438)). Some users have reported\nsuccess with building a CPU-only `jaxlib` from source using the Windows Subsytem\nfor Linux.\n\n### pip installation\n\nTo install a CPU-only version, which might be useful for doing local\ndevelopment on a laptop, you can run\n\n```bash\npip install --upgrade pip\npip install --upgrade jax jaxlib  # CPU-only version\n```\n\nOn Linux, it is often necessary to first update `pip` to a version that supports\n`manylinux2010` wheels.\n\nIf you want to install JAX with both CPU and GPU support, using existing CUDA\nand CUDNN7 installations on your machine (for example, preinstalled on your\ncloud VM), you can run\n\n```bash\n# install jaxlib\nPYTHON_VERSION=cp37  # alternatives: cp36, cp37, cp38\nCUDA_VERSION=cuda92  # alternatives: cuda92, cuda100, cuda101, cuda102\nPLATFORM=linux_x86_64  # alternatives: linux_x86_64\nBASE_URL='https://storage.googleapis.com/jax-releases'\npip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.44-$PYTHON_VERSION-none-$PLATFORM.whl\n\npip install --upgrade jax  # install jax\n```\n\nThe library package name must correspond to the version of the existing CUDA\ninstallation you want to use, with `cuda102` for CUDA 10.2, `cuda101` for CUDA\n10.1, `cuda100` for CUDA 10.0, and `cuda92` for CUDA 9.2. To find your CUDA and\nCUDNN versions, you can run commands like these, depending on your CUDNN install\npath:\n\n```bash\nnvcc --version\ngrep CUDNN_MAJOR -A 2 /usr/local/cuda/include/cudnn.h  # might need different path\n```\n\nThe Python version must match your Python interpreter. There are prebuilt wheels\nfor Python 3.6, 3.7, and 3.8; for anything else, you must build from\nsource. Jax requires Python 3.6 or above. Jax does not support Python 2 any\nmore.\n\nTo try automatic detection of the correct version for your system, you can run: \n\n```bash\npip install --upgrade https://storage.googleapis.com/jax-releases/`nvcc -V | sed -En \"s/.* release ([0-9]*)\\.([0-9]*),.*/cuda\\1\\2/p\"`/jaxlib-0.1.44-`python3 -V | sed -En \"s/Python ([0-9]*)\\.([0-9]*).*/cp\\1\\2/p\"`-none-linux_x86_64.whl jax\n```\n\nPlease let us know on [the issue tracker](https://github.com/google/jax/issues)\nif you run into any errors or problems with the prebuilt wheels.\n\n### Building JAX from source\nSee [Building JAX from\nsource](https://jax.readthedocs.io/en/latest/developer.html#building-from-source).\n\n\n## Citing JAX\n\nTo cite this repository:\n\n```\n@software{jax2018github,\n  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and Skye Wanderman-Milne},\n  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},\n  url = {http://github.com/google/jax},\n  version = {0.1.55},\n  year = {2018},\n}\n```\n\nIn the above bibtex entry, names are in alphabetical order, the version number\nis intended to be that from [jax/version.py](../master/jax/version.py), and\nthe year corresponds to the project's open-source release.\n\nA nascent version of JAX, supporting only automatic differentiation and\ncompilation to XLA, was described in a [paper that appeared at SysML\n2018](https://mlsys.org/Conferences/2019/doc/2018/146.pdf). We're currently working on\ncovering JAX's ideas and capabilities in a more comprehensive and up-to-date\npaper.\n\n## Reference documentation\n\nFor details about the JAX API, see the\n[reference documentation](https://jax.readthedocs.io/).\n\nFor getting started as a JAX developer, see the\n[developer documentation](https://jax.readthedocs.io/en/latest/developer.html).\n"}, "jupyter": {"file_name": "jupyter/notebook/README.md", "raw_text": "# Jupyter Notebook\n\n[![Google Group](https://img.shields.io/badge/-Google%20Group-lightgrey.svg)](https://groups.google.com/forum/#!forum/jupyter)\n[![Build Status](https://travis-ci.org/jupyter/notebook.svg?branch=master)](https://travis-ci.org/jupyter/notebook)\n[![Documentation Status](https://readthedocs.org/projects/jupyter-notebook/badge/?version=latest)](https://jupyter-notebook.readthedocs.io/en/latest/?badge=latest)\n                \n\n\nThe Jupyter notebook is a web-based notebook environment for interactive\ncomputing.\n\n![Jupyter notebook example](docs/resources/running_code_med.png \"Jupyter notebook example\")\n\n### Jupyter notebook, the language-agnostic evolution of IPython notebook\nJupyter notebook is a language-agnostic HTML notebook application for\nProject Jupyter. In 2015, Jupyter notebook was released as a part of\nThe Big Split\u2122 of the IPython codebase. IPython 3 was the last major monolithic\nrelease containing both language-agnostic code, such as the *IPython notebook*,\nand language specific code, such as the *IPython kernel for Python*. As\ncomputing spans across many languages, Project Jupyter will continue to develop the\nlanguage-agnostic **Jupyter notebook** in this repo and with the help of the\ncommunity develop language specific kernels which are found in their own\ndiscrete repos.\n[[The Big Split\u2122 announcement](https://blog.jupyter.org/the-big-split-9d7b88a031a7)]\n[[Jupyter Ascending blog post](https://blog.jupyter.org/jupyter-ascending-1bf5b362d97e)]\n\n## Installation\nYou can find the installation documentation for the\n[Jupyter platform, on ReadTheDocs](https://jupyter.readthedocs.io/en/latest/install.html).\nThe documentation for advanced usage of Jupyter notebook can be found\n[here](https://jupyter-notebook.readthedocs.io/en/latest/).\n\nFor a local installation, make sure you have\n[pip installed](https://pip.readthedocs.io/en/stable/installing/) and run:\n\n    $ pip install notebook\n\n## Usage - Running Jupyter notebook\n\n### Running in a local installation\n\nLaunch with:\n\n    $ jupyter notebook\n\n### Running in a remote installation\n\nYou need some configuration before starting Jupyter notebook remotely. See [Running a notebook server](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html).\n\n## Development Installation\n\nSee [`CONTRIBUTING.rst`](CONTRIBUTING.rst) for how to set up a local development installation.\n\n## Contributing\n\nIf you are interested in contributing to the project, see [`CONTRIBUTING.rst`](CONTRIBUTING.rst).\n\n## Resources\n- [Project Jupyter website](https://jupyter.org)\n- [Online Demo at jupyter.org/try](https://jupyter.org/try)\n- [Documentation for Jupyter notebook](https://jupyter-notebook.readthedocs.io/en/latest/) [[PDF](https://media.readthedocs.org/pdf/jupyter-notebook/latest/jupyter-notebook.pdf)]\n- [Korean Version of Installation](https://github.com/ChungJooHo/Jupyter_Kor_doc/)\n- [Documentation for Project Jupyter](https://jupyter.readthedocs.io/en/latest/index.html) [[PDF](https://media.readthedocs.org/pdf/jupyter/latest/jupyter.pdf)]\n- [Issues](https://github.com/jupyter/notebook/issues)\n- [Technical support - Jupyter Google Group](https://groups.google.com/forum/#!forum/jupyter)\n"}, "jupyter-datatables": {"file_name": "CermakM/jupyter-datatables/README.md", "raw_text": "# Jupyter DataTables\n\n<a href=\"https://mybinder.org/v2/gh/CermakM/jupyter-datatables/master?filepath=examples%2Fjupyter-datatables-0.4.0.ipynb\"\n   target=\"_parent\">\n   <img align=\"left\"\n      src=\"https://mybinder.org/badge_logo.svg\">\n</a>\n<a href=\"https://nbviewer.jupyter.org/github/CermakM/jupyter-datatables/blob/master/examples/\"\n   target=\"_parent\">\n   <img\n      src=\"https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.png\"\n      width=\"109\" height=\"20\">\n</a>\n\nJupyter Notebook extension to leverage pandas DataFrames by integrating DataTables JS.\n\n\n\n<br>\n\n#### About\n\nData scientists and in fact many developers work with `pd.DataFrame` on daily basis to interpret data to process them. In my typical workflow. The common workflow is to display the dataframe, take a look at the data schema and then produce multiple plots to check the distribution of the data to have a clearer picture, perhaps search some data in the table, etc...\n\nWhat if those distribution plots were part of the standard DataFrame and we had the ability to quickly search through the table with minimal effort? What if it was the default representation?\n\nThe jupyter-datatables uses [jupyter-require](https://github.com/CermakM/jupyter-require) to draw the table.\n\n<br>\n\n#### Installation\n\n```bash\npip install jupyter-datatables\n```\n\n<br>\n\n#### Usage\n\n```python\nimport numpy as np\nimport pandas as pd\n\nfrom jupyter_datatables import init_datatables_mode\n\ninit_datatables_mode()\n```\n\nThat's it, your default pandas representation will now use Jupyter DataTables!\n\n```python\ndf = pd.DataFrame(np.abs(np.random.randn(50, 5)), columns=list(string.ascii_uppercase[:5]))\n```\n\n![Jupyter Datatables table representation](https://raw.github.com/CermakM/jupyter-datatables/master/assets/images/jupyter-datatables.png)\n\n<br>\n\nIn most cases, you don't need to worry too much about the size of your data. Jupyter DataTables **calculates required sample size** based on a confidence interval (by default this would be `0.95`) and margin of error and ceils it to the highest 'smart' value.\n\nFor example, for a data containing `100,000` samples, given `0.975` confidence interval and `0.02` margin of error, the Jupyter DataTables would calculate that `3044` samples are required and it would round it up to `4000`.\n\n![Jupyter Datatables long table sample size](https://raw.github.com/CermakM/jupyter-datatables/master/assets/images/jupyter-datatables-long.png)\n\nWith additional note:\n\n> Sample size: 4,000 out of 100,000\n\n<br>\n\nWe can also handle wide tables with ease.\n\n```python\ndf = pd.DataFrame(np.abs(np.random.randn(50, 20)), columns=list(string.ascii_uppercase[:20]))\n```\n\n![Jupyter Datatables wide table representation](https://raw.github.com/CermakM/jupyter-datatables/master/assets/images/jupyter-datatables-wide.gif)\n\n<br>\n\nAs per 0.3.0, there is a support for **interactive tooltips**:\n\n![Jupyter Datatables wide table representation](https://raw.github.com/CermakM/jupyter-datatables/master/assets/images/jupyter-datatables-tooltips.gif)\n\n\nAnd also support for custom indices including `Date` type:\n\n```python\ndft = pd.DataFrame({'A': np.random.rand(5),\n                    'B': [1, 1, 3, 2, 1],\n                    'C': 'This is a very long sentence that should automatically be trimmed',\n                    'D': [pd.Timestamp('20010101'), pd.Timestamp('20010102'), pd.Timestamp('20010103'), pd.Timestamp('20010104'), pd.Timestamp('20010105')],\n                    'E': pd.Series([1.0] * 5).astype('float32'),\n                    'F': [False, True, False, False, True],\n                   })\n\ndft.D = dft.D.apply(pd.to_datetime)\ndft.set_index('D', inplace=True)\n```\n\n![Jupyter Datatables wide table representation](https://raw.github.com/CermakM/jupyter-datatables/master/assets/images/jupyter-datatables-datetime-tooltips.gif)\n\n---\n\n<br>\n\n#### Current status and future plans:\n\nCheck out the [Project Board](https://github.com/users/CermakM/projects/1) where we track issues and TODOs for our Jupyter tooling!\n\n---\n\n> Author: Marek Cermak <macermak@redhat.com>, @AICoE\n"}, "jupyter-nbrequirements": {"file_name": "CermakM/jupyter-nbrequirements/README.md", "raw_text": "# jupyter-nbrequirements &nbsp;[![License](https://img.shields.io/github/license/mashape/apistatus.svg?label=License)](https://github.com/CermakM/jupyter-nbrequirements/blob/master/LICENSE)\n\n[![Dependabot Status](https://api.dependabot.com/badges/status?host=github&repo=CermakM/jupyter-nbrequirements)](https://dependabot.com)\n[![Node CI](https://github.com/cermakm/jupyter-nbrequirements/workflows/Node%20CI/badge.svg)](https://github.com/cermakm/jupyter-nbrequirements/actions) &nbsp;\n[![Release](https://img.shields.io/github/v/tag/cermakm/jupyter-nbrequirements.svg?sort=semver&label=Release)](https://github.com/CermakM/jupyter-nbrequirements/releases/latest)\n\n\n\nDependency management and optimization in Jupyter Notebooks.\n\n<br>\n\n## About\n\nThis extension provides control over the notebook dependencies.\n\nThe main goals of the project are the following:\n\n  - manage notebook requirements without leaving the notebook\n  - provide a unique and optimized* environment for each notebook\n\n*The requirements are optimized using the [Thoth] resolution engine\n\n<br>\n\n## Installation\n\n```bash\npip install jupyter-nbrequirements\n```\n\nAnd enable the required extensions (might not be needed with the latest version, but to be sure..)\n\n```bash\njupyter nbextension install --user --py jupyter_nbrequirements\n```\n\n<br>\n\n## Usage\n\n#### NBRequirements UI\n\nSince [v0.4.0](https://github.com/CermakM/jupyter-nbrequirements/releases/tag/v0.4.0), we've introduced a new UI! Check it out, interact with it and see what it can offer you!\n\n<div style=\"text-align:center\">\n<img alt=\"NBRequirements UI\" src=\"https://raw.githubusercontent.com/CermakM/jupyter-nbrequirements/master/assets/ui.png\">\n</div>\n\nOur development efforts will from now on focus primarily on improving the UI.\n\n#### The old-school approach\n\nThe Jupyter magic is in sync with the UI, so don't worry old schoolers, you can still run the commands manually and the existing notebooks will work!\n\n#### Create the environment for the notebook to run in\n\nSay we want to do an EDA, we will probably need [pandas](https://pandas.pydata.org), a visualization library like [plotly](https://plot.ly) and some additional libraries to make our lives easier, like [sklearn](https://scikit-learn.org/stable/) and [pandas-profiling](https://github.com/pandas-profiling/pandas-profiling).\n\nIn a Jupyter notebook cell:\n\n```\n%dep add pandas --version \">=0.24.0\"\n%dep add plotly\n%dep add sklearn\n%dep add pandas-profiling\n```\n\nAnd perhaps our code would need some refactoring and linter checks later on, so let's add a `dev` dependency.\n\n```\n%dep add --dev black\n```\n\nYou can now check the requirements that your notebook has by issuing `%requirements` (or `%dep`, which is just an alias for it) command:\n\n```\n%requirements\n```\n```\n[packages]\npandas = \">=0.24.0\"\nplotly = \"*\"\nsklearn = \"*\"\npandas-profiling = \"*\"\n\n[dev-packages]\nblack = \"*\"\n\n[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[requires]\npython_version = \"3.6\"\n```\n\nUp to this point, we've been working only with the metadata. In order to create the environment and actually install the dependencies, you run the `%dep ensure` command (insipired by the golang's [dep](https://github.com/golang/dep), for those familiar with Golang).\n\n```\n%dep ensure\n```\n\n> Since this project is still under development and it uses the [Thoth] resolution engine to optimize the notebook dependencies (which is also still under development as well), in case something goes wrong, `ensure` accepts the `engine` parameter, which can be set to `pipenv`\n\n```\n%dep ensure --engine pipenv\n```\n\nCheck out the [examples](/examples/) for more info.\n\n<br>\n\n## Future plans:\n\nSee the [Project Board](https://github.com/CermakM/jupyter-nbrequirements/projects).\n\n<br>\n\n---\n\n> Author: Marek Cermak <macermak@redhat.com>, @AICoE - Project Thoth\n\n\n<!-- References -->\n\n[Thoth]: https://github.com/thoth-station\n"}, "jupyterlab": {"file_name": "jupyterlab/jupyterlab/README.md", "raw_text": "**[Installation](#installation)** |\n**[Documentation](http://jupyterlab.readthedocs.io)** |\n**[Contributing](#contributing)** |\n**[License](#license)** |\n**[Team](#team)** |\n**[Getting help](#getting-help)** |\n\n# [JupyterLab](http://jupyterlab.github.io/jupyterlab/)\n\n[![PyPI version](https://badge.fury.io/py/jupyterlab.svg)](https://badge.fury.io/py/jupyterlab)\n[![Downloads](https://pepy.tech/badge/jupyterlab/month)](https://pepy.tech/project/jupyterlab/month)\n[![Build Status](https://dev.azure.com/jupyterlab/jupyterlab/_apis/build/status/jupyterlab.jupyterlab?branchName=master)](https://dev.azure.com/jupyterlab/jupyterlab/_build/latest?definitionId=1&branchName=master)\n[![Documentation Status](https://readthedocs.org/projects/jupyterlab/badge/?version=stable)](http://jupyterlab.readthedocs.io/en/stable/)\n[![GitHub](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/jupyterlab/jupyterlab/issues)\n[![Discourse](https://img.shields.io/badge/help_forum-discourse-blue.svg)](https://discourse.jupyter.org/c/jupyterlab)\n[![Gitter](https://img.shields.io/badge/social_chat-gitter-blue.svg)](https://gitter.im/jupyterlab/jupyterlab)\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jupyterlab/jupyterlab-demo/master?urlpath=lab/tree/demo)\n\nAn extensible environment for interactive and reproducible computing, based on the\nJupyter Notebook and Architecture. [Currently ready for users.](https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906)\n\n[JupyterLab](http://jupyterlab.readthedocs.io/en/stable/) is the next-generation user interface for [Project Jupyter](https://jupyter.org) offering\nall the familiar building blocks of the classic Jupyter Notebook (notebook,\nterminal, text editor, file browser, rich outputs, etc.) in a flexible and\npowerful user interface.\nJupyterLab will eventually replace the classic Jupyter Notebook.\n\nJupyterLab can be extended using [npm](https://www.npmjs.com/) packages\nthat use our public APIs. To find JupyterLab extensions, search for the npm keyword [jupyterlab-extension](https://www.npmjs.com/search?q=keywords:jupyterlab-extension) or the GitHub topic [jupyterlab-extension](https://github.com/topics/jupyterlab-extension). To learn more about extensions, see the [user documentation](https://jupyterlab.readthedocs.io/en/latest/user/extensions.html).\n\nThe current JupyterLab releases are suitable for general\nusage, and the extension APIs will continue to\nevolve for JupyterLab extension developers.\n\nRead the latest version's documentation on [ReadTheDocs](http://jupyterlab.readthedocs.io/en/latest/).\n\n---\n\n## Getting started\n\n### Installation\n\nJupyterLab can be installed using `conda` or `pip`. For more detailed instructions, consult the [installation guide](http://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html).\n\nProject installation instructions from the git sources are available in the [contributor documentation](CONTRIBUTING.md).\n\n### conda\n\nIf you use `conda`, you can install it with:\n\n```shell\nconda install -c conda-forge jupyterlab\n```\n\n### pip\n\nIf you use `pip`, you can install it with:\n\n```shell\npip install jupyterlab\n```\n\nIf installing using `pip install --user`, you must add the user-level `bin` directory to your `PATH` environment variable in order to launch `jupyter lab`.\n\n#### Installing with Previous Versions of Jupyter Notebook\n\nWhen using a version of Jupyter Notebook earlier than 5.3, the following command must be run\nafter installation to enable the JupyterLab server extension:\n\n```bash\njupyter serverextension enable --py jupyterlab --sys-prefix\n```\n\n### Running\n\nStart up JupyterLab using:\n\n```bash\njupyter lab\n```\n\nJupyterLab will open automatically in the browser. See the [documentation](http://jupyterlab.readthedocs.io/en/stable/getting_started/starting.html) for additional details.\n\n### Prerequisites and Supported Browsers\n\nJupyter notebook version 4.3 or later is required. To check the notebook version, run the command:\n\n```bash\njupyter notebook --version\n```\n\nThe latest versions of the following browsers are currently _known to work_:\n\n- Firefox\n- Chrome\n- Safari\n\nSee our [documentation](http://jupyterlab.readthedocs.io/en/latest/getting_started/installation.html) for additional details.\n\n---\n\n## Development\n\n### Contributing\n\nTo contribute to the project, please read the [contributor documentation](CONTRIBUTING.md).\n\nJupyterLab follows the Jupyter [Community Guides](https://jupyter.readthedocs.io/en/latest/community/content-community.html).\n\n### Extending JupyterLab\n\nTo start developing an extension, see the [developer documentation](https://jupyterlab.readthedocs.io/en/latest/developer/extension_dev.html) and the [API docs](http://jupyterlab.github.io/jupyterlab/index.html).\n\n### License\n\nJupyterLab uses a shared copyright model that enables all contributors to maintain the\ncopyright on their contributions. All code is licensed under the terms of the revised [BSD license](https://github.com/jupyterlab/jupyterlab/blob/master/LICENSE).\n\n### Team\n\nJupyterLab is part of [Project Jupyter](http://jupyter.org/) and is developed by an open community. The maintenance team is assisted by a much larger group of contributors to JupyterLab and Project Jupyter as a whole.\n\nJupyterLab's current maintainers are listed in alphabetical order, with affiliation, and main areas of contribution:\n\n- Afshin Darian, Two Sigma (co-creator, application/high-level architecture,\n  prolific contributions throughout the code base).\n- Vidar T. Fauske, JPMorgan Chase (general development, extensions).\n- Tim George, Cal Poly (UI/UX design, strategy, management, user needs analysis)\n- Brian Granger, AWS (co-creator, strategy, vision, management, UI/UX design,\n  architecture).\n- Jason Grout, Bloomberg (co-creator, vision, general development).\n- Max Klein, JPMorgan Chase (UI Package, build system, general development, extensions).\n- Fernando Perez, UC Berkeley (co-creator, vision).\n- Ian Rose, Quansight/City of LA (general core development, extensions).\n- Saul Shanabrook, Quansight (general development, extensions)\n- Steven Silvester, AWS (co-creator, release management, packaging,\n  prolific contributions throughout the code base).\n\nMaintainer emeritus:\n\n- Chris Colbert, Project Jupyter (co-creator, application/low-level architecture,\n  technical leadership, vision, PhosphorJS)\n- Cameron Oelsen, Cal Poly (UI/UX design).\n- Jessica Forde, Project Jupyter (demo, documentation)\n\nThis list is provided to give the reader context on who we are and how our team functions.\nTo be listed, please submit a pull request with your information.\n\n---\n\n## Getting help\n\nWe encourage you to ask questions on the [Discourse forum](https://discourse.jupyter.org/c/jupyterlab). A question answered there can become a useful resource for others.\n\nPlease use the [GitHub issues page](https://github.com/jupyterlab/jupyterlab/issues) to provide feedback or submit a bug report. To keep resolved issues self-contained, the [lock bot](https://github.com/apps/lock) will lock closed issues as resolved after a period of inactivity. If related discussion is still needed after an issue is locked, please open a new issue and reference the old issue.\n\n### Weekly Dev Meeting\n\nWe have videoconference meetings every week where we discuss what we have been working on and get feedback from one another.\n\nAnyone is welcome to attend, if they would like to discuss a topic or just to listen in.\n\n- When: Wednesdays [9AM Pacific Time](https://www.thetimezoneconverter.com/?t=9%3A00%20am&tz=San%20Francisco&)\n- Where: [`jovyan` Zoom](https://zoom.us/my/jovyan)\n- What: [Meeting notes](https://hackmd.io/Uscrk0N1RhCtX-p6ZHUuWQ?both)\n"}, "jupyterlab-templates": {"file_name": "timkpaine/jupyterlab_templates/README.md", "raw_text": "# jupyterlab_templates\nSupport for jupyter notebook templates in jupyterlab\n\n[![Build Status](https://dev.azure.com/tpaine154/jupyter/_apis/build/status/timkpaine.jupyterlab_templates?branchName=master)](https://dev.azure.com/tpaine154/jupyter/_build/latest?definitionId=11&branchName=master)\n[![GitHub issues](https://img.shields.io/github/issues/timkpaine/jupyterlab_templates.svg)]()\n[![Coverage](https://img.shields.io/azure-devops/coverage/tpaine154/jupyter/11)](https://dev.azure.com/tpaine154/jupyter/_build?definitionId=11&_a=summary)\n[![PyPI](https://img.shields.io/pypi/l/jupyterlab_templates.svg)](https://pypi.python.org/pypi/jupyterlab_templates)\n[![PyPI](https://img.shields.io/pypi/v/jupyterlab_templates.svg)](https://pypi.python.org/pypi/jupyterlab_templates)\n[![npm](https://img.shields.io/npm/v/jupyterlab_templates.svg)](https://www.npmjs.com/package/jupyterlab_templates)\n\n![](https://raw.githubusercontent.com/timkpaine/jupyterlab_templates/master/docs/example1.gif)\n\n\n## Install\n```bash\npip install jupyterlab_templates\njupyter labextension install jupyterlab_templates\njupyter serverextension enable --py jupyterlab_templates\n```\n\n## Adding templates\ninstall the server extension, and add the following to `jupyter_notebook_config.py`\n\n```python3\nc.JupyterLabTemplates.template_dirs = ['list', 'of', 'template', 'directories']\nc.JupyterLabTemplates.include_default = True\nc.JupyterLabTemplates.include_core_paths = True\n```\n\n## Templates for libraries\nThe extension will search *subdirectories* of each parent directory specified in `template_dirs` for templates.\n\nThe `notebook_templates` directory under the jupyter data folder is one of the default parent directory. Thus, if you have tutorials or guides you'd like to install for users, simply copy them into your jupyter data folder inside the `notebook_templates` directory, e.g. `/usr/local/share/jupyter/notebook_templates/bqplot` for `bqplot`.\n\n\n### Flags\n- `template_dirs`: a list of absolute directory paths. All `.ipynb` files in any *subdirectories* of these paths will be listed as templates\n- `include_default`: include the default Sample template (default True)\n- `include_core_paths`: include jupyter core paths (see: jupyter --paths) (default True)\n"}, "karateclub": {"file_name": "benedekrozemberczki/karateclub/README.md", "raw_text": "\n ![Version](https://badge.fury.io/py/karateclub.svg?style=plastic) ![License](https://img.shields.io/github/license/benedekrozemberczki/karateclub.svg?color=blue&style=plastic) [![PyPI download month](https://img.shields.io/pypi/dm/karateclub.svg?color=blue&style=plastic)](https://pypi.python.org/pypi/karateclub/)  [![Arxiv](https://img.shields.io/badge/ArXiv-2003.04819-orange.svg?color=blue&style=plastic)](https://arxiv.org/abs/2003.04819)\n\n<p align=\"center\">\n  <img width=\"90%\" src=\"https://github.com/benedekrozemberczki/karateclub/blob/master/karatelogo.jpg?sanitize=true\" />\n</p>\n\n--------------------------------------------------------------------------------\n\n\n**Karate Club** is an unsupervised machine learning extension library for [NetworkX](https://networkx.github.io/).\n\n\n\nPlease look at the **[Documentation](https://karateclub.readthedocs.io/)**, relevant **[Paper](https://arxiv.org/abs/2003.04819)**, and **[External Resources](https://karateclub.readthedocs.io/en/latest/notes/resources.html)**.\n\n*Karate Club* consists of state-of-the-art methods to do unsupervised learning on graph structured data. To put it simply it is a Swiss Army knife for small-scale graph mining research. First, it provides network embedding techniques at the node and graph level. Second, it includes a variety of overlapping and non-overlapping community detection methods. Implemented methods cover a wide range of network science ([NetSci](https://netscisociety.net/home), [Complenet](https://complenet.weebly.com/)), data mining ([ICDM](http://icdm2019.bigke.org/), [CIKM](http://www.cikm2019.net/), [KDD](https://www.kdd.org/kdd2020/)), artificial intelligence ([AAAI](http://www.aaai.org/Conferences/conferences.php), [IJCAI](https://www.ijcai.org/)) and machine learning ([NeurIPS](https://nips.cc/), [ICML](https://icml.cc/), [ICLR](https://iclr.cc/)) conferences, workshops, and pieces from prominent journals.  \n\n--------------------------------------------------------------------------------\n\n**Citing**\n\nIf you find *Karate Club* useful in your research, please consider citing the following paper:\n\n```bibtex\n>@misc{karateclub2020,\n       title={An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs},\n       author={Benedek Rozemberczki and Oliver Kiss and Rik Sarkar},\n       year={2020},\n       eprint={2003.04819},\n       archivePrefix={arXiv},\n       primaryClass={cs.LG}\n}\n```\n--------------------------------------------------------------------------------\n\n**A simple example**\n\n*Karate Club* makes the use of modern community detection techniques quite easy (see [here](https://karateclub.readthedocs.io/en/latest/notes/introduction.html) for the accompanying tutorial).\nFor example, this is all it takes to use on a Watts-Strogatz graph [Ego-splitting](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf):\n\n```python\nimport networkx as nx\nfrom karateclub import EgoNetSplitter\n\ng = nx.newman_watts_strogatz_graph(1000, 20, 0.05)\n\nsplitter = EgoNetSplitter(1.0)\n\nsplitter.fit(g)\n\nprint(splitter.get_memberships())\n```\n\n--------------------------------------------------------------------------------\n\n**Models included**\n\nIn detail, the following community detection and embedding methods were implemented.\n\n**Overlapping Community Detection**\n\n* **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)\n\n* **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)\n\n* **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)\n\n* **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)\n\n* **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)\n\n* **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)\n\n**Non-Overlapping Community Detection**\n\n* **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)\n\n* **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)\n\n* **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)\n\n* **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)\n\n**Neighbourhood-Based Node Level Embedding**\n\n* **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)\n\n* **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)\n\n* **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)\n\n* **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qui *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)\n\n* **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip! Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)\n\n* **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)\n\n* **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?id=2806512) (CIKM 2015)\n\n* **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)\n\n* **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F\u00e9votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)\n\n* **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)\n\n**Structural Node Level Embedding**\n\n* **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)\n\n* **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)\n\n**Attributed Node Level Embedding**\n\n* **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)\n\n* **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)\n\n* **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)\n\n* **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)\n\n* **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)\n\n* **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)\n\n**Meta Node Embedding**\n\n* **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)\n\n**Graph Level Embedding**\n\n* **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML\n\n* **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)\n\n* **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)\n\n* **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018) \n\n* **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)\n\n* **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)\n\n\nHead over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.\nFor a quick start, check out our [examples](https://github.com/benedekrozemberczki/karateclub/tree/master/examples.py).\n\nIf you notice anything unexpected, please open an [issue](https://github.com/benedekrozemberczki/karateclub/issues) and let us know.\nIf you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/karateclub/issues).\nWe are motivated to constantly make Karate Club even better.\n\n\n--------------------------------------------------------------------------------\n\n**Installation**\n\nKarate Club can be installed with the following pip command.\n\n```sh\n$ pip install karateclub\n```\n\nAs we create new releases frequently, upgrading the package casually might be beneficial.\n\n```sh\n$ pip install karateclub --upgrade\n```\n\n--------------------------------------------------------------------------------\n\n**Running examples**\n\nAs part of the documentation we provide a number of use cases to show how the clusterings and embeddings can be utilized for downstream learning. These can accessed [here](https://karateclub.readthedocs.io/en/latest/notes/introduction.html) with detailed explanations.\n\n\nBesides the case studies we provide synthetic examples for each model. These can be tried out by running the examples script.\n\n```sh\n$ python examples.py\n```\n"}, "Keras": {"file_name": "keras-team/keras/README.md", "raw_text": "\ufeff# Keras: Deep Learning for humans\n\n![Keras logo](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)\n\n[![Build Status](https://travis-ci.org/keras-team/keras.svg?branch=master)](https://travis-ci.org/keras-team/keras)\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/keras-team/keras/blob/master/LICENSE)\n\n## You have just found Keras.\n\nKeras is a high-level neural networks API, written in Python and capable of running on top of [TensorFlow](https://github.com/tensorflow/tensorflow), [CNTK](https://github.com/Microsoft/cntk), or [Theano](https://github.com/Theano/Theano). It was developed with a focus on enabling fast experimentation. *Being able to go from idea to result with the least possible delay is key to doing good research.*\n\nUse Keras if you need a deep learning library that:\n\n- Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n- Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n- Runs seamlessly on CPU and GPU.\n\nRead the documentation at [Keras.io](https://keras.io).\n\nKeras is compatible with: __Python 2.7-3.6__.\n\n\n------------------\n\n## Multi-backend Keras and tf.keras:\n\n**At this time, we recommend that Keras users who use multi-backend Keras with the TensorFlow backend switch to `tf.keras` in TensorFlow 2.0**. `tf.keras` is better maintained and has better integration with TensorFlow features (eager execution, distribution support and other).\n\nKeras 2.2.5 was the last release of Keras implementing the 2.2.* API. It was the last release to only support TensorFlow 1 (as well as Theano and CNTK).\n\nThe current release is Keras 2.3.0, which makes significant API changes and add support for TensorFlow 2.0. The 2.3.0 release will be the last major release of multi-backend Keras. Multi-backend Keras is superseded by `tf.keras`.\n\nBugs present in multi-backend Keras will only be fixed until April 2020 (as part of minor releases).\n\nFor more information about the future of Keras, see [the Keras meeting notes](http://bit.ly/keras-meeting-notes).\n\n\n------------------\n\n## Guiding principles\n\n- __User friendliness.__ Keras is an API designed for human beings, not machines. It puts user experience front and center. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.\n\n- __Modularity.__ A model is understood as a sequence or a graph of standalone, fully configurable modules that can be plugged together with as few restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions and regularization schemes are all standalone modules that you can combine to create new models.\n\n- __Easy extensibility.__ New modules are simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research.\n\n- __Work with Python__. No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility.\n\n\n------------------\n\n\n## Getting started: 30 seconds to Keras\n\nThe core data structure of Keras is a __model__, a way to organize layers. The simplest type of model is the [`Sequential`](https://keras.io/getting-started/sequential-model-guide) model, a linear stack of layers. For more complex architectures, you should use the [Keras functional API](https://keras.io/getting-started/functional-api-guide), which allows to build arbitrary graphs of layers.\n\nHere is the `Sequential` model:\n\n```python\nfrom keras.models import Sequential\n\nmodel = Sequential()\n```\n\nStacking layers is as easy as `.add()`:\n\n```python\nfrom keras.layers import Dense\n\nmodel.add(Dense(units=64, activation='relu', input_dim=100))\nmodel.add(Dense(units=10, activation='softmax'))\n```\n\nOnce your model looks good, configure its learning process with `.compile()`:\n\n```python\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n```\n\nIf you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code).\n```python\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))\n```\n\nYou can now iterate on your training data in batches:\n\n```python\n# x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\nmodel.fit(x_train, y_train, epochs=5, batch_size=32)\n```\n\nAlternatively, you can feed batches to your model manually:\n\n```python\nmodel.train_on_batch(x_batch, y_batch)\n```\n\nEvaluate your performance in one line:\n\n```python\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n```\n\nOr generate predictions on new data:\n\n```python\nclasses = model.predict(x_test, batch_size=128)\n```\n\nBuilding a question answering system, an image classification model, a Neural Turing Machine, or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?\n\nFor a more in-depth tutorial about Keras, you can check out:\n\n- [Getting started with the Sequential model](https://keras.io/getting-started/sequential-model-guide)\n- [Getting started with the functional API](https://keras.io/getting-started/functional-api-guide)\n\nIn the [examples folder](https://github.com/keras-team/keras/tree/master/examples) of the repository, you will find more advanced models: question-answering with memory networks, text generation with stacked LSTMs, etc.\n\n\n------------------\n\n\n## Installation\n\nBefore installing Keras, please install one of its backend engines: TensorFlow, Theano, or CNTK. We recommend the TensorFlow backend.\n\n- [TensorFlow installation instructions](https://www.tensorflow.org/install/).\n- [Theano installation instructions](http://deeplearning.net/software/theano/install.html#install).\n- [CNTK installation instructions](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-cntk-on-your-machine).\n\nYou may also consider installing the following **optional dependencies**:\n\n- [cuDNN](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/) (recommended if you plan on running Keras on GPU).\n- HDF5 and [h5py](http://docs.h5py.org/en/latest/build.html) (required if you plan on saving Keras models to disk).\n- [graphviz](https://graphviz.gitlab.io/download/) and [pydot](https://github.com/erocarrera/pydot) (used by [visualization utilities](https://keras.io/visualization/) to plot model graphs).\n\nThen, you can install Keras itself. There are two ways to install Keras:\n\n- **Install Keras from PyPI (recommended):**\n\nNote: These installation steps assume that you are on a Linux or Mac environment.\nIf you are on Windows, you will need to remove `sudo` to run the commands below.\n\n```sh\nsudo pip install keras\n```\n\nIf you are using a virtualenv, you may want to avoid using sudo:\n\n```sh\npip install keras\n```\n\n- **Alternatively: install Keras from the GitHub source:**\n\nFirst, clone Keras using `git`:\n\n```sh\ngit clone https://github.com/keras-team/keras.git\n```\n\n Then, `cd` to the Keras folder and run the install command:\n```sh\ncd keras\nsudo python setup.py install\n```\n\n------------------\n\n\n## Configuring your Keras backend\n\nBy default, Keras will use TensorFlow as its tensor manipulation library. [Follow these instructions](https://keras.io/backend/) to configure the Keras backend.\n\n------------------\n\n\n## Support\n\nYou can ask questions and join the development discussion:\n\n- On the [Keras Google group](https://groups.google.com/forum/#!forum/keras-users).\n- On the [Keras Slack channel](https://kerasteam.slack.com). Use [this link](https://keras-slack-autojoin.herokuapp.com/) to request an invitation to the channel.\n\nYou can also post **bug reports and feature requests** (only) in [GitHub issues](https://github.com/keras-team/keras/issues). Make sure to read [our guidelines](https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md) first.\n\n\n------------------\n\n\n## Why this name, Keras?\n\nKeras (\u03ba\u03ad\u03c1\u03b1\u03c2) means _horn_ in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the _Odyssey_, where dream spirits (_Oneiroi_, singular _Oneiros_) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words \u03ba\u03ad\u03c1\u03b1\u03c2 (horn) / \u03ba\u03c1\u03b1\u03af\u03bd\u03c9 (fulfill), and \u1f10\u03bb\u03ad\u03c6\u03b1\u03c2 (ivory) / \u1f10\u03bb\u03b5\u03c6\u03b1\u03af\u03c1\u03bf\u03bc\u03b1\u03b9 (deceive).\n\nKeras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\n\n>_\"Oneiroi are beyond our unravelling --who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them.\"_ Homer, Odyssey 19. 562 ff (Shewring translation).\n\n------------------\n"}, "librosa": {"file_name": "librosa/librosa/README.md", "raw_text": "librosa\n=======\nA python package for music and audio analysis.  \n\n[![PyPI](https://img.shields.io/pypi/v/librosa.svg)](https://pypi.python.org/pypi/librosa)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/librosa/badges/version.svg)](https://anaconda.org/conda-forge/librosa)\n[![License](https://img.shields.io/pypi/l/librosa.svg)](https://github.com/librosa/librosa/blob/master/LICENSE.md)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.591533.svg)](https://doi.org/10.5281/zenodo.591533)\n\n[![Build Status](https://travis-ci.org/librosa/librosa.png?branch=master)](http://travis-ci.org/librosa/librosa?branch=master)\n[![Build status](https://ci.appveyor.com/api/projects/status/8i1hhr8yj78195xf?svg=true)](https://ci.appveyor.com/project/bmcfee/librosa)\n[![Coverage Status](https://coveralls.io/repos/librosa/librosa/badge.svg?branch=master)](https://coveralls.io/r/librosa/librosa?branch=master)\n\n\nDocumentation\n-------------\nSee http://librosa.github.io/librosa/ for a complete reference manual and introductory tutorials.\n\n\nDemonstration notebooks\n-----------------------\nWhat does librosa do?  Here are some quick demonstrations:\n\n* [Introduction notebook](http://nbviewer.ipython.org/github/librosa/librosa/blob/master/examples/LibROSA%20demo.ipynb): a brief introduction to some commonly used features.\n* [Decomposition and IPython integration](http://nbviewer.ipython.org/github/librosa/librosa/blob/master/examples/LibROSA%20audio%20effects%20and%20playback.ipynb): an intermediate demonstration, illustrating how to process and play back sound\n\n\nInstallation\n------------\n\nThe latest stable release is available on PyPI, and you can install it by saying\n```\npip install librosa\n```\n\nAnaconda users can install using ``conda-forge``:\n```\nconda install -c conda-forge librosa\n```\n\nTo build librosa from source, say `python setup.py build`.\nThen, to install librosa, say `python setup.py install`.\nIf all went well, you should be able to execute the demo scripts under `examples/`\n(OS X users should follow the installation guide given below).\n\nAlternatively, you can download or clone the repository and use `pip` to handle dependencies:\n\n```\nunzip librosa.zip\npip install -e librosa\n```\nor\n```\ngit clone https://github.com/librosa/librosa.git\npip install -e librosa\n```\n\nBy calling `pip list` you should see `librosa` now as an installed package:\n```\nlibrosa (0.x.x, /path/to/librosa)\n```\n\n### Hints for the Installation\n\n`librosa` uses `soundfile` and `audioread` to load audio files.\nNote that `soundfile` does not currently support MP3, which will cause librosa to\nfall back on the `audioread` library.\n\n#### soundfile\n\nIf you're using `conda` to install librosa, then most audio coding dependencies (except MP3) will be handled automatically.\n\nIf you're using `pip` on a Linux environment, you may need to install `libsndfile`\nmanually.  Please refer to the [SoundFile installation documentation](https://pysoundfile.readthedocs.io/#installation) for details.\n\n#### audioread and MP3 support\n\nTo fuel `audioread` with more audio-decoding power (e.g., for reading MP3 files),\nyou may need to install either *ffmpeg* or *GStreamer*.\n\n*Note that on some platforms, `audioread` needs at least one of the programs to work properly.*\n\nIf you are using Anaconda, install *ffmpeg* by calling\n```\nconda install -c conda-forge ffmpeg\n```\n\nIf you are not using Anaconda, here are some common commands for different operating systems:\n\n* Linux (apt-get): `apt-get install ffmpeg` or `apt-get install gstreamer1.0-plugins-base gstreamer1.0-plugins-ugly`\n* Linux (yum): `yum install ffmpeg` or `yum install gstreamer1.0-plugins-base gstreamer1.0-plugins-ugly`\n* Mac: `brew install ffmpeg` or `brew install gstreamer`\n* Windows: download binaries from this [website]( https://gstreamer.freedesktop.org/) \n\nFor GStreamer, you also need to install the Python bindings with \n```\npip install pygobject\n```\n\nDiscussion\n----------\n\nPlease direct non-development questions and discussion topics to our web forum at\nhttps://groups.google.com/forum/#!forum/librosa\n\n\nCiting\n------\n\nIf you want to cite librosa in a scholarly work, there are two ways to do it.\n\n- If you are using the library for your work, for the sake of reproducibility, please cite\n  the version you used as indexed at Zenodo:\n\n    [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.591533.svg)](https://doi.org/10.5281/zenodo.591533)\n\n- If you wish to cite librosa for its design, motivation etc., please cite the paper\n  published at SciPy 2015:\n\n    McFee, Brian, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. \"librosa: Audio and music signal analysis in python.\" In Proceedings of the 14th python in science conference, pp. 18-25. 2015.\n"}, "lightgbm": {"file_name": "microsoft/LightGBM/README.md", "raw_text": "LightGBM, Light Gradient Boosting Machine\n=========================================\n\n[![Azure Pipelines Build Status](https://lightgbm-ci.visualstudio.com/lightgbm-ci/_apis/build/status/Microsoft.LightGBM?branchName=master)](https://lightgbm-ci.visualstudio.com/lightgbm-ci/_build/latest?definitionId=1)\n[![Appveyor Build Status](https://ci.appveyor.com/api/projects/status/1ys5ot401m0fep6l/branch/master?svg=true)](https://ci.appveyor.com/project/guolinke/lightgbm/branch/master)\n[![Travis Build Status](https://travis-ci.org/microsoft/LightGBM.svg?branch=master)](https://travis-ci.org/microsoft/LightGBM)\n[![Documentation Status](https://readthedocs.org/projects/lightgbm/badge/?version=latest)](https://lightgbm.readthedocs.io/)\n[![License](https://img.shields.io/github/license/microsoft/lightgbm.svg)](https://github.com/microsoft/LightGBM/blob/master/LICENSE)\n[![Python Versions](https://img.shields.io/pypi/pyversions/lightgbm.svg?logo=python&logoColor=white)](https://pypi.org/project/lightgbm)\n[![PyPI Version](https://img.shields.io/pypi/v/lightgbm.svg?logo=pypi&logoColor=white)](https://pypi.org/project/lightgbm)\n[![Join Gitter at https://gitter.im/Microsoft/LightGBM](https://badges.gitter.im/Microsoft/LightGBM.svg)](https://gitter.im/Microsoft/LightGBM?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Slack](https://lightgbm-slack-autojoin.herokuapp.com/badge.svg)](https://lightgbm-slack-autojoin.herokuapp.com)\n\nLightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n- Faster training speed and higher efficiency.\n- Lower memory usage.\n- Better accuracy.\n- Support of parallel and GPU learning.\n- Capable of handling large-scale data.\n\nFor further details, please refer to [Features](https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst).\n\nBenefitting from these advantages, LightGBM is being widely-used in many [winning solutions](https://github.com/microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions) of machine learning competitions.\n\n[Comparison experiments](https://github.com/microsoft/LightGBM/blob/master/docs/Experiments.rst#comparison-experiment) on public datasets show that LightGBM can outperform existing boosting frameworks on both efficiency and accuracy, with significantly lower memory consumption. What's more, [parallel experiments](https://github.com/microsoft/LightGBM/blob/master/docs/Experiments.rst#parallel-experiment) show that LightGBM can achieve a linear speed-up by using multiple machines for training in specific settings.\n\nGet Started and Documentation\n-----------------------------\n\nOur primary documentation is at https://lightgbm.readthedocs.io/ and is generated from this repository. If you are new to LightGBM, follow [the installation instructions](https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html) on that site.\n\nNext you may want to read:\n\n- [**Examples**](https://github.com/microsoft/LightGBM/tree/master/examples) showing command line usage of common tasks.\n- [**Features**](https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst) and algorithms supported by LightGBM.\n- [**Parameters**](https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst) is an exhaustive list of customization you can make.\n- [**Parallel Learning**](https://github.com/microsoft/LightGBM/blob/master/docs/Parallel-Learning-Guide.rst) and [**GPU Learning**](https://github.com/microsoft/LightGBM/blob/master/docs/GPU-Tutorial.rst) can speed up computation.\n- [**Laurae++ interactive documentation**](https://sites.google.com/view/lauraepp/parameters) is a detailed guide for hyperparameters.\n- [**Optuna Hyperparameter Tuner**](https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258) provides automated tuning for LightGBM hyperparameters.\n\nDocumentation for contributors:\n\n- [**How we update readthedocs.io**](https://github.com/microsoft/LightGBM/blob/master/docs/README.rst).\n- Check out the [**Development Guide**](https://github.com/microsoft/LightGBM/blob/master/docs/Development-Guide.rst).\n\nNews\n----\n\nPlease refer to changelogs at [GitHub releases](https://github.com/microsoft/LightGBM/releases) page.\n\nSome old update logs are available at [Key Events](https://github.com/microsoft/LightGBM/blob/master/docs/Key-Events.md) page.\n\nExternal (Unofficial) Repositories\n----------------------------------\n\nOptuna (hyperparameter optimization framework): https://github.com/optuna/optuna\n\nJulia-package: https://github.com/Allardvm/LightGBM.jl\n\nJPMML (Java PMML converter): https://github.com/jpmml/jpmml-lightgbm\n\nTreelite (model compiler for efficient deployment): https://github.com/dmlc/treelite\n\ncuML Forest Inference Library (GPU-accelerated inference): https://github.com/rapidsai/cuml\n\nm2cgen (model appliers for various languages): https://github.com/BayesWitnesses/m2cgen\n\nleaves (Go model applier): https://github.com/dmitryikh/leaves\n\nONNXMLTools (ONNX converter): https://github.com/onnx/onnxmltools\n\nSHAP (model output explainer): https://github.com/slundberg/shap\n\nMMLSpark (LightGBM on Spark): https://github.com/Azure/mmlspark\n\nKubeflow Fairing (LightGBM on Kubernetes): https://github.com/kubeflow/fairing\n\nML.NET (.NET/C#-package): https://github.com/dotnet/machinelearning\n\nLightGBM.NET (.NET/C#-package): https://github.com/rca22/LightGBM.Net\n\nDask-LightGBM (distributed and parallel Python-package): https://github.com/dask/dask-lightgbm\n\nRuby gem: https://github.com/ankane/lightgbm\n\nSupport\n-------\n\n- Ask a question [on Stack Overflow with the `lightgbm` tag](https://stackoverflow.com/questions/ask?tags=lightgbm), we monitor this for new questions.\n- Discuss on the [LightGBM Gitter](https://gitter.im/Microsoft/LightGBM).\n- Discuss on the [LightGBM Slack team](https://lightgbm.slack.com).\n  - Use [this invite link](https://lightgbm-slack-autojoin.herokuapp.com/) to join the team.\n- Open **bug reports** and **feature requests** (not questions) on [GitHub issues](https://github.com/microsoft/LightGBM/issues).\n\nHow to Contribute\n-----------------\n\nCheck [CONTRIBUTING](https://github.com/microsoft/LightGBM/blob/master/CONTRIBUTING.md) page.\n\nMicrosoft Open Source Code of Conduct\n-------------------------------------\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\nReference Papers\n----------------\n\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. \"[LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)\". Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157.\n\nQi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu. \"[A Communication-Efficient Parallel Algorithm for Decision Tree](http://papers.nips.cc/paper/6380-a-communication-efficient-parallel-algorithm-for-decision-tree)\". Advances in Neural Information Processing Systems 29 (NIPS 2016), pp. 1279-1287.\n\nHuan Zhang, Si Si and Cho-Jui Hsieh. \"[GPU Acceleration for Large-scale Tree Boosting](https://arxiv.org/abs/1706.08359)\". SysML Conference, 2018.\n\n**Note**: If you use LightGBM in your GitHub projects, please add `lightgbm` in the `requirements.txt`.\n\nLicense\n-------\n\nThis project is licensed under the terms of the MIT license. See [LICENSE](https://github.com/microsoft/LightGBM/blob/master/LICENSE) for additional details.\n"}, "lime": {"file_name": "marcotcr/lime/README.md", "raw_text": "# lime\n\n[![Build Status](https://travis-ci.org/marcotcr/lime.svg?branch=master)](https://travis-ci.org/marcotcr/lime)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/marcotcr/lime/master)\n\nThis project is about explaining what machine learning classifiers (or models) are doing.\nAt the moment, we support explaining individual predictions for text classifiers or classifiers that act on tables (numpy arrays of numerical or categorical data) or images, with a package called lime (short for local interpretable model-agnostic explanations).\nLime is based on the work presented in [this paper](https://arxiv.org/abs/1602.04938) ([bibtex here for citation](https://github.com/marcotcr/lime/blob/master/citation.bib)). Here is a link to the promo video:\n\n<a href=\"https://www.youtube.com/watch?v=hUnRCxnydCc\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/marcotcr/lime/master/doc/images/video_screenshot.png\" width=\"450\" alt=\"KDD promo video\"/></a>\n\nOur plan is to add more packages that help users understand and interact meaningfully with machine learning.\n\nLime is able to explain any black box classifier, with two or more classes. All we require is that the classifier implements a function that takes in raw text or a numpy array and outputs a probability for each class. Support for scikit-learn classifiers is built-in.\n\n## Installation\n\nThe lime package is on [PyPI](https://pypi.python.org/pypi/lime). Simply run:\n\n```sh\npip install lime\n```\n\nOr clone the repository and run:\n\n```sh\npip install .\n```\n\nWe dropped python2 support in `0.2.0`, `0.1.1.37` was the last version before that.\n\n## Screenshots\n\nBelow are some screenshots of lime explanations. These are generated in html, and can be easily produced and embedded in ipython notebooks. We also support visualizations using matplotlib, although they don't look as nice as these ones.\n\n#### Two class case, text\n\nNegative (blue) words indicate atheism, while positive (orange) words indicate christian. The way to interpret the weights by applying them to the prediction probabilities. For example, if we remove the words Host and NNTP from the document, we expect the classifier to predict atheism with probability 0.58 - 0.14 - 0.11 = 0.31.\n\n![twoclass](doc/images/twoclass.png)\n\n#### Multiclass case\n\n![multiclass](doc/images/multiclass.png)\n\n#### Tabular data\n\n![tabular](doc/images/tabular.png)\n\n#### Images (explaining prediction of 'Cat' in pros and cons)\n\n<img src=\"https://raw.githubusercontent.com/marcotcr/lime/master/doc/images/images.png\" width=200 />\n\n## Tutorials and API\n\nFor example usage for text classifiers, take a look at the following two tutorials (generated from ipython notebooks):\n\n- [Basic usage, two class. We explain random forest classifiers.](https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html)\n- [Multiclass case](https://marcotcr.github.io/lime/tutorials/Lime%20-%20multiclass.html)\n\nFor classifiers that use numerical or categorical data, take a look at the following tutorial (this is newer, so please let me know if you find something wrong):\n\n- [Tabular data](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html)\n- [Tabular data with H2O models](https://marcotcr.github.io/lime/tutorials/Tutorial_H2O_continuous_and_cat.html)\n\nFor image classifiers:\n\n- [Images - basic](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20images.html)\n- [Images - Faces](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20Faces%20and%20GradBoost.ipynb)\n- [Images with Keras](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20Image%20Classification%20Keras.ipynb)\n- [MNIST with random forests](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20MNIST%20and%20RF.ipynb)\n- [Images with PyTorch](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20images%20-%20Pytorch.ipynb)\n\nFor regression:\n\n- [Simple regression](https://marcotcr.github.io/lime/tutorials/Using%2Blime%2Bfor%2Bregression.html)\n\nSubmodular Pick:\n\n- [Submodular Pick](https://github.com/marcotcr/lime/tree/master/doc/notebooks/Submodular%20Pick%20examples.ipynb)\n\nThe raw (non-html) notebooks for these tutorials are available [here](https://github.com/marcotcr/lime/tree/master/doc/notebooks).\n\nThe API reference is available [here](https://lime-ml.readthedocs.io/en/latest/).\n\n## What are explanations?\n\nIntuitively, an explanation is a local linear approximation of the model's behaviour.\nWhile the model may be very complex globally, it is easier to approximate it around the vicinity of a particular instance.\nWhile treating the model as a black box, we perturb the instance we want to explain and learn a sparse linear model around it, as an explanation.\nThe figure below illustrates the intuition for this procedure. The model's decision function is represented by the blue/pink background, and is clearly nonlinear.\nThe bright red cross is the instance being explained (let's call it X).\nWe sample instances around X, and weight them according to their proximity to X (weight here is indicated by size).\nWe then learn a linear model (dashed line) that approximates the model well in the vicinity of X, but not necessarily globally. For more information, [read our paper](https://arxiv.org/abs/1602.04938), or take a look at [this blog post](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime).\n\n<img src=\"https://raw.githubusercontent.com/marcotcr/lime/master/doc/images/lime.png\" width=300px />\n\n## Contributing\n\nPlease read [this](CONTRIBUTING.md).\n"}, "ludwig": {"file_name": "uber/ludwig/README.md", "raw_text": "![Ludwig logo](https://github.com/uber/ludwig/raw/master/docs/images/ludwig_hero.png \"Ludwig logo\")\n\nLudwig is a toolbox built on top of TensorFlow that allows users to train and test deep learning models without the need to write code.\n\nAll you need to provide is a CSV file containing your data, a list of columns to use as inputs, and a list of columns to use as outputs, Ludwig will do the rest.\nSimple commands can be used to train models both locally and in a distributed way, and to use them to predict new data.\n\nA programmatic API is also available in order to use Ludwig from your python code.\nA suite of visualization tools allows you to analyze models' training and test performance and to compare them.\n\nLudwig is built with extensibility principles in mind and is based on data type abstractions, making it easy to add support for new data types as well as new model architectures.\n\nIt can be used by practitioners to quickly train and test deep learning models as well as by researchers to obtain strong baselines to compare against and have an experimentation setting that ensures comparability by performing standard data preprocessing and visualization.\n\nLudwig provides a set of model architectures that can be combined together to create an end-to-end model for a given use case. As an analogy, if deep learning libraries provide the building blocks to make your building, Ludwig provides the buildings to make your city, and you can choose among the available buildings or add your own building to the set of available ones.\n\nThe core design principles we baked into the toolbox are:\n- No coding required: no coding skills are required to train a model and use it for obtaining predictions.\n- Generality: a new data type-based approach to deep learning model design that makes the tool usable across many different use cases.\n- Flexibility: experienced users have extensive control over model building and training, while newcomers will find it easy to use.\n- Extensibility: easy to add new model architecture and new feature data types.\n- Understandability: deep learning model internals are often considered black boxes, but we provide standard visualizations to understand their performance and compare their predictions.\n- Open Source: Apache License 2.0\n\n\nInstallation\n============\n\nLudwig's basic requirements are the following:\n\n- tensorflow\n- numpy\n- pandas\n- scipy\n- scikit-learn\n- Cython\n- h5py\n- tabulate\n- tqdm\n- PyYAML\n- absl-py\n\nLudwig has been developed and tested with Python 3 in mind.\nIf you don\u2019t have Python 3 installed, install it by running:\n\n```\nsudo apt install python3  # on ubuntu\nbrew install python3      # on mac\n```\n\nYou may want to use a virtual environment to maintain an isolated [Python environment](https://docs.python-guide.org/dev/virtualenvs/).\n\n```\nvirtualenv -p python3 venv\n```\n\nIn order to install Ludwig just run:\n\n```\npip install ludwig\n```\n\nor install it by building the source code from the repository:\n\n```\ngit clone git@github.com:uber/ludwig.git\ncd ludwig\nvirtualenv -p python3 venv\nsource venv/bin/activate\npip install -r requirements.txt\npython setup.py install\n```\n\nThis will install only Ludwig-s basic requirements, different feature types require different dependencies.\nWe divided them as different extras so that users could install only the ones they actually need.\n\nText features extra packages can be installed with `pip install ludwig[text]` and include:\n\n- spacy\n- bert-tensorflow\n\nIf you intend to use text features and want to use [spaCy](http://spacy.io) based language tokenizers, install language-specific models with:\n```\npython -m spacy download <language_code>\n```\nMore details in the [User Guide](https://uber.github.io/ludwig/user_guide/#spacy-based-word-format-options).\n\nImage features extra packages can be installed with `pip install ludwig[image]` and include:\n\n- scikit-image\n\nAudio features extra packages can be installed with `pip install ludwig[audio]` and include:\n\n- soundfile\n\nVisualization extra packages can be installed with `pip install ludwig[viz]` and include:\n\n- matplotlib\n- seaborn\n\nModel serving extra packages can be installed with `pip install ludwig[serve]` and include:\n\n- fastapi\n- uvicorn\n- pydantic\n- python-multipart\n\nAny combination of extra packages can be installed at the same time with `pip install ludwig[extra1,extra2,...]` like for instance `pip install ludwig[text,viz]`.\nThe full set of dependencies can be installed with `pip install ludwig[full]`.\n\nBeware that the `tensorflow` package contained in the `requirements.txt` file is the CPU version. If you prefer to install the GPU version, uninstall `tensorflow` and replace it with `tensorflow=gpu` after having installed `ludwig`, being careful at matching the version ludwig requires, as shown in `requirements.txt`.\n\nIf you want to train Ludwig models in a distributed way, you need to also install the `horovod` and the `mpi4py` packages.\nPlease follow the instructions on [Horovod's repository](https://github.com/uber/horovod) to install it.\n\n\nBasic Principles\n----------------\n\nLudwig provides two main functionalities: training models and using them to predict.\nIt is based on datatype abstraction, so that the same data preprocessing and postprocessing will be performed on different datasets that share data types and the same encoding and decoding models developed for one task can be reused for different tasks.\n\nTraining a model in Ludwig is pretty straightforward: you provide a CSV dataset and a model definition YAML file.\n\nThe model definition contains a list of input features and output features, all you have to do is specify names of the columns in the CSV that are inputs to your model alongside with their data types, and names of columns in the CSV that will be outputs, the target variables which the model will learn to predict.\nLudwig will compose a deep learning model accordingly and train it for you.\n\nCurrently, the available datatypes in Ludwig are:\n\n- binary\n- numerical\n- category\n- set\n- bag\n- sequence\n- text\n- timeseries\n- image\n- audio\n- date\n- h3\n- vector\n\nThe model definition can contain additional information, in particular how to preprocess each column in the CSV, which encoder and decoder to use for each one, feature hyperparameters and training parameters.\nThis allows ease of use for novices and flexibility for experts.\n\n\nTraining\n--------\n\nFor example, given a text classification dataset like the following:\n\n| doc_text                              | class    |\n|---------------------------------------|----------|\n| Former president Barack Obama ...     | politics |\n| Juventus hired Cristiano Ronaldo ...  | sport    |\n| LeBron James joins the Lakers ...     | sport    |\n| ...                                   | ...      |\n\nyou want to learn a model that uses the content of the `doc_text` column as input to predict the values in the `class` column.\nYou can use the following model definition:\n\n```yaml\n{input_features: [{name: doc_text, type: text}], output_features: [{name: class, type: category}]}\n```\n\nand start the training typing the following command in your console:\n\n```\nludwig train --data_csv path/to/file.csv --model_definition \"{input_features: [{name: doc_text, type: text}], output_features: [{name: class, type: category}]}\"\n```\n\nwhere `path/to/file.csv` is the path to a UTF-8 encoded CSV file containing the dataset in the previous table.\nLudwig will perform a random split of the data, preprocess it, build a WordCNN model (the default for text features) that decodes output classes through a softmax classifier, train the model on the training set until the accuracy on the validation set stops improving.\nTraining progress will be displayed in the console, but TensorBoard can also be used.\n\nIf you prefer to use an RNN encoder and increase the number of epochs you want the model to train for, all you have to do is to change the model definition to:\n\n```yaml\n{input_features: [{name: doc_text, type: text, encoder: rnn}], output_features: [{name: class, type: category}], training: {epochs: 50}}\n```\n\nRefer to the [User Guide](https://uber.github.io/ludwig/user_guide/) to find out all the options available to you in the model definition and take a look at the [Examples](https://uber.github.io/ludwig/examples/) to see how you can use Ludwig for several different tasks.\n\nAfter training, Ludwig will create a directory under `results` containing the trained model with its hyperparameters and summary statistics of the training process.\nYou can visualize them using one of the several visualization options available in the `visualize` tool, for instance:\n\n```\nludwig visualize --visualization learning_curves --training_statistics path/to/training_statistics.json\n```\n\nThe commands will display a graph that looks like the following, where you can see loss and accuracy as functions of train iteration number:\n\n![Learning Curves](https://github.com/uber/ludwig/raw/master/docs/images/getting_started_learning_curves.png \"Learning Curves\")\n\nSeveral visualizations are available, please refer to [Visualizations](https://uber.github.io/ludwig/user_guide/#visualizations) for more details.\n\n\nDistributed Training\n--------------------\n\nYou can distribute the training of your models using [Horovod](https://github.com/uber/horovod), which allows training on a single machine with multiple GPUs as well as on multiple machines with multiple GPUs.\nRefer to the [User Guide](https://uber.github.io/ludwig/user_guide/#distributed-training) for more details.\n\n\nPredict\n-------\n\nIf you have new data and you want your previously trained model to predict target output values, you can type the following command in your console:\n\n```\nludwig predict --data_csv path/to/data.csv --model_path /path/to/model\n```\n\nRunning this command will return model predictions and some test performance statistics if the dataset contains ground truth information to compare to.\nThose can be visualized by the `visualize` tool, which can also be used to compare performances and predictions of different models, for instance:\n\n```\nludwig visualize --visualization compare_performance --test_statistics path/to/test_statistics_model_1.json path/to/test_statistics_model_2.json\n```\n\nwill return a bar plot comparing the models on different measures:\n\n![Performance Comparison](https://github.com/uber/ludwig/raw/master/docs/images/compare_performance.png \"Performance Comparison\")\n\nA handy `ludwig experiment` command that performs training and prediction one after the other is also available.\n\n\nProgrammatic API\n----------------\n\nLudwig also provides a simple programmatic API that allows you to train or load a model and use it to obtain predictions on new data:\n\n```python\nfrom ludwig.api import LudwigModel\n\n# train a model\nmodel_definition = {...}\nmodel = LudwigModel(model_definition)\ntrain_stats = model.train(training_dataframe)\n\n# or load a model\nmodel = LudwigModel.load(model_path)\n\n# obtain predictions\npredictions = model.predict(test_dataframe)\n\nmodel.close()\n```\n\n`model_definition` is a dictionary containing the same information of the YAML file.\nMore details are provided in the [User Guide](https://uber.github.io/ludwig/user_guide/) and in the [API documentation](https://uber.github.io/ludwig/api/).\n\n\nExtensibility\n-------------\n\nLudwig is built from the ground up with extensibility in mind.\nIt is easy to add an additional data type that is not currently supported by adding a datatype-specific implementation of abstract classes that contain functions to preprocess the data, encode it, and decode it.\n\nFurthermore, new models, with their own specific hyperparameters, can be easily added by implementing a class that accepts tensors (of a specific rank, depending on the datatype) as inputs and provides tensors as output.\nThis encourages reuse and sharing new models with the community.\nRefer to the [Developer Guide](https://uber.github.io/ludwig/developer_guide/) for further details.\n\n\nFull documentation\n------------------\n\nYou can find the full documentation [here](http://uber.github.io/ludwig/).\n"}, "Markdown": {"file_name": "Python-Markdown/markdown/README.md", "raw_text": "[Python-Markdown][]\n===================\n\n[![Build Status][travis-button]][travis]\n[![Coverage Status][codecov-button]][codecov]\n[![Latest Version][mdversion-button]][md-pypi]\n[![Python Versions][pyversion-button]][md-pypi]\n[![BSD License][bsdlicense-button]][bsdlicense]\n[![Code of Conduct][codeofconduct-button]][Code of Conduct]\n\n[travis-button]: https://img.shields.io/travis/Python-Markdown/markdown.svg\n[travis]: https://travis-ci.org/Python-Markdown/markdown\n[codecov-button]: https://codecov.io/gh/Python-Markdown/markdown/branch/master/graph/badge.svg\n[codecov]: https://codecov.io/gh/Python-Markdown/markdown\n[mdversion-button]: https://img.shields.io/pypi/v/Markdown.svg\n[md-pypi]: https://pypi.org/project/Markdown/\n[pyversion-button]: https://img.shields.io/pypi/pyversions/Markdown.svg\n[bsdlicense-button]: https://img.shields.io/badge/license-BSD-yellow.svg\n[bsdlicense]: https://opensource.org/licenses/BSD-3-Clause\n[codeofconduct-button]: https://img.shields.io/badge/code%20of%20conduct-contributor%20covenant-green.svg?style=flat-square\n[Code of Conduct]: https://github.com/Python-Markdown/markdown/blob/master/CODE_OF_CONDUCT.md\n\nThis is a Python implementation of John Gruber's [Markdown][].\nIt is almost completely compliant with the reference implementation,\nthough there are a few known issues. See [Features][] for information\non what exactly is supported and what is not. Additional features are\nsupported by the [Available Extensions][].\n\n[Python-Markdown]: https://Python-Markdown.github.io/\n[Markdown]: https://daringfireball.net/projects/markdown/\n[Features]: https://Python-Markdown.github.io#Features\n[Available Extensions]: https://Python-Markdown.github.io/extensions\n\nDocumentation\n-------------\n\nInstallation and usage documentation is available in the `docs/` directory\nof the distribution and on the project website at\n<https://Python-Markdown.github.io/>.\n\nSee the change log at <https://Python-Markdown.github.io/change_log>.\n\nSupport\n-------\n\nYou may report bugs, ask for help, and discuss various other issues on the [bug tracker][].\n\n[bug tracker]: https://github.com/Python-Markdown/markdown/issues\n\nCode of Conduct\n---------------\n\nEveryone interacting in the Python-Markdown project's codebases, issue trackers,\nand mailing lists is expected to follow the [Code of Conduct].\n"}, "matplotlib": {"file_name": "matplotlib/matplotlib/README.rst", "raw_text": "|PyPi|_ |Downloads|_ |NUMFocus|_\n\n|DiscourseBadge|_ |Gitter|_ |GitHubIssues|_ |GitTutorial|_\n\n|Travis|_ |AzurePipelines|_ |AppVeyor|_ |Codecov|_ |LGTM|_\n\n.. |Travis| image:: https://travis-ci.com/matplotlib/matplotlib.svg?branch=master\n.. _Travis: https://travis-ci.com/matplotlib/matplotlib\n\n.. |AzurePipelines| image:: https://dev.azure.com/matplotlib/matplotlib/_apis/build/status/matplotlib.matplotlib?branchName=master\n.. _AzurePipelines: https://dev.azure.com/matplotlib/matplotlib/_build/latest?definitionId=1&branchName=master\n\n.. |AppVeyor| image:: https://ci.appveyor.com/api/projects/status/github/matplotlib/matplotlib?branch=master&svg=true\n.. _AppVeyor: https://ci.appveyor.com/project/matplotlib/matplotlib\n\n.. |Codecov| image:: https://codecov.io/github/matplotlib/matplotlib/badge.svg?branch=master&service=github\n.. _Codecov: https://codecov.io/github/matplotlib/matplotlib?branch=master\n\n.. |LGTM| image:: https://img.shields.io/lgtm/grade/python/g/matplotlib/matplotlib.svg?logo=lgtm&logoWidth=18\n.. _LGTM: https://lgtm.com/projects/g/matplotlib/matplotlib\n\n.. |DiscourseBadge| image:: https://img.shields.io/badge/help_forum-discourse-blue.svg\n.. _DiscourseBadge: https://discourse.matplotlib.org\n\n.. |Gitter| image:: https://badges.gitter.im/matplotlib/matplotlib.svg\n.. _Gitter: https://gitter.im/matplotlib/matplotlib\n\n.. |GitHubIssues| image:: https://img.shields.io/badge/issue_tracking-github-blue.svg\n.. _GitHubIssues: https://github.com/matplotlib/matplotlib/issues\n\n.. |GitTutorial| image:: https://img.shields.io/badge/PR-Welcome-%23FF8300.svg?\n.. _GitTutorial: https://git-scm.com/book/en/v2/GitHub-Contributing-to-a-Project\n\n.. |PyPi| image:: https://badge.fury.io/py/matplotlib.svg\n.. _PyPi: https://badge.fury.io/py/matplotlib\n\n.. |Downloads| image:: https://pepy.tech/badge/matplotlib/month\n.. _Downloads: https://pepy.tech/project/matplotlib/month\n\n.. |NUMFocus| image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A\n.. _NUMFocus: https://numfocus.org\n\n.. image:: https://matplotlib.org/_static/logo2.svg\n\nMatplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n\nCheck out our `home page <https://matplotlib.org/>`_ for more information.\n\n.. image:: https://matplotlib.org/_static/readme_preview.png\n\nMatplotlib produces publication-quality figures in a variety of hardcopy formats\nand interactive environments across platforms. Matplotlib can be used in Python scripts,\nthe Python and IPython shell, web application servers, and various\ngraphical user interface toolkits.\n\n\nInstall\n=======\n\nFor installation instructions and requirements, see `INSTALL.rst <INSTALL.rst>`_  or the\n`install <https://matplotlib.org/users/installing.html>`_ documentation.\n\nTest\n====\n\nAfter installation, launch the test suite::\n\n  python -m pytest\n\nRead the `testing guide <https://matplotlib.org/devel/testing.html>`_ for more information and alternatives.\n\nContribute\n==========\nYou've discovered a bug or something else you want to change - excellent!\n\nYou've worked out a way to fix it \u2013 even better!\n\nYou want to tell us about it \u2013 best of all!\n\nStart at the `contributing guide <https://matplotlib.org/devdocs/devel/contributing.html>`_!\n\nContact\n=======\n\n`Discourse <https://discourse.matplotlib.org/>`_ is the discussion forum for general questions and discussions and our recommended starting point.\n\nOur active mailing lists (which are mirrored on Discourse) are:\n\n* `Users <https://mail.python.org/mailman/listinfo/matplotlib-users>`_ mailing list: matplotlib-users@python.org\n* `Announcement  <https://mail.python.org/mailman/listinfo/matplotlib-announce>`_ mailing list: matplotlib-announce@python.org\n* `Development <https://mail.python.org/mailman/listinfo/matplotlib-devel>`_ mailing list: matplotlib-devel@python.org\n\nGitter_ is for coordinating development and asking questions directly related\nto contributing to matplotlib.\n\n\nCiting Matplotlib\n=================\nIf Matplotlib contributes to a project that leads to publication, please\nacknowledge this by citing Matplotlib.\n\n`A ready-made citation entry <https://matplotlib.org/citing.html>`_ is available.\n"}, "markovify": {"file_name": "jsvine/markovify/README.md", "raw_text": "[![Version](https://img.shields.io/pypi/v/markovify.svg)](https://pypi.python.org/pypi/markovify) [![Build status](https://travis-ci.org/jsvine/markovify.png)](https://travis-ci.org/jsvine/markovify) [![Code coverage](https://img.shields.io/coveralls/jsvine/markovify.svg)](https://coveralls.io/github/jsvine/markovify) [![Support Python versions](https://img.shields.io/pypi/pyversions/markovify.svg)](https://pypi.python.org/pypi/markovify)\n\n\n# Markovify\n\nMarkovify is a simple, extensible Markov chain generator. Right now, its primary use is for building Markov models of large corpora of text and generating random sentences from that. However, in theory, it could be used for [other applications](http://en.wikipedia.org/wiki/Markov_chain#Applications).\n\n- [Why Markovify?](#why-markovify)\n- [Installation](#installation)\n- [Basic Usage](#basic-usage)\n- [Advanced Usage](#advanced-usage)\n- [Markovify In The Wild](#markovify-in-the-wild)\n- [Thanks](#thanks)\n\n## Why Markovify?\n\nSome reasons:\n\n- Simplicity. \"Batteries included,\" but it is easy to override key methods.\n\n- Models can be stored as JSON, allowing you to cache your results and save them for later.\n\n- Text parsing and sentence generation methods are highly extensible, allowing you to set your own rules.\n\n- Relies only on pure-Python libraries, and very few of them.\n\n- Tested on Python 2.7, 3.4, 3.5, 3.6 and 3.7.\n\n\n## Installation\n\n```\npip install markovify\n```\n\n## Basic Usage\n\n```python\nimport markovify\n\n# Get raw text as string.\nwith open(\"/path/to/my/corpus.txt\") as f:\n    text = f.read()\n\n# Build the model.\ntext_model = markovify.Text(text)\n\n# Print five randomly-generated sentences\nfor i in range(5):\n    print(text_model.make_sentence())\n\n# Print three randomly-generated sentences of no more than 280 characters\nfor i in range(3):\n    print(text_model.make_short_sentence(280))\n```\n\nNotes:\n\n- The usage examples here assume you are trying to markovify text. If you would like to use the underlying `markovify.Chain` class, which is not text-specific, check out [the (annotated) source code](markovify/chain.py).\n\n- Markovify works best with large, well-punctuated texts. If your text does not use `.`s to delineate sentences, put each sentence on a newline, and use the `markovify.NewlineText` class instead of `markovify.Text` class.\n\n- If you have accidentally read the input text as one long sentence, markovify will be unable to generate new sentences from it due to a lack of beginning and ending delimiters. This issue can occur if you have read a newline delimited file using the `markovify.Text` command instead of `markovify.NewlineText`. To check this, the command `[key for key in txt.chain.model.keys() if \"___BEGIN__\" in key]` command will return all of the possible sentence-starting words and should return more than one result.\n\n- By default, the `make_sentence` method tries a maximum of 10 times per invocation, to make a sentence that does not overlap too much\u00a0with the original text. If it is successful, the method returns the sentence as a string. If not, it returns `None`. To increase or decrease the number of attempts, use the `tries` keyword argument, e.g., call `.make_sentence(tries=100)`.\n\n- By default, `markovify.Text` tries to generate sentences that do not simply regurgitate chunks of the original text. The default rule is to suppress any generated sentences that exactly overlaps the original text by 15 words or 70% of the sentence's word count. You can change this rule by passing `max_overlap_ratio` and/or `max_overlap_total` to the `make_sentence` method. Alternatively, this check can be disabled entirely by passing `test_output` as False.\n\n## Advanced Usage\n\n### Specifying the model's state size\n\nState size is a number of words the probability of a next word depends on.\n\nBy default, `markovify.Text` uses a state size of 2. But you can instantiate a model with a different state size. E.g.,:\n\n```python\ntext_model = markovify.Text(text, state_size=3)\n```\n\n### Combining models\n\nWith `markovify.combine(...)`, you can combine two or more Markov chains. The function accepts two arguments:\n\n- `models`: A list of `markovify` objects to combine. Can be instances of `markovify.Chain` or `markovify.Text` (or their subclasses), but all must be of the same type.\n- `weights`: Optional. A list \u2014\u00a0the exact length of `models` \u2014\u00a0of ints or floats indicating how much relative emphasis to place on each source. Default: `[ 1, 1, ... ]`.\n\nFor instance:\n\n```python\nmodel_a = markovify.Text(text_a)\nmodel_b = markovify.Text(text_b)\n\nmodel_combo = markovify.combine([ model_a, model_b ], [ 1.5, 1 ])\n```\n\nThis code snippet would combine `model_a` and `model_b`, but, it would also place 50% more weight on the connections from `model_a`.\n\n### Compiling a model\n\nOnce a model has been generated, it may also be compiled for improved text generation speed and reduced size.\n```python\ntext_model = markovify.Text(text)\ntext_model = text_model.compile()\n```\n\nModels may also be compiled in-place:\n```python\ntext_model = markovify.Text(text)\ntext_model.compile(inplace = True)\n```\n\nCurrently, compiled models may not be combined with other models using `markovify.combine(...)`.\nIf you wish to combine models, do that first and then compile the result.\n\n### Working with messy texts\n\nStarting with `v0.7.2`, `markovify.Text` accepts two additional parameters: `well_formed` and `reject_reg`.\n\n- Setting `well_formed = False` skips the step in which input sentences are rejected if they contain one of the 'bad characters' (i.e. `()[]'\"`)\n\n- Setting `reject_reg` to a regular expression of your choice allows you change the input-sentence rejection pattern. This only applies if `well_formed` is True, and if the expression is non-empty.\n\n\n### Extending `markovify.Text`\n\nThe `markovify.Text` class is highly extensible; most methods can be overridden. For example, the following `POSifiedText` class uses NLTK's part-of-speech tagger to generate a Markov model that obeys sentence structure better than a naive model. (It works; however, be warned: `pos_tag` is very slow.)\n\n```python\nimport markovify\nimport nltk\nimport re\n\nclass POSifiedText(markovify.Text):\n    def word_split(self, sentence):\n        words = re.split(self.word_split_pattern, sentence)\n        words = [ \"::\".join(tag) for tag in nltk.pos_tag(words) ]\n        return words\n\n    def word_join(self, words):\n        sentence = \" \".join(word.split(\"::\")[0] for word in words)\n        return sentence\n```\n\nOr, you can use [spaCy](https://spacy.io/) which is [way faster](https://spacy.io/docs/api/#benchmarks):\n\n```python\nimport markovify\nimport re\nimport spacy\n\nnlp = spacy.load(\"en\")\n\nclass POSifiedText(markovify.Text):\n    def word_split(self, sentence):\n        return [\"::\".join((word.orth_, word.pos_)) for word in nlp(sentence)]\n\n    def word_join(self, words):\n        sentence = \" \".join(word.split(\"::\")[0] for word in words)\n        return sentence\n```\n\nThe most useful `markovify.Text` models you can override are:\n\n- `sentence_split`\n- `sentence_join`\n- `word_split`\n- `word_join`\n- `test_sentence_input`\n- `test_sentence_output`\n\nFor details on what they do, see [the (annotated) source code](markovify/text.py).\n\n### Exporting\n\nIt can take a while to generate a Markov model from a large corpus. Sometimes you'll want to generate once and reuse it later. To export a generated `markovify.Text` model, use `my_text_model.to_json()`. For example:\n\n```python\ncorpus = open(\"sherlock.txt\").read()\n\ntext_model = markovify.Text(corpus, state_size=3)\nmodel_json = text_model.to_json()\n# In theory, here you'd save the JSON to disk, and then read it back later.\n\nreconstituted_model = markovify.Text.from_json(model_json)\nreconstituted_model.make_short_sentence(280)\n\n>>> 'It cost me something in foolscap, and I had no idea that he was a man of evil reputation among women.'\n```\n\nYou can also export the underlying Markov chain on its own \u2014\u00a0i.e., excluding the original corpus and the `state_size` metadata \u2014 via `my_text_model.chain.to_json()`.\n\n### Generating `markovify.Text` models from very large corpora\n\nBy default, the `markovify.Text` class loads, and retains, your textual corpus, so that it can compare generated sentences with the original (and only emit novel sentences). However, with very large corpora, loading the entire text at once (and retaining it) can be memory-intensive. To overcome this, you can `(a)` tell Markovify not to retain the original:\n\n```python\nwith open(\"path/to/my/huge/corpus.txt\") as f:\n    text_model = markovify.Text(f, retain_original=False)\n\nprint(text_model.make_sentence())\n```\n\nAnd `(b)` read in the corpus line-by-line or file-by-file and combine them into one model at each step:\n\n```python\ncombined_model = None\nfor (dirpath, _, filenames) in os.walk(\"path/to/my/huge/corpus\"):\n    for filename in filenames:\n        with open(os.path.join(dirpath, filename)) as f:\n            model = markovify.Text(f, retain_original=False)\n            if combined_model:\n                combined_model = markovify.combine(models=[combined_model, model])\n            else:\n                combined_model = model\n\nprint(combined_model.make_sentence())\n```\n\n\n## Markovify In The Wild\n\n- BuzzFeed's [Tom Friedman Sentence Generator](http://www.buzzfeed.com/jsvine/the-tom-friedman-sentence-generator) / [@mot_namdeirf](https://twitter.com/mot_namdeirf).\n- [/u/user_simulator](https://www.reddit.com/user/user_simulator), a Reddit bot that generates comments based on a user's comment history. [[code](https://github.com/trambelus/UserSim)]\n- [SubredditSimulator](https://www.reddit.com/r/SubredditSimulator), which [uses `markovify`](https://www.reddit.com/r/SubredditSimMeta/comments/3d910r/i_was_inspired_by_this_place_and_made_a_twitter/ct3vjp0) to generate random Reddit submissions and comments based on a subreddit's previous activity. [[code](https://github.com/Deimos/SubredditSimulator)]\n- [college crapplication](http://college-crapplication.appspot.com/), a web-app that generates college application essays. [[code](https://github.com/mattr555/college-crapplication)]\n- [@MarkovPicard](https://twitter.com/MarkovPicard), a Twitter bot based on *Star Trek: The Next Generation* transcripts. [[code](https://github.com/rdsheppard95/MarkovPicard)]\n- [sekrits.herokuapp.com](https://sekrits.herokuapp.com/), a `markovify`-powered quiz that challenges you to tell the difference between \"two file titles relating to matters of [Australian] national security\" \u2014 one real and one fake. [[code](https://sekrits.herokuapp.com/)]\n- [Hacker News Simulator](http://news.ycombniator.com/), which does what it says on the tin. [[code](https://github.com/orf/hnewssimulator)]\n- [Stak Attak](http://www.stakattak.me/), a \"poetic stackoverflow answer generator.\" [[code](https://github.com/theannielin/hackharvard)]\n- [MashBOT](https://twitter.com/mashomatic), a `markovify`-powered Twitter bot attached to a printer. Presented by [Helen J Burgess at Babel Toronto 2015](http://electric.press/mash/). [[code](https://github.com/hyperrhiz/mashbot)]\n- [The Mansfield Reporter](http://maxlupo.com/mansfield-reporter/), \"a simple device which can generate new text from some of history's greatest authors [...] running on a tiny Raspberry Pi, displaying through a tft screen from Adafruit.\" \n- [twitter markov](https://github.com/fitnr/twitter_markov), a tool to \"create markov chain (\"_ebooks\") accounts on Twitter.\"\n- [@Bern_Trump_Bot](https://twitter.com/bern_trump_bot), \"Bernie Sanders and Donald Trump driven by Markov Chains.\" [[code](https://github.com/MichaelMartinez/Bern_Trump_Bot)]\n- [@RealTrumpTalk](https://twitter.com/RealTrumpTalk), \"A bot that uses the things that @realDonaldTrump tweets to create it's own tweets.\" [[code](https://github.com/CastleCorp/TrumpTalk)]\n- [Taylor Swift Song Generator](http://taytay.mlavin.org/), which does what it says. [[code](https://github.com/caktus/taytay)]\n- [@BOTtalks](https://twitter.com/bottalks) / [ideasworthautomating.com](http://ideasworthautomating.com/). \"TIM generates talks on a broad spectrum of topics, based on the texts of slightly more coherent talks given under the auspices of his more famous big brother, who shall not be named here.\" [[code](https://github.com/alexislloyd/tedbot)]\n- [Internal Security Zones](http://rebecca-ricks.com/2016/05/06/internal-security-zones/), \"Generative instructions for prison design & maintenance.\" [[code](https://github.com/baricks/internal-security-zones)]\n- [Miraculous Ladybot](http://miraculousladybot.tumblr.com/). Generates [Miraculous Ladybug](https://en.wikipedia.org/wiki/Miraculous:_Tales_of_Ladybug_%26_Cat_Noir) fanfictions and posts them on Tumblr. [[code](https://github.com/veggiedefender/miraculousladybot)]\n- [@HaikuBotto](https://twitter.com/HaikuBotto), \"I'm a bot that writes haiku from literature. beep boop\" [[code](https://github.com/balysv/HaikuBotto)]\n- [Chat Simulator Bot](http://www.telegram.me/ChatSimulatorBot), a bot for Telegram. [[code](https://github.com/GuyAglionby/chatsimulatorbot)]\n- [emojipasta.club](http://emojipasta.club), \"a web service that exposes RESTful endpoints for generating emojipastas, as well as a simple frontend for generating and tweeting emojipasta sentences.\" [[code](https://github.com/ntratcliff/emojipasta.club)]\n- [Towel Generator](http://towel.labs.wasv.me/), \"A system for generating sentences similar to those from the hitchhikers series of books.\" [[code](https://github.com/wastevensv/towelday)]\n- [@mercurialbot](https://twitter.com/mercurialbot), \"A twitter bot that generates tweets based on its mood.\" [[code](https://github.com/brahmcapoor/Mercury)]\n- [becomeacurator.com](http://becomeacurator.com/), which \"generates curatorial statements for contemporary art expositions, using Markov chains and texts from galleries around the world.\" [[code](https://github.com/jjcastro/markov-curatorial-generator)]\n- [mannynotfound/interview-bot](https://github.com/mannynotfound/interview-bot), \"A python based terminal prompt app to automate the interview process.\"\n- [Steam Game Generator](http://applepinegames.com/tech/steam-game-generator), which \"uses data from real Steam games, randomized using Markov chains.\" [[code](https://github.com/applepinegames/steam_game_generator)]\n- [@DicedOnionBot](https://twitter.com/DicedOnionBot), which \"generates new headlines by The Onion by regurgitating and combining old headlines.\" [[code](https://github.com/mobeets/fake-onion)]\n- [@thought__leader](https://twitter.com/thought__leader), \"Thinking thoughts so you don't have to!\" [[blog post](http://jordan-wright.com/blog/post/2016-04-08-i-automated-infosec-thought-leadership/)]\n- [@_murakamibot](https://twitter.com/_murakamibot) and [@jamesjoycebot](https://twitter.com/jamesjoycebot), bots that tweet Haruki Murakami and James Joyce-like sentences. [[code](https://github.com/tmkuba/markovBot)]\n- [shartificialintelligence.com](http://www.shartificialintelligence.com/), \"the world's first creative ad agency staffed entirely with copywriter robots.\" [[code](https://github.com/LesGuessing/shartificial-intelligence)]\n- [@NightValeFeed](https://twitter.com/NightValeFeed), which \"generates tweets by combining [@NightValeRadio](https://twitter.com/NightValeRadio) tweets with [@BuzzFeed](https://twitter.com/BuzzFeed) headlines.\" [[code](https://github.com/stepjue/night-vale-buzzfeed)]\n- [Wynbot9000](https://github.com/ammgws/wynbot), which \"mimics your friends on Google Hangouts.\" [[code](https://github.com/ammgws/wynbot)]\n- [@sealDonaldTrump](https://twitter.com/sealdonaldtrump), \"a twitter bot that sounds like @realDonaldTrump, with an aquatic twist.\" [[code](https://github.com/lukewrites/sealdonaldtrump)]\n- [@veeceebot](https://twitter.com/veeceebot), which is \"like VCs but better!\" [[code](https://github.com/yasyf/vcbot)]\n- [@mar_phil_bot](https://twitter.com/mar_phil_bot), a Twitter bot [trained](http://gfleetwood.github.io/philosophy-bot/) on Nietzsche, Russell, Kant, Machiavelli, and Plato. [[code](https://gist.github.com/gfleetwood/569804c4f2ab372746661996542a8065)]\n- [funzo-facts](https://github.com/smalawi/funzo-facts), a program that generates never-before-seen trivia based on Jeopardy! questions. [[code](https://github.com/smalawi/funzo-facts/blob/master/funzo_fact_gen.py)]\n- [Chains Invent Insanity](http://chainsinventinsanity.com), a [Cards Against Humanity](https://cardsagainsthumanity.com) answer card generator. [[code](https://github.com/TuxOtaku/chains-invent-insanity)]\n- [@CanDennisDream](https://twitter.com/CanDennisDream), a twitter bot that contemplates life by training on existential literature discussions. [[code](https://github.com/GiantsLoveDeathMetal/dennis_bot)]\n- [B-9 Indifference](https://github.com/eoinnoble/b9-indifference), a program that generates a _Star Trek: The Next Generation_ script of arbitrary length using Markov chains trained on the show\u2019s episode and movie scripts. [[code](https://github.com/eoinnoble/b9-indifference)]\n- [adam](http://bziarkowski.pl/adam), polish poetry generator. [[code](https://github.com/bziarkowski/adam)]\n- [Stackexchange Simulator](https://se-simulator.lw1.at/), which uses StackExchange's bulk data to generate random questions and answers. [[code](https://github.com/Findus23/se-simulator)]\n- [@BloggingBot](https://twitter.com/BloggingBot), tweets sentences based on a corpus of 17 years of [blogging](http://artlung.com/blog/2018/02/23/markov-chains-are-hilarious/).\n- [Commencement Speech Generator](https://github.com/whatrocks/markov-commencement-speech), generates \"graduation speech\"-style quotes from a dataset of the \"greatest of all time\" commencement speeches)\n- [@alg_testament](https://twitter.com/alg_testament), tweets sentences based on The Old Testament and two coding textbooks in Russian. [[code](https://github.com/maryszmary/Algorithm-Testament)]  \n- [@IRAMockBot](https://twitter.com/IRAMockBot), uses Twitter's data on tweets from Russian IRA-associated accounts to produce fake IRA tweets, for educational and study purposes.[[code](https://github.com/nwithan8/IRAMockBot)]\n- [Personal Whatsapp Chat Analyzer](https://github.com/Giuzzilla/Personal-Whatsapp-Chat-Analyzer), some basic analytics for WhatsApp chat exports (private & groups), word counting & markov chain phrase generator \n- [DeepfakeBot](https://deepfake-bot.readthedocs.io/) - A system for converting your friends into Discord bots. [[code](https://github.com/rustygentile/deepfake-bot)]\n- [python-markov-novel](https://github.com/accraze/python-markov-novel), writes a random novel using markov chains, broken down into chapters\n- [python-ia-markov](https://github.com/accraze/python-ia-markov), trains Markov models on Internet Archive text files\n- [@bot_homer](https://twitter.com/bot_homer), a Twitter bot trained using Homer Simpson's dialogues of 600 chapters. [[code](https://github.com/ivanlen/simpsons_bot)].\n\nHave other examples? Pull requests welcome.\n\n## Thanks\n\nMany thanks to the following GitHub users for contributing code and/or ideas:\n\n- [@orf](https://github.com/orf)\n- [@deimos](https://github.com/deimos)\n- [@cjmochrie](https://github.com/cjmochrie)\n- [@Jaza](https://github.com/Jaza)\n- [@fitnr](https://github.com/fitnr)\n- [@andela-mfalade](https://github.com/andela-mfalade)\n- [@ntratcliff](https://github.com/ntratcliff)\n- [@schollz](https://github.com/schollz)\n- [@aalireza](https://github.com/aalireza)\n- [@bfontaine](https://github.com/bfontaine)\n- [@tmsherman](https://github.com/tmsherman)\n- [@wodim](https://github.com/wodim)\n- [@eh11fx](https://github.com/eh11fx)\n- [@ammgws](https://github.com/ammgws)\n- [@OtakuMegane](https://github.com/OtakuMegane)\n- [@tsunaminoai](https://github.com/tsunaminoai)\n- [@MatthewScholefield](https://github.com/MatthewScholefield)\n- [@danmayer](https://github.com/danmayer)\n- [@kade-robertson](https://github.com/kade-robertson)\n- [@erikerlandson](https://github.com/erikerlandson)\n\nInitially developed at [BuzzFeed](https://www.buzzfeed.com).\n"}, "mlflow": {"file_name": "mlflow/mlflow/README.rst", "raw_text": "=============================================\nMLflow: A Machine Learning Lifecycle Platform\n=============================================\n\nMLflow is a platform to streamline machine learning development, including tracking experiments, packaging code\ninto reproducible runs, and sharing and deploying models. MLflow offers a set of lightweight APIs that can be\nused with any existing machine learning application or library (TensorFlow, PyTorch, XGBoost, etc), wherever you\ncurrently run ML code (e.g. in notebooks, standalone applications or the cloud). MLflow's current components are:\n\n* `MLflow Tracking <https://mlflow.org/docs/latest/tracking.html>`_: An API to log parameters, code, and\n  results in machine learning experiments and compare them using an interactive UI.\n* `MLflow Projects <https://mlflow.org/docs/latest/projects.html>`_: A code packaging format for reproducible\n  runs using Conda and Docker, so you can share your ML code with others.\n* `MLflow Models <https://mlflow.org/docs/latest/models.html>`_: A model packaging format and tools that let\n  you easily deploy the same model (from any ML library) to batch and real-time scoring on platforms such as\n  Docker, Apache Spark, Azure ML and AWS SageMaker.\n* `MLflow Model Registry <https://mlflow.org/docs/latest/model-registry.html>`_: A centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of MLflow Models.\n\n|docs| |travis| |pypi| |conda-forge| |cran| |maven| |license| |downloads|\n\n.. |docs| image:: https://img.shields.io/badge/docs-latest-success.svg\n    :target: https://mlflow.org/docs/latest/index.html\n    :alt: Latest Docs\n.. |travis| image:: https://img.shields.io/travis/mlflow/mlflow.svg\n    :target: https://travis-ci.org/mlflow/mlflow\n    :alt: Build Status\n.. |pypi| image:: https://img.shields.io/pypi/v/mlflow.svg\n    :target: https://pypi.org/project/mlflow/\n    :alt: Latest Python Release\n.. |conda-forge| image:: https://img.shields.io/conda/vn/conda-forge/mlflow.svg\n    :target: https://anaconda.org/conda-forge/mlflow\n    :alt: Latest Conda Release\n.. |cran| image:: https://img.shields.io/cran/v/mlflow.svg\n    :target: https://cran.r-project.org/package=mlflow\n    :alt: Latest CRAN Release\n.. |maven| image:: https://img.shields.io/maven-central/v/org.mlflow/mlflow-parent.svg\n    :target: https://mvnrepository.com/artifact/org.mlflow\n    :alt: Maven Central\n.. |license| image:: https://img.shields.io/badge/license-Apache%202-brightgreen.svg\n    :target: https://github.com/mlflow/mlflow/blob/master/LICENSE.txt\n    :alt: Apache 2 License\n.. |downloads| image:: https://pepy.tech/badge/mlflow\n    :target: https://pepy.tech/project/mlflow\n    :alt: Total Downloads\n\nInstalling\n----------\nInstall MLflow from PyPI via ``pip install mlflow``\n\nMLflow requires ``conda`` to be on the ``PATH`` for the projects feature.\n\nNightly snapshots of MLflow master are also available `here <https://mlflow-snapshots.s3-us-west-2.amazonaws.com/>`_.\n\nDocumentation\n-------------\nOfficial documentation for MLflow can be found at https://mlflow.org/docs/latest/index.html.\n\nCommunity\n---------\nFor help or questions about MLflow usage (e.g. \"how do I do X?\") see the `docs <https://mlflow.org/docs/latest/index.html>`_\nor `Stack Overflow <https://stackoverflow.com/questions/tagged/mlflow>`_.\n\nTo report a bug, file a documentation issue, or submit a feature request, please open a GitHub issue.\n\nFor release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\nor join us on Slack at https://tinyurl.com/mlflow-slack.\n\nRunning a Sample App With the Tracking API\n------------------------------------------\nThe programs in ``examples`` use the MLflow Tracking API. For instance, run::\n\n    python examples/quickstart/mlflow_tracking.py\n\nThis program will use `MLflow Tracking API <https://mlflow.org/docs/latest/tracking.html>`_,\nwhich logs tracking data in ``./mlruns``. This can then be viewed with the Tracking UI.\n\n\nLaunching the Tracking UI\n-------------------------\nThe MLflow Tracking UI will show runs logged in ``./mlruns`` at `<http://localhost:5000>`_.\nStart it with::\n\n    mlflow ui\n\n**Note:** Running ``mlflow ui`` from within a clone of MLflow is not recommended - doing so will\nrun the dev UI from source. We recommend running the UI from a different working directory,\nspecifying a backend store via the ``--backend-store-uri`` option. Alternatively, see\ninstructions for running the dev UI in the `contributor guide <CONTRIBUTING.rst>`_.\n\n\nRunning a Project from a URI\n----------------------------\nThe ``mlflow run`` command lets you run a project packaged with a MLproject file from a local path\nor a Git URI::\n\n    mlflow run examples/sklearn_elasticnet_wine -P alpha=0.4\n\n    mlflow run https://github.com/mlflow/mlflow-example.git -P alpha=0.4\n\nSee ``examples/sklearn_elasticnet_wine`` for a sample project with an MLproject file.\n\n\nSaving and Serving Models\n-------------------------\nTo illustrate managing models, the ``mlflow.sklearn`` package can log scikit-learn models as\nMLflow artifacts and then load them again for serving. There is an example training application in\n``examples/sklearn_logistic_regression/train.py`` that you can run as follows::\n\n    $ python examples/sklearn_logistic_regression/train.py\n    Score: 0.666\n    Model saved in run <run-id>\n\n    $ mlflow models serve --model-uri runs:/<run-id>/model\n\n    $ curl -d '{\"columns\":[0],\"index\":[0,1],\"data\":[[1],[-1]]}' -H 'Content-Type: application/json'  localhost:5000/invocations\n\n\nContributing\n------------\nWe happily welcome contributions to MLflow. Please see our `contribution guide <CONTRIBUTING.rst>`_\nfor details.\n"}, "mxnet": {"file_name": "apache/incubator-mxnet/README.md", "raw_text": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n<div align=\"center\">\n  <a href=\"https://mxnet.apache.org/\"><img src=\"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet_logo_2.png\"></a><br>\n</div>\n\nApache MXNet (incubating) for Deep Learning\n=====\n| Master         | Docs          | License  |\n| :-------------:|:-------------:|:--------:|\n[![CentOS CPU Build Status](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/centos-cpu/job/master/badge/icon?subject=build%20centos%20cpu)](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/centos-cpu/job/master/) [![CentOS GPU Build Status](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/centos-gpu/job/master/badge/icon?subject=build%20centos%20gpu)](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/centos-gpu/job/master/) [![Clang Build Status](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/clang/job/master/badge/icon?subject=build%20clang)](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/clang/job/master/) <br> [![Edge Build Status](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/edge/job/master/badge/icon?subject=build%20edge)](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/edge/job/master/) [![Miscellaneous Build Status](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/miscellaneous/job/master/badge/icon?subject=build%20miscellaneous)](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/miscellaneous/job/master/) [![Sanity Build Status](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/sanity/job/master/badge/icon?subject=build%20sanity)](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/sanity/job/master/) <br> [![Unix CPU Build Status](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/unix-cpu/job/master/badge/icon?subject=build%20unix%20cpu)](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/unix-cpu/job/master/) [![Unix GPU Build Status](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/unix-gpu/job/master/badge/icon?subject=build%20unix%20gpu)](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/unix-gpu/job/master/) [![Website Build Status](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/website/job/master/badge/icon?subject=build%20website)](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/website/job/master/) <br> [![Windows CPU Build Status](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/windows-cpu/job/master/badge/icon?subject=build%20windows%20cpu)](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/windows-cpu/job/master/) [![Windows GPU Build Status](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/windows-gpu/job/master/badge/icon?subject=build%20windows%20gpu)](http://jenkins.mxnet-ci.amazon-ml.com/job/mxnet-validation/job/windows-gpu/job/master/) | [![Documentation Status](http://jenkins.mxnet-ci.amazon-ml.com/job/restricted-website-build/badge/icon)](https://mxnet.apache.org/) | [![GitHub license](http://dmlc.github.io/img/apache2.svg)](./LICENSE) |\n\n![banner](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/banner.png)\n\nApache MXNet (incubating) is a deep learning framework designed for both *efficiency* and *flexibility*.\nIt allows you to ***mix*** [symbolic and imperative programming](https://mxnet.apache.org/api/architecture/program_model)\nto ***maximize*** efficiency and productivity.\nAt its core, MXNet contains a dynamic dependency scheduler that automatically parallelizes both symbolic and imperative operations on the fly.\nA graph optimization layer on top of that makes symbolic execution fast and memory efficient.\nMXNet is portable and lightweight, scaling effectively to multiple GPUs and multiple machines.\n\nMXNet is more than a deep learning project. It is a collection of\n[blue prints and guidelines](https://mxnet.apache.org/api/architecture/overview) for building\ndeep learning systems, and interesting insights of DL systems for hackers.\n\nAsk Questions\n-------------\n* Please use [discuss.mxnet.io](https://discuss.mxnet.io/) for asking questions.\n* Please use [mxnet/issues](https://github.com/apache/incubator-mxnet/issues) for reporting bugs.\n* [Frequent Asked Questions](https://mxnet.apache.org/faq/faq.html)\n\nHow to Contribute\n-----------------\n* [Contribute to MXNet](https://mxnet.apache.org/community/contribute.html)\n\nWhat's New\n----------\n* [Version 1.6.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.6.0) - MXNet 1.6.0 Release.\n* [Version 1.5.1 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.5.1) - MXNet 1.5.1 Patch Release.\n* [Version 1.5.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.5.0) - MXNet 1.5.0 Release.\n* [Version 1.4.1 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.4.1) - MXNet 1.4.1 Patch Release.\n* [Version 1.4.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.4.0) - MXNet 1.4.0 Release.\n* [Version 1.3.1 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.3.1) - MXNet 1.3.1 Patch Release.\n* [Version 1.3.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.3.0) - MXNet 1.3.0 Release.\n* [Version 1.2.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.2.0) - MXNet 1.2.0 Release.\n* [Version 1.1.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.1.0) - MXNet 1.1.0 Release.\n* [Version 1.0.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.0.0) - MXNet 1.0.0 Release.\n* [Version 0.12.1 Release](https://github.com/apache/incubator-mxnet/releases/tag/0.12.1) - MXNet 0.12.1 Patch Release.\n* [Version 0.12.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/0.12.0) - MXNet 0.12.0 Release.\n* [Version 0.11.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/0.11.0) - MXNet 0.11.0 Release.\n* [Apache Incubator](http://incubator.apache.org/projects/mxnet.html) - We are now an Apache Incubator project.\n* [Version 0.10.0 Release](https://github.com/dmlc/mxnet/releases/tag/v0.10.0) - MXNet 0.10.0 Release.\n* [Version 0.9.3 Release](./docs/architecture/release_note_0_9.md) - First 0.9 official release.\n* [Version 0.9.1 Release (NNVM refactor)](./docs/architecture/release_note_0_9.md) - NNVM branch is merged into master now. An official release will be made soon.\n* [Version 0.8.0 Release](https://github.com/dmlc/mxnet/releases/tag/v0.8.0)\n* [Updated Image Classification with new Pre-trained Models](./example/image-classification)\n* [Notebooks How to Use MXNet](https://github.com/d2l-ai/d2l-en)\n* [MKLDNN for Faster CPU Performance](docs/python_docs/python/tutorials/performance/backend/mkldnn/mkldnn_readme.md)\n* [MXNet Memory Monger, Training Deeper Nets with Sublinear Memory Cost](https://github.com/dmlc/mxnet-memonger)\n* [Tutorial for NVidia GTC 2016](https://github.com/dmlc/mxnet-gtc-tutorial)\n* [MXNet.js: Javascript Package for Deep Learning in Browser (without server)](https://github.com/dmlc/mxnet.js/)\n* [Guide to Creating New Operators (Layers)](https://mxnet.apache.org/api/faq/new_op)\n* [Go binding for inference](https://github.com/songtianyi/go-mxnet-predictor)\n* [Amalgamation and Go Binding for Predictors](https://github.com/jdeng/gomxnet/) - Outdated\n* [Large Scale Image Classification](https://github.com/apache/incubator-mxnet/tree/master/example/image-classification)\n\nContents\n--------\n* [Website](https://mxnet.apache.org)\n* [Documentation](https://mxnet.apache.org/api)\n* [Blog](https://mxnet.apache.org/blog)\n* [Code Examples](https://github.com/apache/incubator-mxnet/tree/master/example)\n* [Installation](https://mxnet.apache.org/get_started)\n* [Features](https://mxnet.apache.org/features)\n* [Ecosystem](https://mxnet.apache.org/ecosystem)\n\nFeatures\n--------\n* Design notes providing useful insights that can re-used by other DL projects\n* Flexible configuration for arbitrary computation graph\n* Mix and match imperative and symbolic programming to maximize flexibility and efficiency\n* Lightweight, memory efficient and portable to smart devices\n* Scales up to multi GPUs and distributed setting with auto parallelism\n* Support for [Python](https://mxnet.apache.org/api/python), [Scala](https://mxnet.apache.org/api/scala), [C++](https://mxnet.apache.org/api/cpp), [Java](https://mxnet.apache.org/api/java), [Clojure](https://mxnet.apache.org/api/clojure), [R](https://mxnet.apache.org/api/r), [Go](https://github.com/jdeng/gomxnet/), [Javascript](https://github.com/dmlc/mxnet.js/), [Perl](https://mxnet.apache.org/api/perl), [Matlab](https://github.com/apache/incubator-mxnet/tree/master/matlab), and [Julia](https://mxnet.apache.org/api/julia)\n* Cloud-friendly and directly compatible with AWS S3, AWS Deep Learning AMI, AWS SageMaker, HDFS, and Azure\n\nLicense\n-------\nLicensed under an [Apache-2.0](https://github.com/apache/incubator-mxnet/blob/master/LICENSE) license.\n\nReference Paper\n---------------\n\nTianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao,\nBing Xu, Chiyuan Zhang, and Zheng Zhang.\n[MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems](https://github.com/dmlc/web-data/raw/master/mxnet/paper/mxnet-learningsys.pdf).\nIn Neural Information Processing Systems, Workshop on Machine Learning Systems, 2015\n\nHistory\n-------\nMXNet emerged from a collaboration by the authors of [cxxnet](https://github.com/dmlc/cxxnet), [minerva](https://github.com/dmlc/minerva), and [purine2](https://github.com/purine/purine2). The project reflects what we have learned from the past projects. MXNet combines aspects of each of these projects to achieve flexibility, speed, and memory efficiency.\n"}, "netron": {"file_name": "lutzroeder/netron/README.md", "raw_text": "\n<p align='center'><a href='https://github.com/lutzroeder/netron'><img width='400' src='.github/logo.png'/></a></p>\n\nNetron is a viewer for neural network, deep learning and machine learning models. \n\nNetron supports **ONNX** (`.onnx`, `.pb`, `.pbtxt`), **Keras** (`.h5`, `.keras`), **Core ML** (`.mlmodel`), **Caffe** (`.caffemodel`, `.prototxt`), **Caffe2** (`predict_net.pb`, `predict_net.pbtxt`), **Darknet** (`.cfg`), **MXNet** (`.model`, `-symbol.json`), **ncnn** (`.param`) and **TensorFlow Lite** (`.tflite`).\n\nNetron has experimental support for **TorchScript** (`.pt`, `.pth`), **PyTorch** (`.pt`, `.pth`), **Torch** (`.t7`), **Arm NN** (`.armnn`), **BigDL** (`.bigdl`, `.model`), **Chainer** (`.npz`, `.h5`), **CNTK** (`.model`, `.cntk`), **Deeplearning4j** (`.zip`), **MediaPipe** (`.pbtxt`), **ML.NET** (`.zip`), **MNN** (`.mnn`), **OpenVINO** (`.xml`), **PaddlePaddle** (`.zip`, `__model__`), **scikit-learn** (`.pkl`), **Tengine** (`.tmfile`), **TensorFlow.js** (`model.json`, `.pb`) and **TensorFlow** (`.pb`, `.meta`, `.pbtxt`, `.ckpt`, `.index`).\n\n<p align='center'><a href='https://www.lutzroeder.com/ai'><img src='.github/screenshot.png' width='800'></a></p>\n\n## Install\n\n**macOS**: [**Download**](https://github.com/lutzroeder/netron/releases/latest) the `.dmg` file or run `brew cask install netron`\n\n**Linux**: [**Download**](https://github.com/lutzroeder/netron/releases/latest) the `.AppImage` file or run `snap install netron`\n\n**Windows**: [**Download**](https://github.com/lutzroeder/netron/releases/latest) the `.exe` installer.\n\n**Browser**: [**Start**](https://www.lutzroeder.com/ai/netron) the browser version.\n\n**Python Server**: Run `pip install netron` and `netron [FILE]` or `import netron; netron.start('[FILE]')`.\n\n## Models\n\nSample model files to download or open using the browser version:\n\n * **ONNX**: [squeezenet](https://raw.githubusercontent.com/onnx/tutorials/master/tutorials/assets/squeezenet.onnx) [[open](https://lutzroeder.github.io/netron?url=https://raw.githubusercontent.com/onnx/tutorials/master/tutorials/assets/squeezenet.onnx)]\n * **CoreML**: [exermote](https://raw.githubusercontent.com/Lausbert/Exermote/master/ExermoteInference/ExermoteCoreML/ExermoteCoreML/Model/Exermote.mlmodel) [[open](https://lutzroeder.github.io/netron?url=https://raw.githubusercontent.com/Lausbert/Exermote/master/ExermoteInference/ExermoteCoreML/ExermoteCoreML/Model/Exermote.mlmodel)]\n * **Darknet**: [yolo](https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolo.cfg) [[open](https://lutzroeder.github.io/netron?url=https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolo.cfg)]\n * **Keras**: [mobilenet](https://raw.githubusercontent.com/aio-libs/aiohttp-demos/master/demos/imagetagger/tests/data/mobilenet.h5) [[open](https://lutzroeder.github.io/netron?url=https://raw.githubusercontent.com/aio-libs/aiohttp-demos/master/demos/imagetagger/tests/data/mobilenet.h5)]\n * **MXNet**: [inception_v3](https://raw.githubusercontent.com/soeaver/mxnet-model/master/cls/inception/inception_v3-symbol.json) [[open](https://lutzroeder.github.io/netron?url=https://raw.githubusercontent.com/soeaver/mxnet-model/master/cls/inception/inception_v3-symbol.json)]\n * **TensorFlow**: [chessbot](https://raw.githubusercontent.com/srom/chessbot/master/model/chessbot.pb) [[open](https://lutzroeder.github.io/netron?url=https://raw.githubusercontent.com/srom/chessbot/master/model/chessbot.pb)]\n * **TensorFlow Lite**: [hair_segmentation](https://raw.githubusercontent.com/google/mediapipe/master/mediapipe/models/hair_segmentation.tflite) [[open](https://lutzroeder.github.io/netron?url=https://raw.githubusercontent.com/google/mediapipe/master/mediapipe/models/hair_segmentation.tflite)]\n * **TorchScript**: [traced_online_pred_layer](https://raw.githubusercontent.com/ApolloAuto/apollo/master/modules/prediction/data/traced_online_pred_layer.pt) [[open](https://lutzroeder.github.io/netron?url=https://raw.githubusercontent.com/ApolloAuto/apollo/master/modules/prediction/data/traced_online_pred_layer.pt)]\n * **Caffe**: [mobilenet_v2](https://raw.githubusercontent.com/shicai/MobileNet-Caffe/master/mobilenet_v2.caffemodel) [[open](https://lutzroeder.github.io/netron?url=https://raw.githubusercontent.com/shicai/MobileNet-Caffe/master/mobilenet_v2.caffemodel)]\n"}, "networkx": {"file_name": "networkx/networkx/README.rst", "raw_text": "NetworkX\n========\n\n.. image:: https://img.shields.io/pypi/v/networkx.svg\n   :target: https://pypi.org/project/networkx/\n\n.. image:: https://img.shields.io/pypi/pyversions/networkx.svg\n   :target: https://pypi.org/project/networkx/\n\n.. image:: https://travis-ci.org/networkx/networkx.svg?branch=master\n   :target: https://travis-ci.org/networkx/networkx\n\n.. image:: https://ci.appveyor.com/api/projects/status/github/networkx/networkx?branch=master&svg=true\n   :target: https://ci.appveyor.com/project/dschult/networkx-pqott\n\n.. image:: https://codecov.io/gh/networkx/networkx/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/networkx/networkx\n\nNetworkX is a Python package for the creation, manipulation,\nand study of the structure, dynamics, and functions\nof complex networks.\n\n- **Website (including documentation):** https://networkx.github.io\n- **Mailing list:** https://groups.google.com/forum/#!forum/networkx-discuss\n- **Source:** https://github.com/networkx/networkx\n- **Bug reports:** https://github.com/networkx/networkx/issues\n\nSimple example\n--------------\n\nFind the shortest path between two nodes in an undirected graph:\n\n.. code:: python\n\n    >>> import networkx as nx\n    >>> G = nx.Graph()\n    >>> G.add_edge('A', 'B', weight=4)\n    >>> G.add_edge('B', 'D', weight=2)\n    >>> G.add_edge('A', 'C', weight=3)\n    >>> G.add_edge('C', 'D', weight=4)\n    >>> nx.shortest_path(G, 'A', 'D', weight='weight')\n    ['A', 'B', 'D']\n\nInstall\n-------\n\nInstall the latest version of NetworkX::\n\n    $ pip install networkx\n\nInstall with all optional dependencies::\n\n    $ pip install networkx[all]\n\nFor additional details, please see `INSTALL.rst`.\n\nBugs\n----\n\nPlease report any bugs that you find `here <https://github.com/networkx/networkx/issues>`_.\nOr, even better, fork the repository on `GitHub <https://github.com/networkx/networkx>`_\nand create a pull request (PR). We welcome all changes, big or small, and we\nwill help you make the PR if you are new to `git` (just ask on the issue and/or\nsee `CONTRIBUTING.rst`).\n\nLicense\n-------\n\nReleased under the 3-Clause BSD license (see `LICENSE.txt`)::\n\n   Copyright (C) 2004-2020 NetworkX Developers\n   Aric Hagberg <hagberg@lanl.gov>\n   Dan Schult <dschult@colgate.edu>\n   Pieter Swart <swart@lanl.gov>\n"}, "nltk": {"file_name": "nltk/nltk/README.md", "raw_text": "# Natural Language Toolkit (NLTK)\n[![PyPI](https://img.shields.io/pypi/v/nltk.svg)](https://pypi.python.org/pypi/nltk) \n[![Travis](https://travis-ci.org/nltk/nltk.svg?branch=develop)](https://travis-ci.org/nltk/nltk)\n\nNLTK -- the Natural Language Toolkit -- is a suite of open source Python\nmodules, data sets, and tutorials supporting research and development in Natural\nLanguage Processing.\n\nFor documentation, please visit [nltk.org](http://www.nltk.org/).\n\n\n## Contributing\n\nDo you want to contribute to NLTK development? Great! Please read more details\nat [CONTRIBUTING.md](CONTRIBUTING.md).\n\nSee also [How to contribute to NLTK](http://www.nltk.org/contribute.html).\n\n\n## Donate\n\nHave you found the toolkit helpful?  Please support NLTK development by donating\nto the project via PayPal, using the link on the NLTK homepage.\n\n\n## Citing\n\nIf you publish work that uses NLTK, please cite the NLTK book, as follows:\n\n    Bird, Steven, Edward Loper and Ewan Klein (2009).\n    Natural Language Processing with Python.  O'Reilly Media Inc.\n\n\n## Copyright\n\nCopyright (C) 2001-2020 NLTK Project\n\nFor license information, see [LICENSE.txt](LICENSE.txt).\n\n[AUTHORS.md](AUTHORS.md) have a list of everyone contributed to NLTK.\n\n\n### Redistributing\n\n- NLTK source code is distributed under the Apache 2.0 License.\n- NLTK documentation is distributed under the Creative Commons\n  Attribution-Noncommercial-No Derivative Works 3.0 United States license.\n- NLTK corpora are provided under the terms given in the README file for each\n  corpus; all are redistributable and available for non-commercial use.\n- NLTK may be freely redistributed, subject to the provisions of these licenses.\n"}, "nni": {"file_name": "microsoft/nni/README.md", "raw_text": "<p align=\"center\">\n<img src=\"docs/img/nni_logo.png\" width=\"300\"/>\n</p>\n\n-----------\n\n[![MIT licensed](https://img.shields.io/badge/license-MIT-brightgreen.svg)](LICENSE)\n[![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration-test-local?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=17&branchName=master)\n[![Issues](https://img.shields.io/github/issues-raw/Microsoft/nni.svg)](https://github.com/Microsoft/nni/issues?q=is%3Aissue+is%3Aopen)\n[![Bugs](https://img.shields.io/github/issues/Microsoft/nni/bug.svg)](https://github.com/Microsoft/nni/issues?q=is%3Aissue+is%3Aopen+label%3Abug)\n[![Pull Requests](https://img.shields.io/github/issues-pr-raw/Microsoft/nni.svg)](https://github.com/Microsoft/nni/pulls?q=is%3Apr+is%3Aopen)\n[![Version](https://img.shields.io/github/release/Microsoft/nni.svg)](https://github.com/Microsoft/nni/releases) [![Join the chat at https://gitter.im/Microsoft/nni](https://badges.gitter.im/Microsoft/nni.svg)](https://gitter.im/Microsoft/nni?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Documentation Status](https://readthedocs.org/projects/nni/badge/?version=latest)](https://nni.readthedocs.io/en/latest/?badge=latest)\n\n[\u7b80\u4f53\u4e2d\u6587](README_zh_CN.md)\n\n**NNI (Neural Network Intelligence)** is a lightweight but powerful toolkit to help users **automate** <a href=\"docs/en_US/FeatureEngineering/Overview.md\">Feature Engineering</a>, <a href=\"docs/en_US/NAS/Overview.md\">Neural Architecture Search</a>, <a href=\"docs/en_US/Tuner/BuiltinTuner.md\">Hyperparameter Tuning</a> and <a href=\"docs/en_US/Compressor/Overview.md\">Model Compression</a>.\n\nThe tool manages automated machine learning (AutoML) experiments, **dispatches and runs** experiments' trial jobs generated by tuning algorithms to search the best neural architecture and/or hyper-parameters in **different training environments** like <a href=\"docs/en_US/TrainingService/LocalMode.md\">Local Machine</a>, <a href=\"docs/en_US/TrainingService/RemoteMachineMode.md\">Remote Servers</a>, <a href=\"docs/en_US/TrainingService/PaiMode.md\">OpenPAI</a>, <a href=\"docs/en_US/TrainingService/KubeflowMode.md\">Kubeflow</a>, <a href=\"docs/en_US/TrainingService/FrameworkControllerMode.md\">FrameworkController on K8S (AKS etc.)</a> and other cloud options.\n\n## **Who should consider using NNI**\n\n* Those who want to **try different AutoML algorithms** in their training code/model.\n* Those who want to run AutoML trial jobs **in different environments** to speed up search.\n* Researchers and data scientists who want to easily **implement and experiment new AutoML algorithms**, may it be: hyperparameter tuning algorithm, neural architect search algorithm or model compression algorithm.\n* ML Platform owners who want to **support AutoML in their platform**.\n\n### **NNI v1.5 has been released! &nbsp;<a href=\"#nni-released-reminder\"><img width=\"48\" src=\"docs/img/release_icon.png\"></a>**\n\n## **NNI capabilities in a glance**\n\nNNI provides CommandLine Tool as well as an user friendly WebUI to manage training experiments. With the extensible API, you can customize your own AutoML algorithms and training services. To make it easy for new users, NNI also provides a set of build-in stat-of-the-art AutoML algorithms and out of box support for popular training platforms.\n\nWithin the following table, we summarized the current NNI capabilities, we are gradually adding new capabilities and we'd love to have your contribution.\n\n<p align=\"center\">\n  <a href=\"#nni-has-been-released\"><img src=\"docs/img/overview.svg\" /></a>\n</p>\n\n<table>\n  <tbody>\n    <tr align=\"center\" valign=\"bottom\">\n    <td>\n      </td>\n      <td>\n        <b>Frameworks & Libraries</b>\n        <img src=\"docs/img/bar.png\"/>\n      </td>\n      <td>\n        <b>Algorithms</b>\n        <img src=\"docs/img/bar.png\"/>\n      </td>\n      <td>\n        <b>Training Services</b>\n        <img src=\"docs/img/bar.png\"/>\n      </td>\n    </tr>\n    </tr>\n    <tr valign=\"top\">\n    <td align=\"center\" valign=\"middle\">\n    <b>Built-in</b>\n      </td>\n      <td>\n      <ul><li><b>Supported Frameworks</b></li>\n        <ul>\n          <li>PyTorch</li>\n          <li>Keras</li>\n          <li>TensorFlow</li>\n          <li>MXNet</li>\n          <li>Caffe2</li>\n          <a href=\"docs/en_US/SupportedFramework_Library.md\">More...</a><br/>\n        </ul>\n        </ul>\n      <ul>\n        <li><b>Supported Libraries</b></li>\n          <ul>\n           <li>Scikit-learn</li>\n           <li>XGBoost</li>\n           <li>LightGBM</li>\n           <a href=\"docs/en_US/SupportedFramework_Library.md\">More...</a><br/>\n          </ul>\n      </ul>\n        <ul>\n        <li><b>Examples</b></li>\n         <ul>\n           <li><a href=\"examples/trials/mnist-pytorch\">MNIST-pytorch</li></a>\n           <li><a href=\"examples/trials/mnist-tfv1\">MNIST-tensorflow</li></a>\n           <li><a href=\"examples/trials/mnist-keras\">MNIST-keras</li></a>\n           <li><a href=\"docs/en_US/TrialExample/GbdtExample.md\">Auto-gbdt</a></li>\n           <li><a href=\"docs/en_US/TrialExample/Cifar10Examples.md\">Cifar10-pytorch</li></a>\n           <li><a href=\"docs/en_US/TrialExample/SklearnExamples.md\">Scikit-learn</a></li>\n           <li><a href=\"docs/en_US/TrialExample/EfficientNet.md\">EfficientNet</a></li>\n              <a href=\"docs/en_US/SupportedFramework_Library.md\">More...</a><br/>\n          </ul>\n        </ul>\n      </td>\n      <td align=\"left\" >\n        <a href=\"docs/en_US/Tuner/BuiltinTuner.md\">Hyperparameter Tuning</a>\n        <ul>\n          <b>Exhaustive search</b>\n          <ul>\n            <li><a href=\"docs/en_US/Tuner/BuiltinTuner.md#Random\">Random Search</a></li>\n            <li><a href=\"docs/en_US/Tuner/BuiltinTuner.md#GridSearch\">Grid Search</a></li>\n            <li><a href=\"docs/en_US/Tuner/BuiltinTuner.md#Batch\">Batch</a></li>\n            </ul>\n          <b>Heuristic search</b>\n          <ul>\n            <li><a href=\"docs/en_US/Tuner/BuiltinTuner.md#Evolution\">Na\u00efve Evolution</a></li>\n            <li><a href=\"docs/en_US/Tuner/BuiltinTuner.md#Anneal\">Anneal</a></li>\n            <li><a href=\"docs/en_US/Tuner/BuiltinTuner.md#Hyperband\">Hyperband</a></li>\n            <li><a href=\"docs/en_US/Tuner/BuiltinTuner.md#PBTTuner\">PBT</a></li>\n          </ul>\n          <b>Bayesian optimization</b>\n            <ul>\n              <li><a href=\"docs/en_US/Tuner/BuiltinTuner.md#BOHB\">BOHB</a></li>\n              <li><a href=\"docs/en_US/Tuner/BuiltinTuner.md#TPE\">TPE</a></li>\n            <li><a href=\"docs/en_US/Tuner/BuiltinTuner.md#SMAC\">SMAC</a></li>\n            <li><a href=\"docs/en_US/Tuner/BuiltinTuner.md#MetisTuner\">Metis Tuner</a></li>\n            <li><a href=\"docs/en_US/Tuner/BuiltinTuner.md#GPTuner\">GP Tuner</a></li>\n            </ul>\n          <b>RL Based</b>\n          <ul>\n            <li><a href=\"docs/en_US/Tuner/BuiltinTuner.md#PPOTuner\">PPO Tuner</a> </li>\n          </ul>\n        </ul>\n          <a href=\"docs/en_US/NAS/Overview.md\">Neural Architecture Search</a>\n          <ul>\n            <ul>\n              <li><a href=\"docs/en_US/NAS/ENAS.md\">ENAS</a></li>\n              <li><a href=\"docs/en_US/NAS/DARTS.md\">DARTS</a></li>\n              <li><a href=\"docs/en_US/NAS/PDARTS.md\">P-DARTS</a></li>\n              <li><a href=\"docs/en_US/NAS/CDARTS.md\">CDARTS</a></li>\n              <li><a href=\"docs/en_US/NAS/SPOS.md\">SPOS</a></li>\n              <li><a href=\"docs/en_US/NAS/Proxylessnas.md\">ProxylessNAS</a></li>\n              <li><a href=\"docs/en_US/Tuner/BuiltinTuner.md#NetworkMorphism\">Network Morphism</a></li>\n              <li><a href=\"docs/en_US/NAS/TextNAS.md\">TextNAS</a></li>\n            </ul>\n          </ul>\n          <a href=\"docs/en_US/Compressor/Overview.md\">Model Compression</a>\n          <ul>\n            <b>Pruning</b>\n            <ul>\n              <li><a href=\"docs/en_US/Compressor/Pruner.md#agp-pruner\">AGP Pruner</a></li>\n              <li><a href=\"docs/en_US/Compressor/Pruner.md#slim-pruner\">Slim Pruner</a></li>\n              <li><a href=\"docs/en_US/Compressor/Pruner.md#fpgm-pruner\">FPGM Pruner</a></li>\n            </ul>\n            <b>Quantization</b>\n            <ul>\n              <li><a href=\"docs/en_US/Compressor/Quantizer.md#qat-quantizer\">QAT Quantizer</a></li>\n              <li><a href=\"docs/en_US/Compressor/Quantizer.md#dorefa-quantizer\">DoReFa Quantizer</a></li>\n            </ul>\n          </ul>\n          <a href=\"docs/en_US/FeatureEngineering/Overview.md\">Feature Engineering (Beta)</a>\n          <ul>\n          <li><a href=\"docs/en_US/FeatureEngineering/GradientFeatureSelector.md\">GradientFeatureSelector</a></li>\n          <li><a href=\"docs/en_US/FeatureEngineering/GBDTSelector.md\">GBDTSelector</a></li>\n          </ul>\n          <a href=\"docs/en_US/Assessor/BuiltinAssessor.md\">Early Stop Algorithms</a>\n          <ul>\n          <li><a href=\"docs/en_US/Assessor/BuiltinAssessor.md#Medianstop\">Median Stop</a></li>\n          <li><a href=\"docs/en_US/Assessor/BuiltinAssessor.md#Curvefitting\">Curve Fitting</a></li>\n          </ul>\n      </td>\n      <td>\n      <ul>\n        <li><a href=\"docs/en_US/TrainingService/LocalMode.md\">Local Machine</a></li>\n        <li><a href=\"docs/en_US/TrainingService/RemoteMachineMode.md\">Remote Servers</a></li>\n        <li><b>Kubernetes based services</b></li>\n            <ul><li><a href=\"docs/en_US/TrainingService/PaiMode.md\">OpenPAI</a></li>\n            <li><a href=\"docs/en_US/TrainingService/KubeflowMode.md\">Kubeflow</a></li>\n            <li><a href=\"docs/en_US/TrainingService/FrameworkControllerMode.md\">FrameworkController on K8S (AKS etc.)</a></li>\n            </ul>\n      </ul>\n      </td>\n    </tr>\n      <tr align=\"center\" valign=\"bottom\">\n      </td>\n      </tr>\n      <tr valign=\"top\">\n       <td valign=\"middle\">\n    <b>References</b>\n      </td>\n     <td style=\"border-top:#FF0000 solid 0px;\">\n      <ul>\n        <li><a href=\"https://nni.readthedocs.io/en/latest/autotune_ref.html#trial\">Python API</a></li>\n        <li><a href=\"docs/en_US/Tutorial/AnnotationSpec.md\">NNI Annotation</a></li>\n         <li><a href=\"https://nni.readthedocs.io/en/latest/installation.html\">Supported OS</a></li>\n      </ul>\n      </td>\n       <td style=\"border-top:#FF0000 solid 0px;\">\n      <ul>\n        <li><a href=\"docs/en_US/Tuner/CustomizeTuner.md\">CustomizeTuner</a></li>\n        <li><a href=\"docs/en_US/Assessor/CustomizeAssessor.md\">CustomizeAssessor</a></li>\n      </ul>\n      </td>\n        <td style=\"border-top:#FF0000 solid 0px;\">\n      <ul>\n        <li><a href=\"docs/en_US/TrainingService/SupportTrainingService.md\">Support TrainingService</li>\n        <li><a href=\"docs/en_US/TrainingService/HowToImplementTrainingService.md\">Implement TrainingService</a></li>\n      </ul>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n## **Installation**\n\n### **Install**\n\nNNI supports and is tested on Ubuntu >= 16.04, macOS >= 10.14.1, and Windows 10 >= 1809. Simply run the following `pip install` in an environment that has `python 64-bit >= 3.5`.\n\nLinux or macOS\n\n```bash\npython3 -m pip install --upgrade nni\n```\n\nWindows\n\n```bash\npython -m pip install --upgrade nni\n```\n\nIf you want to try latest code, please [install NNI](https://nni.readthedocs.io/en/latest/installation.html) from source code.\n\nFor detail system requirements of NNI, please refer to [here](https://nni.readthedocs.io/en/latest/Tutorial/InstallationLinux.html#system-requirements) for Linux & macOS, and [here](https://nni.readthedocs.io/en/latest/Tutorial/InstallationWin.html#system-requirements) for Windows.\n\nNote:\n\n* If there is any privilege issue, add `--user` to install NNI in the user directory.\n* Currently NNI on Windows supports local, remote and pai mode. Anaconda or Miniconda is highly recommended to install NNI on Windows.\n* If there is any error like `Segmentation fault`, please refer to [FAQ](docs/en_US/Tutorial/FAQ.md). For FAQ on Windows, please refer to [NNI on Windows](docs/en_US/Tutorial/InstallationWin.md#faq).\n\n### **Verify installation**\n\nThe following example is built on TensorFlow 1.x. Make sure **TensorFlow 1.x is used** when running it.\n\n* Download the examples via clone the source code.\n\n  ```bash\n  git clone -b v1.5 https://github.com/Microsoft/nni.git\n  ```\n\n* Run the MNIST example.\n\n  Linux or macOS\n\n  ```bash\n  nnictl create --config nni/examples/trials/mnist-tfv1/config.yml\n  ```\n\n  Windows\n\n  ```bash\n  nnictl create --config nni\\examples\\trials\\mnist-tfv1\\config_windows.yml\n  ```\n\n* Wait for the message `INFO: Successfully started experiment!` in the command line. This message indicates that your experiment has been successfully started. You can explore the experiment using the `Web UI url`.\n\n```text\nINFO: Starting restful server...\nINFO: Successfully started Restful server!\nINFO: Setting local config...\nINFO: Successfully set local config!\nINFO: Starting experiment...\nINFO: Successfully started experiment!\n-----------------------------------------------------------------------\nThe experiment id is egchD4qy\nThe Web UI urls are: http://223.255.255.1:8080   http://127.0.0.1:8080\n-----------------------------------------------------------------------\n\nYou can use these commands to get more information about the experiment\n-----------------------------------------------------------------------\n         commands                       description\n1. nnictl experiment show        show the information of experiments\n2. nnictl trial ls               list all of trial jobs\n3. nnictl top                    monitor the status of running experiments\n4. nnictl log stderr             show stderr log content\n5. nnictl log stdout             show stdout log content\n6. nnictl stop                   stop an experiment\n7. nnictl trial kill             kill a trial job by id\n8. nnictl --help                 get help information about nnictl\n-----------------------------------------------------------------------\n```\n\n* Open the `Web UI url` in your browser, you can view detail information of the experiment and all the submitted trial jobs as shown below. [Here](docs/en_US/Tutorial/WebUI.md) are more Web UI pages.\n\n<table style=\"border: none\">\n    <th><img src=\"./docs/img/webui_overview_page.png\" alt=\"drawing\" width=\"395\"/></th>\n    <th><img src=\"./docs/img/webui_trialdetail_page.png\" alt=\"drawing\" width=\"410\"/></th>\n</table>\n\n## **Documentation**\n\n* To learn about what's NNI, read the [NNI Overview](https://nni.readthedocs.io/en/latest/Overview.html).\n* To get yourself familiar with how to use NNI, read the [documentation](https://nni.readthedocs.io/en/latest/index.html).\n* To get started and install NNI on your system, please refer to [Install NNI](https://nni.readthedocs.io/en/latest/installation.html).\n\n## **Contributing**\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the Code of [Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact opencode@microsoft.com with any additional questions or comments.\n\nAfter getting familiar with contribution agreements, you are ready to create your first PR =), follow the NNI developer tutorials to get start:\n\n* We recommend new contributors to start with simple issues: ['good first issue'](https://github.com/Microsoft/nni/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) or ['help-wanted'](https://github.com/microsoft/nni/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22).\n* [NNI developer environment installation tutorial](docs/en_US/Tutorial/SetupNniDeveloperEnvironment.md)\n* [How to debug](docs/en_US/Tutorial/HowToDebug.md)\n* If you have any questions on usage, review [FAQ](https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/FAQ.md) first, if there are no relevant issues and answers to your question, try contact NNI dev team and users in [Gitter](https://gitter.im/Microsoft/nni?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) or [File an issue](https://github.com/microsoft/nni/issues/new/choose) on GitHub.\n* [Customize your own Tuner](docs/en_US/Tuner/CustomizeTuner.md)\n* [Implement customized TrainingService](docs/en_US/TrainingService/HowToImplementTrainingService.md)\n* [Implement a new NAS trainer on NNI](docs/en_US/NAS/Advanced.md)\n* [Customize your own Advisor](docs/en_US/Tuner/CustomizeAdvisor.md)\n\n## **External Repositories and References**\nWith authors' permission, we listed a set of NNI usage examples and relevant articles.\n\n* ### **External Repositories** ###\n   * Run [ENAS](examples/tuners/enas_nni/README.md) with NNI\n   * Run [Neural Network Architecture Search](examples/trials/nas_cifar10/README.md) with NNI\n   * [Automatic Feature Engineering](examples/feature_engineering/auto-feature-engineering/README.md) with NNI\n   * [Hyperparameter Tuning for Matrix Factorization](https://github.com/microsoft/recommenders/blob/master/notebooks/04_model_select_and_optimize/nni_surprise_svd.ipynb) with NNI\n   * [scikit-nni](https://github.com/ksachdeva/scikit-nni) Hyper-parameter search for scikit-learn pipelines using NNI\n* ### **Relevant Articles** ###\n  * [Hyper Parameter Optimization Comparison](docs/en_US/CommunitySharings/HpoComparision.md)\n  * [Neural Architecture Search Comparison](docs/en_US/CommunitySharings/NasComparision.md)\n  * [Parallelizing a Sequential Algorithm TPE](docs/en_US/CommunitySharings/ParallelizingTpeSearch.md)\n  * [Automatically tuning SVD with NNI](docs/en_US/CommunitySharings/RecommendersSvd.md)\n  * [Automatically tuning SPTAG with NNI](docs/en_US/CommunitySharings/SptagAutoTune.md)\n  * [Find thy hyper-parameters for scikit-learn pipelines using Microsoft NNI](https://towardsdatascience.com/find-thy-hyper-parameters-for-scikit-learn-pipelines-using-microsoft-nni-f1015b1224c1)\n  * **Blog (in Chinese)** - [AutoML tools (Advisor, NNI and Google Vizier) comparison](http://gaocegege.com/Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/katib-new#%E6%80%BB%E7%BB%93%E4%B8%8E%E5%88%86%E6%9E%90) by [@gaocegege](https://github.com/gaocegege) - \u603b\u7ed3\u4e0e\u5206\u6790 section of design and implementation of kubeflow/katib\n  * **Blog (in Chinese)** - [A summary of NNI new capabilities in 2019](https://mp.weixin.qq.com/s/7_KRT-rRojQbNuJzkjFMuA) by @squirrelsc\n\n## **Feedback**\n\n* Discuss on the NNI [Gitter](https://gitter.im/Microsoft/nni?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) in NNI.\n* [File an issue](https://github.com/microsoft/nni/issues/new/choose) on GitHub.\n* Ask a question with NNI tags on [Stack Overflow](https://stackoverflow.com/questions/tagged/nni?sort=Newest&edited=true).\n\n## Related Projects\n\nTargeting at openness and advancing state-of-art technology, [Microsoft Research (MSR)](https://www.microsoft.com/en-us/research/group/systems-research-group-asia/) had also released few other open source projects.\n\n* [OpenPAI](https://github.com/Microsoft/pai) : an open source platform that provides complete AI model training and resource management capabilities, it is easy to extend and supports on-premise, cloud and hybrid environments in various scale.\n* [FrameworkController](https://github.com/Microsoft/frameworkcontroller) : an open source general-purpose Kubernetes Pod Controller that orchestrate all kinds of applications on Kubernetes by a single controller.\n* [MMdnn](https://github.com/Microsoft/MMdnn) : A comprehensive, cross-framework solution to convert, visualize and diagnose deep neural network models. The \"MM\" in MMdnn stands for model management and \"dnn\" is an acronym for deep neural network.\n* [SPTAG](https://github.com/Microsoft/SPTAG) : Space Partition Tree And Graph (SPTAG) is an open source library for large scale vector approximate nearest neighbor search scenario.\n\nWe encourage researchers and students leverage these projects to accelerate the AI development and research.\n\n## **License**\n\nThe entire codebase is under [MIT license](LICENSE)\n"}, "notify-run": {"file_name": "notify-run/notify.run/README.md", "raw_text": "[![Build Status](https://travis-ci.org/paulgb/notify.run.svg?branch=master)](https://travis-ci.org/paulgb/notify.run)\n\n<img src=\"https://avatars2.githubusercontent.com/u/53474526?s=200&v=4\" height=\"30\" /> notify.run\n============================================================\n\n**[notify.run](https://notify.run) makes it easy to programmatically send notifications to your own phone or desktop.** It is provided as a free web service that can be used without installation (on both the sending and receiving end). This repository contains the source of the web service, as well as a Python client that provides integration with Jupyter and Keras.\n\n<img src=\"screenshot.png\" width=\"600\" />\n\nThis repository contains the source code for the Python client, website, and server of notify.run.\n\n- **If you are interested in using the public instance of notify.run to send notifications to yourself, you don\u2019t need to download anything from this repo.** Just follow the instructions at [notify.run](https://notify.run).\n- If you are interested in using the Python client to send notifications via notify.run, see [py_client/README.rst](py_client/README.rst).\n- If you are interested in self-hosting your own notify.run server, see [notify-run-server](https://github.com/notify-run/notify-run-server).\n"}, "numba": {"file_name": "numba/numba/README.rst", "raw_text": "*****\nNumba\n*****\n\n.. image:: https://badges.gitter.im/numba/numba.svg\n   :target: https://gitter.im/numba/numba?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge\n   :alt: Gitter\n\nA Just-In-Time Compiler for Numerical Functions in Python\n#########################################################\n\nNumba is an open source, NumPy-aware optimizing compiler for Python sponsored\nby Anaconda, Inc.  It uses the LLVM compiler project to generate machine code\nfrom Python syntax.\n\nNumba can compile a large subset of numerically-focused Python, including many\nNumPy functions.  Additionally, Numba has support for automatic\nparallelization of loops, generation of GPU-accelerated code, and creation of\nufuncs and C callbacks.\n\nFor more information about Numba, see the Numba homepage:\nhttp://numba.pydata.org\n\nSupported Platforms\n===================\n\n* Operating systems and CPU:\n\n  - Linux: x86 (32-bit), x86_64, ppc64le (POWER8 and 9), ARMv7 (32-bit),\n    ARMv8 (64-bit)\n  - Windows: x86, x86_64\n  - macOS: x86_64\n\n* (Optional) Accelerators and GPUs:\n\n  * NVIDIA GPUs (Kepler architecture or later) via CUDA driver on Linux, Windows,\n    macOS (< 10.14)\n  * AMD GPUs via ROCm driver on Linux\n\nDependencies\n============\n\n* Python versions: 3.6-3.8\n* llvmlite 0.32.*\n* NumPy >=1.15 (can build with 1.11 for ABI compatibility)\n\nOptionally:\n\n* Scipy >=1.0.0 (for ``numpy.linalg`` support)\n\n\nInstalling\n==========\n\nThe easiest way to install Numba and get updates is by using the Anaconda\nDistribution: https://www.anaconda.com/download\n\n::\n\n   $ conda install numba\n\nFor more options, see the Installation Guide: http://numba.pydata.org/numba-doc/latest/user/installing.html\n\nDocumentation\n=============\n\nhttp://numba.pydata.org/numba-doc/latest/index.html\n\n\nMailing Lists\n=============\n\nJoin the Numba mailing list numba-users@continuum.io:\nhttps://groups.google.com/a/continuum.io/d/forum/numba-users\n\nSome old archives are at: http://librelist.com/browser/numba/\n\n\nContinuous Integration\n======================\n\n.. image:: https://dev.azure.com/numba/numba/_apis/build/status/numba.numba?branchName=master\n    :target: https://dev.azure.com/numba/numba/_build/latest?definitionId=1?branchName=master\n    :alt: Azure Pipelines\n"}, "numexpr": {"file_name": "pydata/numexpr/README.rst", "raw_text": "======================================================\nNumExpr: Fast numerical expression evaluator for NumPy\n======================================================\n\n:Author: David M. Cooke, Francesc Alted and others\n:Contact: faltet@gmail.com\n:URL: https://github.com/pydata/numexpr\n:Documentation: http://numexpr.readthedocs.io/en/latest/\n:Travis CI: |travis|\n:Appveyor: |appveyor|\n:PyPi: |version|\n:DOI: |doi|\n:readthedocs: |docs|\n\n.. |travis| image:: https://travis-ci.org/pydata/numexpr.png?branch=master\n        :target: https://travis-ci.org/pydata/numexpr\n.. |appveyor| image:: https://ci.appveyor.com/api/projects/status/we2ff01vqlmlb9ip\n        :target: https://ci.appveyor.com/project/robbmcleod/numexpr\n.. |docs| image:: https://readthedocs.org/projects/numexpr/badge/?version=latest\n        :target: http://numexpr.readthedocs.io/en/latest\n.. |doi| image:: https://zenodo.org/badge/doi/10.5281/zenodo.2483274.svg\n        :target:  https://doi.org/10.5281/zenodo.2483274\n.. |version| image:: https://img.shields.io/pypi/v/numexpr.png\n        :target: https://pypi.python.org/pypi/numexpr\n\n\nWhat is NumExpr?\n----------------\n\nNumExpr is a fast numerical expression evaluator for NumPy.  With it,\nexpressions that operate on arrays (like :code:`'3*a+4*b'`) are accelerated\nand use less memory than doing the same calculation in Python.\n\nIn addition, its multi-threaded capabilities can make use of all your\ncores -- which generally results in substantial performance scaling compared\nto NumPy.\n\nLast but not least, numexpr can make use of Intel's VML (Vector Math\nLibrary, normally integrated in its Math Kernel Library, or MKL).\nThis allows further acceleration of transcendent expressions.\n\n\nHow NumExpr achieves high performance\n-------------------------------------\n\nThe main reason why NumExpr achieves better performance than NumPy is\nthat it avoids allocating memory for intermediate results. This\nresults in better cache utilization and reduces memory access in\ngeneral. Due to this, NumExpr works best with large arrays.\n\nNumExpr parses expressions into its own op-codes that are then used by\nan integrated computing virtual machine. The array operands are split\ninto small chunks that easily fit in the cache of the CPU and passed\nto the virtual machine. The virtual machine then applies the\noperations on each chunk. It's worth noting that all temporaries and\nconstants in the expression are also chunked. Chunks are distributed among \nthe available cores of the CPU, resulting in highly parallelized code \nexecution.\n\nThe result is that NumExpr can get the most of your machine computing\ncapabilities for array-wise computations. Common speed-ups with regard\nto NumPy are usually between 0.95x (for very simple expressions like\n:code:`'a + 1'`) and 4x (for relatively complex ones like :code:`'a*b-4.1*a >\n2.5*b'`), although much higher speed-ups can be achieved for some functions \nand complex math operations (up to 15x in some cases).\n\nNumExpr performs best on matrices that are too large to fit in L1 CPU cache. \nIn order to get a better idea on the different speed-ups that can be achieved \non your platform, run the provided benchmarks.\n\n\nUsage\n-----\n\n::\n\n  >>> import numpy as np\n  >>> import numexpr as ne\n\n  >>> a = np.arange(1e6)   # Choose large arrays for better speedups\n  >>> b = np.arange(1e6)\n\n  >>> ne.evaluate(\"a + 1\")   # a simple expression\n  array([  1.00000000e+00,   2.00000000e+00,   3.00000000e+00, ...,\n           9.99998000e+05,   9.99999000e+05,   1.00000000e+06])\n\n  >>> ne.evaluate('a*b-4.1*a > 2.5*b')   # a more complex one\n  array([False, False, False, ...,  True,  True,  True], dtype=bool)\n\n  >>> ne.evaluate(\"sin(a) + arcsinh(a/b)\")   # you can also use functions\n  array([        NaN,  1.72284457,  1.79067101, ...,  1.09567006,\n          0.17523598, -0.09597844])\n\n  >>> s = np.array(['abba', 'abbb', 'abbcdef'])\n  >>> ne.evaluate(\"'abba' == s\")   # string arrays are supported too\n  array([ True, False, False], dtype=bool)\n\n\nDocumentation\n-------------\n\nPlease see the official documentation at `numexpr.readthedocs.io <https://numexpr.readthedocs.io>`_.\nIncluded is a user guide, benchmark results, and the reference API.\n\n\nAuthors\n-------\n\nPlease see `AUTHORS.txt <https://github.com/pydata/numexpr/blob/master/AUTHORS.txt>`_.\n\n\nLicense\n-------\n\nNumExpr is distributed under the `MIT <http://www.opensource.org/licenses/mit-license.php>`_ license.\n\n\n.. Local Variables:\n.. mode: text\n.. coding: utf-8\n.. fill-column: 70\n.. End:\n"}, "numpy": {"file_name": "numpy/numpy/README.md", "raw_text": "# <img alt=\"NumPy\" src=\"https://cdn.rawgit.com/numpy/numpy/master/branding/icons/numpylogo.svg\" height=\"60\">\n\n[![Travis](https://img.shields.io/travis/numpy/numpy/master.svg?label=Travis%20CI)](\n    https://travis-ci.org/numpy/numpy)\n[![Azure](https://dev.azure.com/numpy/numpy/_apis/build/status/azure-pipeline%20numpy.numpy)](\n    https://dev.azure.com/numpy/numpy/_build/latest?definitionId=5)\n[![codecov](https://codecov.io/gh/numpy/numpy/branch/master/graph/badge.svg)](\n    https://codecov.io/gh/numpy/numpy)\n\nNumPy is the fundamental package needed for scientific computing with Python.\n\n- **Website:** https://www.numpy.org\n- **Documentation:** https://docs.scipy.org/\n- **Mailing list:** https://mail.python.org/mailman/listinfo/numpy-discussion\n- **Source code:** https://github.com/numpy/numpy\n- **Contributing:** https://www.numpy.org/devdocs/dev/index.html\n- **Bug reports:** https://github.com/numpy/numpy/issues\n- **Report a security vulnerability:** https://tidelift.com/docs/security\n\nIt provides:\n\n- a powerful N-dimensional array object\n- sophisticated (broadcasting) functions\n- tools for integrating C/C++ and Fortran code\n- useful linear algebra, Fourier transform, and random number capabilities\n\nTesting:\n\n- NumPy versions &ge; 1.15 require `pytest`\n- NumPy versions &lt; 1.15 require `nose`\n\nTests can then be run after installation with:\n\n    python -c 'import numpy; numpy.test()'\n\n\nCall for Contributions\n----------------------\n\nNumPy appreciates help from a wide range of different backgrounds.\nWork such as high level documentation or website improvements are valuable\nand we would like to grow our team with people filling these roles.\nSmall improvements or fixes are always appreciated and issues labeled as easy\nmay be a good starting point.\nIf you are considering larger contributions outside the traditional coding work,\nplease contact us through the mailing list.\n\n\n[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)\n"}, "onnx": {"file_name": "onnx/onnx/README.md", "raw_text": "<p align=\"center\"><img width=\"40%\" src=\"docs/ONNX_logo_main.png\" /></p>\n\n[![Build Status](https://img.shields.io/travis/onnx/onnx/master.svg?label=Linux)](https://travis-ci.org/onnx/onnx)\n[![Build status](https://img.shields.io/appveyor/ci/onnx/onnx/master.svg?label=Windows)](https://ci.appveyor.com/project/onnx/onnx)\n[![Build Status](https://img.shields.io/jenkins/s/http/powerci.osuosl.org/onnx-ppc64le-nightly-build.svg?label=Linux%20ppc64le)](http://powerci.osuosl.org/job/onnx-ppc64le-nightly-build/)\n\n[Open Neural Network Exchange (ONNX)](https://onnx.ai) is an open ecosystem that empowers AI developers\nto choose the right tools as their project evolves. ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard\ndata types. Currently we focus on the capabilities needed for inferencing (scoring).\n\nONNX is [widely supported](http://onnx.ai/supported-tools) and can be found in many frameworks, tools, and hardware. Enabling interoperability between different frameworks and streamlining the path from research to production helps increase the speed of innovation in the AI community. We invite the community to join us and further evolve ONNX.\n\n# Use ONNX\n* [Tutorials for creating ONNX models](https://github.com/onnx/tutorials).\n* [Pre-trained ONNX models](https://github.com/onnx/models)\n\n# Learn about the ONNX spec\n* [Overview](docs/Overview.md)\n* [ONNX intermediate representation spec](docs/IR.md)\n* [Versioning principles of the spec](docs/Versioning.md)\n* [Operators documentation](docs/Operators.md)\n* [Python API Overview](docs/PythonAPIOverview.md)\n\n# Programming utilities for working with ONNX Graphs\n* [Shape and Type Inference](docs/ShapeInference.md)\n* [Graph Optimization](docs/Optimizer.md)\n* [Opset Version Conversion](docs/VersionConverter.md)\n\n# Contribute\nONNX is a [community project](community). We encourage you to join the effort and contribute feedback, ideas, and code. You can participate in the [SIGs](community/sigs.md) and [Working Groups](community/working-groups.md) to shape the future of ONNX.\n\nCheck out our [contribution guide](https://github.com/onnx/onnx/blob/master/docs/CONTRIBUTING.md) to get started.\n\nIf you think some operator should be added to ONNX specification, please read\n[this document](docs/AddNewOp.md).\n\n# Discuss\nWe encourage you to open [Issues](https://github.com/onnx/onnx/issues), or use Gitter for more real-time discussion:\n[![Join the chat at https://gitter.im/onnx/Lobby](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/onnx/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n# Follow Us\nStay up to date with the latest ONNX news. [[Facebook](https://www.facebook.com/onnxai/)] [[Twitter](https://twitter.com/onnxai)]\n\n\n\n\n\n\n# Installation\n\n## Binaries\n\nA binary build of ONNX is available from [Conda](https://conda.io), in [conda-forge](https://conda-forge.org/):\n\n```\nconda install -c conda-forge onnx\n```\n\n## Source\n\n### Linux and MacOS\nYou will need an install of protobuf and numpy to build ONNX.  One easy\nway to get these dependencies is via\n[Anaconda](https://www.anaconda.com/download/):\n\n```\n# Use conda-forge protobuf, as default doesn't come with protoc\nconda install -c conda-forge protobuf numpy\n```\n\nYou can then install ONNX from PyPi (Note: Set environment variable `ONNX_ML=1` for onnx-ml):\n\n```\npip install onnx\n```\n\nYou can also build and install ONNX locally from source code:\n\n```\ngit clone https://github.com/onnx/onnx.git\ncd onnx\ngit submodule update --init --recursive\npython setup.py install\n```\n\nNote: When installing in a non-Anaconda environment, make sure to install the Protobuf compiler before running the pip installation of onnx. For example, on Ubuntu:\n\n```\nsudo apt-get install protobuf-compiler libprotoc-dev\npip install onnx\n```\n\n### Windows\nWhen building on Windows it is highly recommended that you also build protobuf locally as a static library. The version distributed with conda-forge is a DLL and this is a conflict as ONNX expects it to be a static lib.\n\n#### Instructions to build protobuf and ONNX on windows\nStep 1 : Build protobuf locally\n```\ngit clone https://github.com/protocolbuffers/protobuf.git\ncd protobuf\ngit checkout 3.9.x\ncd cmake\n# Explicitly set -Dprotobuf_MSVC_STATIC_RUNTIME=OFF to make sure protobuf does not statically link to runtime library\ncmake -G \"Visual Studio 15 2017 Win64\" -Dprotobuf_MSVC_STATIC_RUNTIME=OFF -Dprotobuf_BUILD_TESTS=OFF -Dprotobuf_BUILD_EXAMPLES=OFF -DCMAKE_INSTALL_PREFIX=<protobuf_install_dir>\nmsbuild protobuf.sln /m /p:Configuration=Release\nmsbuild INSTALL.vcxproj /p:Configuration=Release\n```\n\nStep 2: Build ONNX\n```\n# Get ONNX\ngit clone https://github.com/onnx/onnx.git\ncd onnx\ngit submodule update --init --recursive\n\n# Set environment variables to find protobuf and turn off static linking of ONNX to runtime library.\n# Even better option is to add it to user\\system PATH so this step can be performed only once.\n# For more details check https://docs.microsoft.com/en-us/cpp/build/reference/md-mt-ld-use-run-time-library?view=vs-2017\nset PATH=<protobuf_install_dir>\\bin;%PATH%\nset USE_MSVC_STATIC_RUNTIME=0\n\n# Optional : Set environment variable `ONNX_ML=1` for onnx-ml\n\n# Build ONNX\npython setup.py install\n```\n\nIf you do not want to build protobuf and instead want to use protobuf from conda forge then follow these instructions. \nHowever please note : This method is just added as a convenience for users and there is very limited support from ONNX team when using this method.\n\n#### Instructions to build ONNX on windows in anaconda environment\n\n```\n# Use conda-forge protobuf\nconda install -c conda-forge protobuf=3.9.2 numpy\n\n# Get ONNX\ngit clone https://github.com/onnx/onnx.git\ncd onnx\ngit submodule update --init --recursive\n\n# Set environment variable for ONNX to use protobuf shared lib\nset CMAKE_ARGS=\"-DONNX_USE_PROTOBUF_SHARED_LIBS=ON\"\n\n# Build ONNX\n# Optional : Set environment variable `ONNX_ML=1` for onnx-ml\n\npython setup.py install\n```\n\n## Verify Installation\nAfter installation, run\n\n```\npython -c \"import onnx\"\n```\n\nto verify it works.  Note that this command does not work from\na source checkout directory; in this case you'll see:\n\n```\nModuleNotFoundError: No module named 'onnx.onnx_cpp2py_export'\n```\n\nChange into another directory to fix this error.\n\n# Testing\n\nONNX uses [pytest](https://docs.pytest.org) as test driver. In order to run tests, first you need to install pytest:\n\n```\npip install pytest nbval\n```\n\nAfter installing pytest, do\n\n```\npytest\n```\n\nto run tests.\n\n# Development\n\nCheck out [contributor guide](https://github.com/onnx/onnx/blob/master/docs/CONTRIBUTING.md) for instructions.\n\n# License\n\n[MIT License](LICENSE)\n\n# Code of Conduct\n\n[ONNX Open Source Code of Conduct](https://onnx.ai/codeofconduct.html)\n"}, "opencv-python": {"file_name": "skvark/opencv-python/README.md", "raw_text": "[![Downloads](http://pepy.tech/badge/opencv-python)](http://pepy.tech/project/opencv-python)\n\n## OpenCV on Wheels\n\n**Unofficial** pre-built OpenCV packages for Python.\n\n### Installation and Usage\n\n1. If you have previous/other manually installed (= not installed via ``pip``) version of OpenCV installed (e.g. cv2 module in the root of Python's site-packages), remove it before installation to avoid conflicts.\n2. Select the correct package for your environment:\n\n    There are four different packages and you should **select only one of them**. Do not install multiple different packages in the same environment. There is no plugin architecture: all the packages use the same namespace (`cv2`). If you installed multiple different packages in the same environment, uninstall them all with ``pip uninstall`` and reinstall only one package.\n\n    **a.** Packages for standard desktop environments (Windows, macOS, almost any GNU/Linux distribution)\n\n    - run ``pip install opencv-python`` if you need only main modules\n    - run ``pip install opencv-contrib-python`` if you need both main and contrib modules (check extra modules listing from [OpenCV documentation](https://docs.opencv.org/master/))\n\n    **b.** Packages for server (headless) environments\n\n    These packages do not contain any GUI functionality. They are smaller and suitable for more restricted environments.\n\n    - run ``pip install opencv-python-headless`` if you need only main modules\n    - run ``pip install opencv-contrib-python-headless`` if you need both main and contrib modules (check extra modules listing from [OpenCV documentation](https://docs.opencv.org/master/))\n\n3. Import the package:\n\n    ``import cv2``\n\n    All packages contain haarcascade files. ``cv2.data.haarcascades`` can be used as a shortcut to the data folder. For example:\n\n    ``cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")``\n\n5. Read [OpenCV documentation](https://docs.opencv.org/master/)\n\n6. Before opening a new issue, read the FAQ below and have a look at the other issues which are already open.\n\nFrequently Asked Questions\n--------------------------\n\n**Q: Do I need to install also OpenCV separately?**\n\nA: No, the packages are special wheel binary packages and they already contain statically built OpenCV binaries.\n\n**Q: Pip fails with ``Could not find a version that satisfies the requirement ...``?**\n\nA: Most likely the issue is related to too old pip and can be fixed by running ``pip install --upgrade pip``. Note that the wheel (especially manylinux) format does not currently support properly ARM architecture so there are no packages for ARM based platforms in PyPI. However, ``opencv-python`` packages for Raspberry Pi can be found from https://www.piwheels.org/.\n\n**Q: Import fails on Windows: ``ImportError: DLL load failed: The specified module could not be found.``?**\n\nA: If the import fails on Windows, make sure you have [Visual C++ redistributable 2015](https://www.microsoft.com/en-us/download/details.aspx?id=48145) installed. If you are using older Windows version than Windows 10 and latest system updates are not installed, [Universal C Runtime](https://support.microsoft.com/en-us/help/2999226/update-for-universal-c-runtime-in-windows) might be also required.\n\nWindows N and KN editions do not include Media Feature Pack which is required by OpenCV. If you are using Windows N or KN edition, please install also [Windows Media Feature Pack](https://support.microsoft.com/en-us/help/3145500/media-feature-pack-list-for-windows-n-editions).\n\nIf you have Windows Server 2012+, media DLLs are probably missing too; please install the Feature called \"Media Foundation\" in the Server Manager. Beware, some posts advise to install \"Windows Server Essentials Media Pack\", but this one requires the \"Windows Server Essentials Experience\" role, and this role will deeply affect your Windows Server configuration (by enforcing active directory integration etc.); so just installing the \"Media Foundation\" should be a safer choice.\n\nIf the above does not help, check if you are using Anaconda. Old Anaconda versions have a bug which causes the error, see [this issue](https://github.com/skvark/opencv-python/issues/36) for a manual fix.\n\nIf you still encounter the error after you have checked all the previous solutions, download [Dependencies](https://github.com/lucasg/Dependencies) and open the ``cv2.pyd`` (located usually at ``C:\\Users\\username\\AppData\\Local\\Programs\\Python\\PythonXX\\Lib\\site-packages\\cv2``) file with it to debug missing DLL issues.\n\n**Q: I have some other import errors?**\n\nA: Make sure you have removed old manual installations of OpenCV Python bindings (cv2.so or cv2.pyd in site-packages).\n\n**Q: Why the packages do not include non-free algorithms?**\n\nA: Non-free algorithms such as SURF are not included in these packages because they are patented / non-free and therefore cannot be distributed as built binaries. Note that SIFT is included in the builds due to patent expiration since OpenCV versions 4.3.0 and 3.4.10. See this issue for more info: https://github.com/skvark/opencv-python/issues/126\n\n**Q: Why the package and import are different (opencv-python vs. cv2)?**\n\nA: It's easier for users to understand ``opencv-python`` than ``cv2`` and it makes it easier to find the package with search engines. `cv2` (old interface in old OpenCV versions was named as `cv`) is the name that OpenCV developers chose when they created the binding generators. This is kept as the import name to be consistent with different kind of tutorials around the internet. Changing the import name or behaviour would be also confusing to experienced users who are accustomed to the ``import cv2``.\n\n## Documentation for opencv-python\n\n[![AppVeyor CI test status (Windows)](https://img.shields.io/appveyor/ci/skvark/opencv-python.svg?maxAge=3600&label=Windows)](https://ci.appveyor.com/project/skvark/opencv-python)\n[![Travis CI test status (Linux and OS X)](https://img.shields.io/travis/skvark/opencv-python.svg?maxAge=3600&label=Linux+macOS)](https://travis-ci.org/skvark/opencv-python)\n\nThe aim of this repository is to provide means to package each new [OpenCV release](https://github.com/opencv/opencv/releases) for the most used Python versions and platforms.\n\n### Build process\n\nThe project is structured like a normal Python package with a standard ``setup.py`` file.\nThe build process for a single entry in the build matrices is as follows (see for example ``appveyor.yml`` file):\n\n0. In Linux and MacOS build: get OpenCV's optional C dependencies that we compile against\n\n1. Checkout repository and submodules\n\n   -  OpenCV is included as submodule and the version is updated\n      manually by maintainers when a new OpenCV release has been made\n   -  Contrib modules are also included as a submodule\n\n2. Find OpenCV version from the sources\n3. Install Python dependencies\n\n   - ``setup.py`` installs the dependencies itself, so you need to run it in an environment\n     where you have the rights to install modules with Pip for the running Python\n\n4. Build OpenCV\n\n   -  tests are disabled, otherwise build time increases too much\n   -  there are 4 build matrix entries for each build combination: with and without contrib modules, with and without GUI (headless)\n   -  Linux builds run in manylinux Docker containers (CentOS 5)\n\n5. Rearrange OpenCV's build result, add our custom files and generate wheel\n\n6. Linux and macOS wheels are transformed with auditwheel and delocate, correspondingly\n\n7. Install the generated wheel\n8. Test that Python can import the library and run some sanity checks\n9. Use twine to upload the generated wheel to PyPI (only in release builds)\n\nSteps 1--5 are handled by ``setup.py bdist_wheel``.\n\nThe build can be customized with environment variables.\nIn addition to any variables that OpenCV's build accepts, we recognize:\n\n- ``ENABLE_CONTRIB`` and ``ENABLE_HEADLESS``. Set to ``1`` to build the contrib and/or headless version\n- ``CMAKE_ARGS``. Additional arguments for OpenCV's CMake invocation. You can use this to make a custom build.\n\n### Licensing\n\nOpencv-python package (scripts in this repository) is available under MIT license.\n\nOpenCV itself is available under [3-clause BSD License](https://github.com/opencv/opencv/blob/master/LICENSE).\n\nThird party package licenses are at [LICENSE-3RD-PARTY.txt](https://github.com/skvark/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\nAll wheels ship with [FFmpeg](http://ffmpeg.org) licensed under the [LGPLv2.1](http://www.gnu.org/licenses/old-licenses/lgpl-2.1.html).\n\nLinux wheels ship with [Qt 4.8.7](http://doc.qt.io/qt-4.8/lgpl.html) licensed under the [LGPLv2.1](http://www.gnu.org/licenses/old-licenses/lgpl-2.1.html).\n\nMacOS wheels ship with [Qt 5](http://doc.qt.io/qt-5/lgpl.html) licensed under the [LGPLv3](http://www.gnu.org/licenses/lgpl-3.0.html).\n\n### Versioning\n\n``find_version.py`` script searches for the version information from OpenCV sources and appends also a revision number specific to this repository to the version string.\n\n### Releases\n\nA release is made and uploaded to PyPI when a new tag is pushed to master branch. These tags differentiate packages (this repo might have modifications but OpenCV version stays same) and should be incremented sequentially. In practice, release version numbers look like this:\n\n``cv_major.cv_minor.cv_revision.package_revision`` e.g. ``3.1.0.0``\n\nThe master branch follows OpenCV master branch releases. 3.4 branch follows OpenCV 3.4 bugfix releases.\n\n### Development builds\n\nEvery commit to the master branch of this repo will be built. Possible build artifacts use local version identifiers:\n\n``cv_major.cv_minor.cv_revision+git_hash_of_this_repo`` e.g. ``3.1.0+14a8d39``\n\nThese artifacts can't be and will not be uploaded to PyPI.\n\n### Manylinux wheels\n\nLinux wheels are built using [manylinux](https://github.com/pypa/python-manylinux-demo). These wheels should work out of the box for most of the distros (which use GNU C standard library) out there since they are built against an old version of glibc.\n\nThe default ``manylinux`` images have been extended with some OpenCV dependencies. See [Docker folder](https://github.com/skvark/opencv-python/tree/master/docker) for more info.\n\n### Supported Python versions\n\nPython 3.x releases are provided for officially supported versions (not in EOL).\n\nCurrently, builds for following Python versions are provided:\n\n- 3.5\n- 3.6\n- 3.7\n- 3.8\n"}, "paddlepaddle": {"file_name": "PaddlePaddle/Paddle/README.md", "raw_text": "\ufeff\n# PaddlePaddle\n\nEnglish | [\u7b80\u4f53\u4e2d\u6587](./README_cn.md)\n\n[![Build Status](https://travis-ci.org/PaddlePaddle/Paddle.svg?branch=develop)](https://travis-ci.org/PaddlePaddle/Paddle)\n[![Documentation Status](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](http://www.paddlepaddle.org.cn/documentation/docs/en/1.7/beginners_guide/index_en.html)\n[![Documentation Status](https://img.shields.io/badge/\u4e2d\u6587\u6587\u6863-\u6700\u65b0-brightgreen.svg)](http://www.paddlepaddle.org.cn/documentation/docs/zh/1.7/beginners_guide/index_cn.html)\n[![Release](https://img.shields.io/github/release/PaddlePaddle/Paddle.svg)](https://github.com/PaddlePaddle/Paddle/releases)\n[![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](LICENSE)\n\nWelcome to the PaddlePaddle GitHub.\n\nPaddlePaddle, as the only independent R&D deep learning platform in China, has been officially open-sourced to professional communities since 2016. It is an industrial platform with advanced technologies and rich features that cover core deep learning frameworks, basic model libraries, end-to-end development kits, tool & component as well as service platforms.\nPaddlePaddle is originated from industrial practices with dedication and commitments to industrialization. It has been widely adopted by a wide range of sectors including manufacturing, agriculture, enterprise service and so on while serving more than 1.5 million developers. With such advantages, PaddlePaddle has helped an increasing number of partners commercialize AI.\n\n\n\n## Installation\n\n### Latest PaddlePaddle Release: [v1.7](https://github.com/PaddlePaddle/Paddle/tree/release/1.7)\n\nOur vision is to enable deep learning for everyone via PaddlePaddle.\nPlease refer to our [release announcement](https://github.com/PaddlePaddle/Paddle/releases) to track the latest feature of PaddlePaddle.\n### Install Latest Stable Release:\n```\n# Linux CPU\npip install paddlepaddle\n# Linux GPU cuda10cudnn7\npip install paddlepaddle-gpu\n# Linux GPU cuda9cudnn7\npip install paddlepaddle-gpu==1.7.2.post97\n\n```\nIt is recommended to read [this doc](https://www.paddlepaddle.org.cn/documentation/docs/en/beginners_guide/install/index_en.html) on our website.\n\nNow our developers could acquire Tesla V100 online computing resources for free. If you create a program by AI Studio, you would obtain 12 hours to train models online per day. If you could insist on that for five consecutive days, then you would own extra 48 hours. [Click here to start](http://ai.baidu.com/support/news?action=detail&id=981).\n\n## FOUR LEADING TECHNOLOGIES\n\n- **Agile Framework for Industrial Development of Deep Neural Networks**\n\n    The PaddlePaddle deep learning framework facilitates the development while lowering the technical burden,through leveraging a programmable scheme to architect the neural networks. It supports both declarative programming and imperative programming with both development flexibility and high runtime performance preserved.  The neural architectures could be automatically designed by algorithms with better performance than the ones designed by human experts.\n\n\n-  **Support Ultra-Large-Scale Training of Deep Neural Networks**\n\n    PaddlePaddle has made breakthroughs in ultra-large-scale deep neural networks training. It launched the world's first large-scale open source training platform that supports the deep networks training with 100 billions of features and trillions of parameters using data sources distributed over hundreds of nodes. PaddlePaddle overcomes the online deep learning challenges for ultra-large-scale deep learning models, and further achieved  the real-time model updating with more than 1 trillion parameters.\n     [Click here to learn more](https://github.com/PaddlePaddle/Fleet)\n\n\n- **Accelerated High-Performance Inference over Ubiquitous Deployments**\n\n    PaddlePaddle is not only compatible with other open-source frameworks for models training, but also works well on the ubiquitous developments, varying from platforms to devices. More specific, PaddlePaddle accelerates the inference procedure with fastest speed-up. Note that, a recent breakthrough of inference speed has been made by PaddlePaddle on Huawei's Kirin NPU, through the hardware/software co-optimization.\n     [Click here to learn more](https://github.com/PaddlePaddle/Paddle-Lite)\n     \n     \n- **Industry-Oriented Models and Libraries with Open Source Repositories**\n\n     PaddlePaddle includes and maintains more than 100 mainstream models that have been practiced and polished for a long time in industry. Some of these models have won major prizes from key international competitions. In the meanwhile, PaddlePaddle has further more than 200 pre-training models (some of them with source codes) to facilitate the rapid development of industrial applications.\n     [Click here to learn more](https://github.com/PaddlePaddle/models)\n     \n\n## Documentation\n\nWe provide [English](http://www.paddlepaddle.org.cn/documentation/docs/en/1.7/beginners_guide/index_en.html) and\n[Chinese](http://www.paddlepaddle.org.cn/documentation/docs/zh/1.7/beginners_guide/index_cn.html) documentation.\n\n- [Basic Deep Learning Models](https://www.paddlepaddle.org.cn/documentation/docs/en/beginners_guide/basics/index_en.html#basic-deep-learning-models)\n\n  You might want to start from how to implement deep learning basics with PaddlePaddle.\n\n\n- [User Guides](https://www.paddlepaddle.org.cn/documentation/docs/en/user_guides/index_en.html)\n\n  You might have got the hang of Beginner\u2019s Guide, and wish to model practical problems and build your original networks.\n  \n  \n- [Advanced User Guides](https://www.paddlepaddle.org.cn/documentation/docs/en/advanced_usage/index_en.html)\n\n  So far you have already been familiar with Fluid. And the next expectation should be building a more efficient model or inventing your original Operator. \n\n\n- [API Reference](https://www.paddlepaddle.org.cn/documentation/docs/en/api/index_en.html)\n\n   Our new API enables much shorter programs.\n\n\n- [How to Contribute](http://paddlepaddle.org.cn/documentation/docs/en/1.7/advanced_usage/development/contribute_to_paddle/index_en.html)\n\n   We appreciate your contributions!\n\n## Communication\n\n- [Github Issues](https://github.com/PaddlePaddle/Paddle/issues): bug reports, feature requests, install issues, usage issues, etc.\n- QQ discussion group: 796771754 (PaddlePaddle).\n- [Forums](http://ai.baidu.com/forum/topic/list/168?pageNo=1): discuss implementations, research, etc.\n\n## Copyright and License\nPaddlePaddle is provided under the [Apache-2.0 license](LICENSE).\n"}, "pandas": {"file_name": "pandas-dev/pandas/README.md", "raw_text": "<div align=\"center\">\n  <img src=\"https://dev.pandas.io/static/img/pandas.svg\"><br>\n</div>\n\n-----------------\n\n# pandas: powerful Python data analysis toolkit\n[![PyPI Latest Release](https://img.shields.io/pypi/v/pandas.svg)](https://pypi.org/project/pandas/)\n[![Conda Latest Release](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/anaconda/pandas/)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3509134.svg)](https://doi.org/10.5281/zenodo.3509134)\n[![Package Status](https://img.shields.io/pypi/status/pandas.svg)](https://pypi.org/project/pandas/)\n[![License](https://img.shields.io/pypi/l/pandas.svg)](https://github.com/pandas-dev/pandas/blob/master/LICENSE)\n[![Travis Build Status](https://travis-ci.org/pandas-dev/pandas.svg?branch=master)](https://travis-ci.org/pandas-dev/pandas)\n[![Azure Build Status](https://dev.azure.com/pandas-dev/pandas/_apis/build/status/pandas-dev.pandas?branch=master)](https://dev.azure.com/pandas-dev/pandas/_build/latest?definitionId=1&branch=master)\n[![Coverage](https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=master)](https://codecov.io/gh/pandas-dev/pandas)\n[![Downloads](https://anaconda.org/conda-forge/pandas/badges/downloads.svg)](https://pandas.pydata.org)\n[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/pydata/pandas)\n[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)\n\n## What is it?\n\n**pandas** is a Python package providing fast, flexible, and expressive data\nstructures designed to make working with \"relational\" or \"labeled\" data both\neasy and intuitive. It aims to be the fundamental high-level building block for\ndoing practical, **real world** data analysis in Python. Additionally, it has\nthe broader goal of becoming **the most powerful and flexible open source data\nanalysis / manipulation tool available in any language**. It is already well on\nits way towards this goal.\n\n## Main Features\nHere are just a few of the things that pandas does well:\n\n  - Easy handling of [**missing data**][missing-data] (represented as\n    `NaN`) in floating point as well as non-floating point data\n  - Size mutability: columns can be [**inserted and\n    deleted**][insertion-deletion] from DataFrame and higher dimensional\n    objects\n  - Automatic and explicit [**data alignment**][alignment]: objects can\n    be explicitly aligned to a set of labels, or the user can simply\n    ignore the labels and let `Series`, `DataFrame`, etc. automatically\n    align the data for you in computations\n  - Powerful, flexible [**group by**][groupby] functionality to perform\n    split-apply-combine operations on data sets, for both aggregating\n    and transforming data\n  - Make it [**easy to convert**][conversion] ragged,\n    differently-indexed data in other Python and NumPy data structures\n    into DataFrame objects\n  - Intelligent label-based [**slicing**][slicing], [**fancy\n    indexing**][fancy-indexing], and [**subsetting**][subsetting] of\n    large data sets\n  - Intuitive [**merging**][merging] and [**joining**][joining] data\n    sets\n  - Flexible [**reshaping**][reshape] and [**pivoting**][pivot-table] of\n    data sets\n  - [**Hierarchical**][mi] labeling of axes (possible to have multiple\n    labels per tick)\n  - Robust IO tools for loading data from [**flat files**][flat-files]\n    (CSV and delimited), [**Excel files**][excel], [**databases**][db],\n    and saving/loading data from the ultrafast [**HDF5 format**][hdfstore]\n  - [**Time series**][timeseries]-specific functionality: date range\n    generation and frequency conversion, moving window statistics,\n    date shifting and lagging.\n\n\n   [missing-data]: https://pandas.pydata.org/pandas-docs/stable/missing_data.html#working-with-missing-data\n   [insertion-deletion]: https://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion\n   [alignment]: https://pandas.pydata.org/pandas-docs/stable/dsintro.html?highlight=alignment#intro-to-data-structures\n   [groupby]: https://pandas.pydata.org/pandas-docs/stable/groupby.html#group-by-split-apply-combine\n   [conversion]: https://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe\n   [slicing]: https://pandas.pydata.org/pandas-docs/stable/indexing.html#slicing-ranges\n   [fancy-indexing]: https://pandas.pydata.org/pandas-docs/stable/indexing.html#advanced-indexing-with-ix\n   [subsetting]: https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\n   [merging]: https://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging\n   [joining]: https://pandas.pydata.org/pandas-docs/stable/merging.html#joining-on-index\n   [reshape]: https://pandas.pydata.org/pandas-docs/stable/reshaping.html#reshaping-and-pivot-tables\n   [pivot-table]: https://pandas.pydata.org/pandas-docs/stable/reshaping.html#pivot-tables-and-cross-tabulations\n   [mi]: https://pandas.pydata.org/pandas-docs/stable/indexing.html#hierarchical-indexing-multiindex\n   [flat-files]: https://pandas.pydata.org/pandas-docs/stable/io.html#csv-text-files\n   [excel]: https://pandas.pydata.org/pandas-docs/stable/io.html#excel-files\n   [db]: https://pandas.pydata.org/pandas-docs/stable/io.html#sql-queries\n   [hdfstore]: https://pandas.pydata.org/pandas-docs/stable/io.html#hdf5-pytables\n   [timeseries]: https://pandas.pydata.org/pandas-docs/stable/timeseries.html#time-series-date-functionality\n\n## Where to get it\nThe source code is currently hosted on GitHub at:\nhttps://github.com/pandas-dev/pandas\n\nBinary installers for the latest released version are available at the [Python\npackage index](https://pypi.org/project/pandas) and on conda.\n\n```sh\n# conda\nconda install pandas\n```\n\n```sh\n# or PyPI\npip install pandas\n```\n\n## Dependencies\n- [NumPy](https://www.numpy.org)\n- [python-dateutil](https://labix.org/python-dateutil)\n- [pytz](https://pythonhosted.org/pytz)\n\nSee the [full installation instructions](https://pandas.pydata.org/pandas-docs/stable/install.html#dependencies) for minimum supported versions of required, recommended and optional dependencies.\n\n## Installation from sources\nTo install pandas from source you need Cython in addition to the normal\ndependencies above. Cython can be installed from pypi:\n\n```sh\npip install cython\n```\n\nIn the `pandas` directory (same one where you found this file after\ncloning the git repo), execute:\n\n```sh\npython setup.py install\n```\n\nor for installing in [development mode](https://pip.pypa.io/en/latest/reference/pip_install.html#editable-installs):\n\n\n```sh\npython -m pip install -e . --no-build-isolation --no-use-pep517\n```\n\nIf you have `make`, you can also use `make develop` to run the same command.\n\nor alternatively\n\n```sh\npython setup.py develop\n```\n\nSee the full instructions for [installing from source](https://pandas.pydata.org/pandas-docs/stable/install.html#installing-from-source).\n\n## License\n[BSD 3](LICENSE)\n\n## Documentation\nThe official documentation is hosted on PyData.org: https://pandas.pydata.org/pandas-docs/stable\n\n## Background\nWork on ``pandas`` started at AQR (a quantitative hedge fund) in 2008 and\nhas been under active development since then.\n\n## Getting Help\n\nFor usage questions, the best place to go to is [StackOverflow](https://stackoverflow.com/questions/tagged/pandas).\nFurther, general questions and discussions can also take place on the [pydata mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata).\n\n## Discussion and Development\nMost development discussion is taking place on github in this repo. Further, the [pandas-dev mailing list](https://mail.python.org/mailman/listinfo/pandas-dev) can also be used for specialized discussions or design issues, and a [Gitter channel](https://gitter.im/pydata/pandas) is available for quick development related questions.\n\n## Contributing to pandas [![Open Source Helpers](https://www.codetriage.com/pandas-dev/pandas/badges/users.svg)](https://www.codetriage.com/pandas-dev/pandas)\n\nAll contributions, bug reports, bug fixes, documentation improvements, enhancements and ideas are welcome.\n\nA detailed overview on how to contribute can be found in the **[contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)**. There is also an [overview](.github/CONTRIBUTING.md) on GitHub.\n\nIf you are simply looking to start working with the pandas codebase, navigate to the [GitHub \"issues\" tab](https://github.com/pandas-dev/pandas/issues) and start looking through interesting issues. There are a number of issues listed under [Docs](https://github.com/pandas-dev/pandas/issues?labels=Docs&sort=updated&state=open) and [good first issue](https://github.com/pandas-dev/pandas/issues?labels=good+first+issue&sort=updated&state=open) where you could start out.\n\nYou can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to [subscribe to pandas on CodeTriage](https://www.codetriage.com/pandas-dev/pandas).\n\nOr maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking \u2018this can be improved\u2019...you can do something about it!\n\nFeel free to ask questions on the [mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata) or on [Gitter](https://gitter.im/pydata/pandas).\n\nAs contributors and maintainers to this project, you are expected to abide by pandas' code of conduct. More information can be found at: [Contributor Code of Conduct](https://github.com/pandas-dev/pandas/blob/master/.github/CODE_OF_CONDUCT.md)\n"}, "pandas-profiling": {"file_name": "pandas-profiling/pandas-profiling/README.md", "raw_text": "# Pandas Profiling\n\n![Pandas Profiling Logo Header](http://pandas-profiling.github.io/pandas-profiling/docs/assets/logo_header.png)\n\n[![Build Status](https://travis-ci.com/pandas-profiling/pandas-profiling.svg?branch=master)](https://travis-ci.com/pandas-profiling/pandas-profiling)\n[![Code Coverage](https://codecov.io/gh/pandas-profiling/pandas-profiling/branch/master/graph/badge.svg?token=gMptB4YUnF)](https://codecov.io/gh/pandas-profiling/pandas-profiling)\n[![Release Version](https://img.shields.io/github/release/pandas-profiling/pandas-profiling.svg)](https://github.com/pandas-profiling/pandas-profiling/releases)\n[![Python Version](https://img.shields.io/pypi/pyversions/pandas-profiling)](https://pypi.org/project/pandas-profiling/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\n\nGenerates profile reports from a pandas `DataFrame`. \nThe pandas `df.describe()` function is great but a little basic for serious exploratory data analysis. \n`pandas_profiling` extends the pandas DataFrame with `df.profile_report()` for quick data analysis.\n\nFor each column the following statistics - if relevant for the column type - are presented in an interactive HTML report:\n\n* **Type inference**: detect the [types](#types) of columns in a dataframe.\n* **Essentials**: type, unique values, missing values\n* **Quantile statistics** like minimum value, Q1, median, Q3, maximum, range, interquartile range\n* **Descriptive statistics** like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness\n* **Most frequent values**\n* **Histogram**\n* **Correlations** highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices\n* **Missing values** matrix, count, heatmap and dendrogram of missing values\n* **Text analysis** learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data.\n\n## Announcements\n\n### New in v2.6.0\n\n#### Dependency policy\nThe current dependency policy is suboptimal. Pinning the dependencies is great for reproducibility (high guarantee to work), but on the downside requires frequent maintenance and introduces compatibility issues with other packages. Therefore, we are moving away from pinning dependencies and instead specify a minimum version. \n\n#### Pandas v1\nEarly releases of pandas v1 demonstrated many regressions that broke functionality (as acknowledged by the authors [here](https://github.com/pandas-dev/pandas/issues/31523). At this point, pandas is more stable and we notice high demand for compatibility. We move on to support pandas' latest versions. To ensure compatibility with both versions, we have extended the test matrix to test against both pandas 0.x.y and 1.x.y.\n\n#### Python 3.6+ features\nPython 3.6 introduces ordered dicts and f-strings, which we now rely on. This means that from pandas-profiling 2.6, you should minimally run Python 3.6. For users that for some reason cannot update, you can use pandas-profiling 2.5.0, but you unfortunately won't benefit from updates or maintenance.\n\n#### Extended continuous integration\nStarting from this release, we use Github Actions and Travis CI combined to increase maintainability. \nTravis CI handles the testing, Github Actions automates part of the development process by running black and building the docs.\n\n### Support `pandas-profiling`\nWith your help, we got approved for [GitHub Sponsors](https://github.com/sponsors/sbrugman)! \nIt's extra exciting that GitHub **matches your contribution** for the first year.\nTherefore, we welcome you to support the project through GitHub! \n\nFind more information here:\n\n - [Sponsor the project on GitHub](https://github.com/sponsors/sbrugman)\n - [Read the release notes v2.6.0](https://github.com/pandas-profiling/pandas-profiling/releases/tag/v2.6.0) \n\n *April 14, 2020 \ud83d\udc98*\n\n---\n\n_Contents:_ **[Examples](#examples)** |\n**[Installation](#installation)** | **[Documentation](#documentation)** |\n**[Large datasets](#large-datasets)** | **[Command line usage](#command-line-usage)** |\n**[Advanced usage](#advanced-usage)** |\n**[Types](#types)** | **[How to contribute](#how-to-contribute)** |\n**[Editor Integration](#editor-integration)** | **[Dependencies](#dependencies)**\n\n---\n\n## Examples\n\nThe following examples can give you an impression of what the package can do:\n\n* [Census Income](http://pandas-profiling.github.io/pandas-profiling/examples/census/census_report.html) (US Adult Census data relating income)\n* [NASA Meteorites](http://pandas-profiling.github.io/pandas-profiling/examples/meteorites/meteorites_report.html) (comprehensive set of meteorite landings) [![Open In Colab](https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667)](https://colab.research.google.com/github/pandas-profiling/pandas-profiling/blob/master/examples/meteorites/meteorites.ipynb) [![Binder](https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667)](https://mybinder.org/v2/gh/pandas-profiling/pandas-profiling/master?filepath=examples%2Fmeteorites%2Fmeteorites.ipynb)\n* [Titanic](http://pandas-profiling.github.io/pandas-profiling/examples/titanic/titanic_report.html) (the \"Wonderwall\" of datasets) [![Open In Colab](https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667)](https://colab.research.google.com/github/pandas-profiling/pandas-profiling/blob/master/examples/titanic/titanic.ipynb) [![Binder](https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667)](https://mybinder.org/v2/gh/pandas-profiling/pandas-profiling/master?filepath=examples%2Ftitanic%2Ftitanic.ipynb)\n* [NZA](http://pandas-profiling.github.io/pandas-profiling/examples/nza/nza_report.html) (open data from the Dutch Healthcare Authority)\n* [Stata Auto](http://pandas-profiling.github.io/pandas-profiling/examples/stata_auto/stata_auto_report.html) (1978 Automobile data)\n* [Vektis](http://pandas-profiling.github.io/pandas-profiling/examples/vektis/vektis_report.html) (Vektis Dutch Healthcare data)\n* [Website Inaccessibility](http://pandas-profiling.github.io/pandas-profiling/examples/website_inaccessibility/website_inaccessibility_report.html) (demonstrates the URL type)\n* [Colors](http://pandas-profiling.github.io/pandas-profiling/examples/colors/colors_report.html) (a simple colors dataset)\n* [Russian Vocabulary](http://pandas-profiling.github.io/pandas-profiling/examples/russian_vocabulary/russian_vocabulary.html) (demonstrates text analysis)\n* [Orange prices](http://pandas-profiling.github.io/pandas-profiling/examples/themes/united_report.html) and [Coal prices](http://pandas-profiling.github.io/pandas-profiling/examples/themes/flatly_report.html) (showcase report themes)\n* [Tutorial: report structure using Kaggle data (advanced)](http://pandas-profiling.github.io/pandas-profiling/examples/kaggle/modify_report_structure.ipynb) (modify the report's structure) [![Open In Colab](https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667)](https://colab.research.google.com/github/pandas-profiling/pandas-profiling/blob/master/examples/kaggle/modify_report_structure.ipynb) [![Binder](https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667)](https://mybinder.org/v2/gh/pandas-profiling/pandas-profiling/master?filepath=examples%2Fkaggle%2Fmodify_report_structure.ipynb)\n\n## Installation\n\n### Using pip\n\n[![PyPi Downloads](https://pepy.tech/badge/pandas-profiling)](https://pepy.tech/project/pandas-profiling)\n[![PyPi Monthly Downloads](https://pepy.tech/badge/pandas-profiling/month)](https://pepy.tech/project/pandas-profiling/month)\n[![PyPi Version](https://badge.fury.io/py/pandas-profiling.svg)](https://pypi.org/project/pandas-profiling/)\n\nYou can install using the pip package manager by running\n\n    pip install pandas-profiling[notebook,html]\n    \nAlternatively, you could install the latest version directly from Github:\n\n    pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n    \n    \n### Using conda\n\n[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/pandas-profiling.svg)](https://anaconda.org/conda-forge/pandas-profiling)\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/pandas-profiling.svg)](https://anaconda.org/conda-forge/pandas-profiling) \n \nYou can install using the conda package manager by running\n\n    conda install -c conda-forge pandas-profiling\n\n### From source\n\nDownload the source code by cloning the repository or by pressing ['Download ZIP'](https://github.com/pandas-profiling/pandas-profiling/archive/master.zip) on this page. \nInstall by navigating to the proper directory and running\n\n    python setup.py install\n    \n## Documentation\n\nThe documentation for `pandas_profiling` can be found [here](https://pandas-profiling.github.io/pandas-profiling/docs/).\n\n### Getting started\n\nStart by loading in your pandas DataFrame, e.g. by using\n```python\nimport numpy as np\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\n\ndf = pd.DataFrame(\n    np.random.rand(100, 5),\n    columns=['a', 'b', 'c', 'd', 'e']\n)\n```\nTo generate the report, run:\n```python\nprofile = ProfileReport(df, title='Pandas Profiling Report', html={'style':{'full_width':True}})\n```\n\n#### Jupyter Notebook\n\nWe recommend generating reports interactively by using the Jupyter notebook. \nThere are two interfaces (see animations below): through widgets and through a HTML report.\n\n<img alt=\"Notebook Widgets\" src=\"http://pandas-profiling.github.io/pandas-profiling/docs/assets/widgets.gif\" width=\"800\" />\n\nThis is achieved by simply displaying the report. In the Jupyter Notebook, run:\n```python\nprofile.to_widgets()\n```\n\nThe HTML report can be included in a Juyter notebook:\n\n<img alt=\"HTML\" src=\"http://pandas-profiling.github.io/pandas-profiling/docs/assets/iframe.gif\" width=\"800\" />\n\nRun the following code:\n\n```python\nprofile.to_notebook_iframe()\n```\n\n#### Saving the report\n\nIf you want to generate a HTML report file, save the `ProfileReport` to an object and use the `to_file()` function:\n```python\nprofile.to_file(output_file=\"your_report.html\")\n```\nAlternatively, you can obtain the data as json:\n```python\n# As a string\njson_data = profile.to_json()\n\n# As a file\nprofile.to_file(output_file=\"your_report.json\")\n```\n\n### Large datasets\n\nVersion 2.4 introduces minimal mode. \nThis is a default configuration that disables expensive computations (such as correlations and dynamic binning).\nUse the following syntax:\n\n```python\nprofile = ProfileReport(large_dataset, minimal=True)\nprofile.to_file(output_file=\"output.html\")\n```\n\n### Command line usage\n\nFor standard formatted CSV files that can be read immediately by pandas, you can use the `pandas_profiling` executable. Run\n\n\tpandas_profiling -h\n\nfor information about options and arguments.\n\n### Advanced usage\n\nA set of options is available in order to adapt the report generated.\n\n* `title` (`str`): Title for the report ('Pandas Profiling Report' by default).\n* `pool_size` (`int`): Number of workers in thread pool. When set to zero, it is set to the number of CPUs available (0 by default).\n* `progress_bar` (`bool`): If True, `pandas-profiling` will display a progress bar.\n\nMore settings can be found in the [default configuration file](https://github.com/pandas-profiling/pandas-profiling/blob/master/src/pandas_profiling/config_default.yaml), [minimal configuration file](https://github.com/pandas-profiling/pandas-profiling/blob/master/src/pandas_profiling/config_minimal.yaml) and [dark themed configuration file](https://github.com/pandas-profiling/pandas-profiling/blob/master/src/pandas_profiling/config_dark.yaml).\n\n__Example__\n```python\nprofile = df.profile_report(title='Pandas Profiling Report', plot={'histogram': {'bins': 8}})\nprofile.to_file(output_file=\"output.html\")\n```\n\n## Types\n\nTypes are a powerful abstraction for effective data analysis, that goes beyond the logical data types (integer, float etc.).\n`pandas-profiling` currently recognizes the following types:\n\n- Boolean\n- Numerical\n- Date\n- Categorical\n- URL\n- Path\n\nWe have developed a type system for Python, tailored for data analysis: [visions](https://github.com/dylan-profiler/visions).\nSelecting the right typeset drastically reduces the complexity the code of your analysis.\nFuture versions of `pandas-profiling` will have extended type support through `visions`!\n\n## How to contribute\n\n[![Questions: Stackoverflow \"pandas-profiling\"](https://img.shields.io/badge/stackoverflow%20tag-pandas%20profiling-yellow)](https://stackoverflow.com/questions/tagged/pandas-profiling)\n\nThe package is actively maintained and developed as open-source software. \nIf `pandas-profiling` was helpful or interesting to you, you might want to get involved. \nThere are several ways of contributing and helping our thousands of users.\nIf you would like to be a industry partner or sponsor, please [drop us a line](mailto:pandasprofiling@gmail.com).\n\nThe documentation is generated using [`pdoc3`](https://github.com/pdoc3/pdoc). \nIf you are contributing to this project, you can rebuild the documentation using:\n```\nmake docs\n```\nor on Windows:\n```\nmake.bat docs\n```\n\nRead more on getting involved in the [Contribution Guide](https://github.com/pandas-profiling/pandas-profiling/blob/master/CONTRIBUTING.md).\n\n\n## Editor integration\n### PyCharm integration \n1. Install `pandas-profiling` via the instructions above\n2. Locate your `pandas-profiling` executable.\n\n\t  On macOS / Linux / BSD:\n\t\n\t```console\n\t$ which pandas_profiling\n\t(example) /usr/local/bin/pandas_profiling\n\t```\n\t\n\t  On Windows:\n\t\n\t```console\n\t$ where pandas_profiling\n\t(example) C:\\ProgramData\\Anaconda3\\Scripts\\pandas_profiling.exe\n\t```\n\n2. In Pycharm, go to _Settings_ (or _Preferences_ on macOS) > _Tools_ > _External tools_\n3. Click the _+_ icon to add a new external tool\n4. Insert the following values\n\t- Name: Pandas Profiling\n    - Program: *__The location obtained in step 2__*\n    - Arguments: \"$FilePath$\" \"$FileDir$/$FileNameWithoutAllExtensions$_report.html\"\n    - Working Directory: $ProjectFileDir$\n  \n<img alt=\"PyCharm Integration\" src=\"http://pandas-profiling.github.io/pandas-profiling/docs/assets/pycharm-integration.png\" width=\"400\" />\n  \nTo use the PyCharm Integration, right click on any dataset file:\n_External Tools_ > _Pandas Profiling_.\n\n### Other integrations\n\nOther editor integrations may be contributed via pull requests.\n\n## Dependencies\n\nThe profile report is written in HTML and CSS, which means pandas-profiling requires a modern browser. \n\nYou need [Python 3](https://python3statement.org/) to run this package. Other dependencies can be found in the requirements files:\n\n| Filename | Requirements|\n|----------|-------------|\n| [requirements.txt](https://github.com/pandas-profiling/pandas-profiling/blob/master/requirements.txt) | Package requirements|\n| [requirements-dev.txt](https://github.com/pandas-profiling/pandas-profiling/blob/master/requirements-dev.txt)  |  Requirements for development|\n| [requirements-test.txt](https://github.com/pandas-profiling/pandas-profiling/blob/master/requirements-test.txt) | Requirements for testing|\n| [setup.py](https://github.com/pandas-profiling/pandas-profiling/blob/master/setup.py) | Requirements for Widgets etc. |\n"}, "papermill": {"file_name": "nteract/papermill/README.md", "raw_text": "<a href=\"https://github.com/nteract/papermill\"><img src=\"https://media.githubusercontent.com/media/nteract/logos/master/nteract_papermill/exports/images/png/papermill_logo_wide.png\" height=\"48px\" /></a>\n=======================================================================================================================================================================\n\n<!---(binder links generated at https://mybinder.readthedocs.io/en/latest/howto/badges.html and compressed at https://tinyurl.com) -->\n[![Travis Build Status](https://travis-ci.org/nteract/papermill.svg?branch=master)](https://travis-ci.org/nteract/papermill)\n[![Azure Build Status](https://dev.azure.com/nteract/nteract/_apis/build/status/nteract.papermill?branchName=master)](https://dev.azure.com/nteract/nteract/_build/latest?definitionId=5&branchName=master)\n[![image](https://codecov.io/github/nteract/papermill/coverage.svg?branch=master)](https://codecov.io/github/nteract/papermill?branch=master)\n[![Documentation Status](https://readthedocs.org/projects/papermill/badge/?version=latest)](http://papermill.readthedocs.io/en/latest/?badge=latest)\n[![badge](https://tinyurl.com/ybwovtw2)](https://mybinder.org/v2/gh/nteract/papermill/master?filepath=binder%2Fprocess_highlight_dates.ipynb)\n[![badge](https://tinyurl.com/y7uz2eh9)](https://mybinder.org/v2/gh/nteract/papermill/master?)\n[![Python 3.6](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/)\n[![Python 3.7](https://img.shields.io/badge/python-3.7-blue.svg)](https://www.python.org/downloads/release/python-370/)\n[![Python 3.8](https://img.shields.io/badge/python-3.8-blue.svg)](https://www.python.org/downloads/release/python-380/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)\n\n**papermill** is a tool for parameterizing, executing, and analyzing\nJupyter Notebooks.\n\nPapermill lets you:\n\n-   **parameterize** notebooks\n-   **execute** notebooks\n\nThis opens up new opportunities for how notebooks can be used. For\nexample:\n\n-   Perhaps you have a financial report that you wish to run with\n    different values on the first or last day of a month or at the\n    beginning or end of the year, **using parameters** makes this task\n    easier.\n-   Do you want to run a notebook and depending on its results, choose a\n    particular notebook to run next? You can now programmatically\n    **execute a workflow** without having to copy and paste from\n    notebook to notebook manually.\n\n## Installation\n\nFrom the command line:\n\n``` {.sourceCode .bash}\npip install papermill\n```\n\nFor all optional io dependencies, you can specify individual bundles\nlike `s3`, or `azure` -- or use `all`\n\n``` {.sourceCode .bash}\npip install papermill[all]\n```\n\n## Python Version Support\n\nThis library currently supports Python 3.5+ versions. As minor Python\nversions are officially sunset by the Python org papermill will similarly\ndrop support in the future.\n\n## Usage\n\n### Parameterizing a Notebook\n\nTo parameterize your notebook designate a cell with the tag ``parameters``.\n\n![enable parameters in Jupyter](docs/img/enable_parameters.gif)\n\nPapermill looks for the ``parameters`` cell and treats this cell as defaults for the parameters passed in at execution time. Papermill will add a new cell tagged with ``injected-parameters`` with input parameters in order to overwrite the values in ``parameters``. If no cell is tagged with ``parameters`` the injected cell will be inserted at the top of the notebook.\n\nAdditionally, if you rerun notebooks through papermill and it will reuse the ``injected-parameters`` cell from the prior run. In this case Papermill will replace the old ``injected-parameters`` cell with the new run's inputs.\n\n![image](docs/img/parameters.png)\n\n### Executing a Notebook\n\nThe two ways to execute the notebook with parameters are: (1) through\nthe Python API and (2) through the command line interface.\n\n#### Execute via the Python API\n\n``` {.sourceCode .python}\nimport papermill as pm\n\npm.execute_notebook(\n   'path/to/input.ipynb',\n   'path/to/output.ipynb',\n   parameters = dict(alpha=0.6, ratio=0.1)\n)\n```\n\n#### Execute via CLI\n\nHere's an example of a local notebook being executed and output to an\nAmazon S3 account:\n\n``` {.sourceCode .bash}\n$ papermill local/input.ipynb s3://bkt/output.ipynb -p alpha 0.6 -p l1_ratio 0.1\n```\n\n**NOTE:**\nIf you use multiple AWS accounts, and you have [properly configured your AWS  credentials](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html), then you can specify which account to use by setting the `AWS_PROFILE` environment variable at the command-line. For example:\n\n``` {.sourceCode .bash}\n$ AWS_PROFILE=dev_account papermill local/input.ipynb s3://bkt/output.ipynb -p alpha 0.6 -p l1_ratio 0.1\n```\n\nIn the above example, two parameters are set: ``alpha`` and ``l1_ratio`` using ``-p`` (``--parameters`` also works). Parameter values that look like booleans or numbers will be interpreted as such. Here are the different ways users may set parameters:\n\n``` {.sourceCode .bash}\n$ papermill local/input.ipynb s3://bkt/output.ipynb -r version 1.0\n```\n\nUsing ``-r`` or ``--parameters_raw``, users can set parameters one by one. However, unlike ``-p``, the parameter will remain a string, even if it may be interpreted as a number or boolean.\n\n``` {.sourceCode .bash}\n$ papermill local/input.ipynb s3://bkt/output.ipynb -f parameters.yaml\n```\n\nUsing ``-f`` or ``--parameters_file``, users can provide a YAML file from which parameter values should be read.\n\n``` {.sourceCode .bash}\n$ papermill local/input.ipynb s3://bkt/output.ipynb -y \"\nalpha: 0.6\nl1_ratio: 0.1\"\n```\n\nUsing ``-y`` or ``--parameters_yaml``, users can directly provide a YAML string containing parameter values.\n\n``` {.sourceCode .bash}\n$ papermill local/input.ipynb s3://bkt/output.ipynb -b YWxwaGE6IDAuNgpsMV9yYXRpbzogMC4xCg==\n```\n\nUsing ``-b`` or ``--parameters_base64``, users can provide a YAML string, base64-encoded, containing parameter values.\n\nWhen using YAML to pass arguments, through ``-y``, ``-b`` or ``-f``, parameter values can be arrays or dictionaries:\n\n``` {.sourceCode .bash}\n$ papermill local/input.ipynb s3://bkt/output.ipynb -y \"\nx:\n    - 0.0\n    - 1.0\n    - 2.0\n    - 3.0\nlinear_function:\n    slope: 3.0\n    intercept: 1.0\"\n```\n\n#### Supported Name Handlers\n\nPapermill supports the following name handlers for input and output paths during execution:\n\n * Local file system: `local`\n\n * HTTP, HTTPS protocol:  `http://, https://`\n\n * Amazon Web Services: [AWS S3](https://aws.amazon.com/s3/) `s3://`\n\n * Azure: [Azure DataLake Store](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-overview), [Azure Blob Store](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-overview) `adl://, abs://`\n\n * Google Cloud: [Google Cloud Storage](https://cloud.google.com/storage/) `gs://`\n\nDevelopment Guide\n-----------------\n\nRead [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines on how to setup a local development environment and make code changes back to Papermill.\n\nFor development guidelines look in the [DEVELOPMENT_GUIDE.md](./DEVELOPMENT_GUIDE.md) file. This should inform you on how to make particular additions to the code base.\n\nDocumentation\n-------------\n\nWe host the [Papermill documentation](http://papermill.readthedocs.io)\non ReadTheDocs.\n"}, "patsy": {"file_name": "pydata/patsy/README.rst", "raw_text": "Patsy is a Python library for describing statistical models\n(especially linear models, or models that have a linear component) and\nbuilding design matrices. Patsy brings the convenience of `R\n<http://www.r-project.org/>`_ \"formulas\" to Python.\n\n.. image:: https://img.shields.io/badge/docs-read%20now-blue.svg\n   :target: https://patsy.readthedocs.io/\n   :alt: Documentation\n.. image:: https://travis-ci.org/pydata/patsy.png?branch=master\n   :target: https://travis-ci.org/pydata/patsy\n   :alt: Build status\n.. image:: https://coveralls.io/repos/pydata/patsy/badge.png?branch=master\n   :target: https://coveralls.io/r/pydata/patsy?branch=master\n   :alt: Coverage\n.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.592075.svg\n   :target: https://doi.org/10.5281/zenodo.592075\n   :alt: Zenodo\n\nDocumentation:\n  https://patsy.readthedocs.io/\n\nDownloads:\n  http://pypi.python.org/pypi/patsy/\n\nDependencies:\n  * Python (2.6, 2.7, or 3.3+)\n  * six\n  * numpy\n\nOptional dependencies:\n  * nose: needed to run tests\n  * scipy: needed for spline-related functions like ``bs``\n\nInstall:\n  ``pip install patsy`` (or, for traditionalists: ``python setup.py install``)\n\nCode and bug tracker:\n  https://github.com/pydata/patsy\n\nMailing list:\n  * pydata@googlegroups.com\n  * http://groups.google.com/group/pydata\n\nLicense:\n  2-clause BSD, see LICENSE.txt for details.\n"}, "petastorm": {"file_name": "uber/petastorm/README.rst", "raw_text": "\nPetastorm\n=========\n\n.. image:: https://travis-ci.com/uber/petastorm.svg?branch=master\n   :target: https://travis-ci.com/uber/petastorm\n   :alt: Build Status (Travis CI)\n\n.. image:: https://codecov.io/gh/uber/petastorm/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/uber/petastorm/branch/master\n   :alt: Code coverage\n\n.. image:: https://img.shields.io/badge/License-Apache%202.0-blue.svg\n   :target: https://img.shields.io/badge/License-Apache%202.0-blue.svg\n   :alt: License\n\n.. image:: https://badge.fury.io/py/petastorm.svg\n   :target: https://pypi.org/project/petastorm\n   :alt: Latest Version\n\n.. inclusion-marker-start-do-not-remove\n\n.. contents::\n\n\nPetastorm is an open source data access library developed at Uber ATG. This library enables single machine or\ndistributed training and evaluation of deep learning models directly from datasets in Apache Parquet\nformat. Petastorm supports popular Python-based machine learning (ML) frameworks such as\n`Tensorflow <http://www.tensorflow.org/>`_, `PyTorch <https://pytorch.org/>`_, and\n`PySpark <http://spark.apache.org/docs/latest/api/python/pyspark.html>`_. It can also be used from pure Python code.\n\nDocumentation web site: `<https://petastorm.readthedocs.io>`_\n\n\n\nInstallation\n------------\n\n.. code-block:: bash\n\n    pip install petastorm\n\n\nThere are several extra dependencies that are defined by the ``petastorm`` package that are not installed automatically.\nThe extras are: ``tf``, ``tf_gpu``, ``torch``, ``opencv``, ``docs``, ``test``.\n\nFor example to trigger installation of GPU version of tensorflow and opencv, use the following pip command:\n\n.. code-block:: bash\n\n    pip install petastorm[opencv,tf_gpu]\n\n\n\nGenerating a dataset\n--------------------\n\nA dataset created using Petastorm is stored in `Apache Parquet <https://parquet.apache.org/>`_ format.\nOn top of a Parquet schema, petastorm also stores higher-level schema information that makes multidimensional arrays into a native part of a petastorm dataset. \n\nPetastorm supports extensible data codecs. These enable a user to use one of the standard data compressions (jpeg, png) or implement her own.\n\nGenerating a dataset is done using PySpark.\nPySpark natively supports Parquet format, making it easy to run on a single machine or on a Spark compute cluster.\nHere is a minimalistic example writing out a table with some random data.\n\n\n.. code-block:: python\n\n    HelloWorldSchema = Unischema('HelloWorldSchema', [\n       UnischemaField('id', np.int32, (), ScalarCodec(IntegerType()), False),\n       UnischemaField('image1', np.uint8, (128, 256, 3), CompressedImageCodec('png'), False),\n       UnischemaField('other_data', np.uint8, (None, 128, 30, None), NdarrayCodec(), False),\n    ])\n\n\n    def row_generator(x):\n       \"\"\"Returns a single entry in the generated dataset. Return a bunch of random values as an example.\"\"\"\n       return {'id': x,\n               'image1': np.random.randint(0, 255, dtype=np.uint8, size=(128, 256, 3)),\n               'other_data': np.random.randint(0, 255, dtype=np.uint8, size=(4, 128, 30, 3))}\n\n\n    def generate_hello_world_dataset(output_url='file:///tmp/hello_world_dataset'):\n       rows_count = 10\n       rowgroup_size_mb = 256\n\n       spark = SparkSession.builder.config('spark.driver.memory', '2g').master('local[2]').getOrCreate()\n       sc = spark.sparkContext\n\n       # Wrap dataset materialization portion. Will take care of setting up spark environment variables as\n       # well as save petastorm specific metadata\n       with materialize_dataset(spark, output_url, HelloWorldSchema, rowgroup_size_mb):\n\n           rows_rdd = sc.parallelize(range(rows_count))\\\n               .map(row_generator)\\\n               .map(lambda x: dict_to_spark_row(HelloWorldSchema, x))\n\n           spark.createDataFrame(rows_rdd, HelloWorldSchema.as_spark_schema()) \\\n               .coalesce(10) \\\n               .write \\\n               .mode('overwrite') \\\n               .parquet(output_url)\n\n- ``HelloWorldSchema`` is an instance of a ``Unischema`` object.\n  ``Unischema`` is capable of rendering types of its fields into different\n  framework specific formats, such as: Spark ``StructType``, Tensorflow\n  ``tf.DType`` and numpy ``numpy.dtype``.\n- To define a dataset field, you need to specify a ``type``, ``shape``, a\n  ``codec`` instance and whether the field is nullable for each field of the\n  ``Unischema``.\n- We use PySpark for writing output Parquet files. In this example, we launch\n  PySpark on a local box (``.master('local[2]')``). Of course for a larger\n  scale dataset generation we would need a real compute cluster.\n- We wrap spark dataset generation code with the ``materialize_dataset``\n  context manager.  The context manager is responsible for configuring row\n  group size at the beginning and write out petastorm specific metadata at the\n  end.\n- The row generating code is expected to return a Python dictionary indexed by\n  a field name. We use ``row_generator`` function for that. \n- ``dict_to_spark_row`` converts the dictionary into a ``pyspark.Row``\n  object while ensuring schema ``HelloWorldSchema`` compliance (shape,\n  type and is-nullable condition are tested).\n- Once we have a ``pyspark.DataFrame`` we write it out to a parquet\n  storage. The parquet schema is automatically derived from\n  ``HelloWorldSchema``.\n\nPlain Python API\n----------------\nThe ``petastorm.reader.Reader`` class is the main entry point for user\ncode that accesses the data from an ML framework such as Tensorflow or Pytorch.\nThe reader has multiple features such as:\n\n- Selective column readout\n- Multiple parallelism strategies: thread, process, single-threaded (for debug)\n- N-grams readout support\n- Row filtering (row predicates)\n- Shuffling\n- Partitioning for multi-GPU training\n- Local caching\n\nReading a dataset is simple using the ``petastorm.reader.Reader`` class which can be created using the\n``petastorm.make_reader`` factory method:\n\n.. code-block:: python\n\n   from petastorm import make_reader\n\n    with make_reader('hdfs://myhadoop/some_dataset') as reader:\n       for row in reader:\n           print(row)\n\n``hdfs://...`` and ``file://...`` are supported URL protocols.\n\nOnce a ``Reader`` is instantiated, you can use it as an iterator.\n\nTensorflow API\n--------------\n\nTo hookup the reader into a tensorflow graph, you can use the ``tf_tensors``\nfunction:\n\n.. code-block:: python\n\n    with make_reader('file:///some/localpath/a_dataset') as reader:\n       row_tensors = tf_tensors(reader)\n       with tf.Session() as session:\n           for _ in range(3):\n               print(session.run(row_tensors))\n\nAlternatively, you can use new ``tf.data.Dataset`` API;\n\n.. code-block:: python\n\n    with make_reader('file:///some/localpath/a_dataset') as reader:\n        dataset = make_petastorm_dataset(reader)\n        iterator = dataset.make_one_shot_iterator()\n        tensor = iterator.get_next()\n        with tf.Session() as sess:\n            sample = sess.run(tensor)\n            print(sample.id)\n\nPytorch API\n-----------\n\nAs illustrated in\n`pytorch_example.py <https://github.com/uber/petastorm/blob/master/examples/mnist/pytorch_example.py>`_,\nreading a petastorm dataset from pytorch\ncan be done via the adapter class ``petastorm.pytorch.DataLoader``,\nwhich allows custom pytorch collating function and transforms to be supplied.\n\nBe sure you have ``torch`` and ``torchvision`` installed:\n\n.. code-block:: bash\n\n    pip install torchvision\n\nThe minimalist example below assumes the definition of a ``Net`` class and\n``train`` and ``test`` functions, included in ``pytorch_example``:\n\n.. code-block:: python\n\n    import torch\n    from petastorm.pytorch import DataLoader\n\n    torch.manual_seed(1)\n    device = torch.device('cpu')\n    model = Net().to(device)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n    def _transform_row(mnist_row):\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n        return (transform(mnist_row['image']), mnist_row['digit'])\n\n\n    transform = TransformSpec(_transform_row, removed_fields=['idx'])\n\n    with DataLoader(make_reader('file:///localpath/mnist/train', num_epochs=10,\n                                transform_spec=transform), batch_size=64) as train_loader:\n        train(model, device, train_loader, 10, optimizer, 1)\n    with DataLoader(make_reader('file:///localpath/mnist/test', num_epochs=10,\n                                transform_spec=transform), batch_size=1000) as test_loader:\n        test(model, device, test_loader)\n\nSpark Dataset Converter API\n---------------------------\n\nSpark converter API simplifies the data conversion from Spark to TensorFlow or PyTorch.\nThe input Spark DataFrame is first materialized in the parquet format and then loaded as\na ``tf.data.Dataset`` or ``torch.utils.data.DataLoader``.\n\nThe minimalist example below assumes the definition of a compiled ``tf.keras`` model and a\nSpark DataFrame containing a feature column followed by a label column.\n\n.. code-block:: python\n\n    from petastorm.spark import SparkDatasetConverter, make_spark_converter\n    import tensorflow.compat.v1 as tf  # pylint: disable=import-error\n\n    # specify a cache dir first.\n    # the dir is used to save materialized spark dataframe files\n    spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, 'hdfs:/...')\n\n    df = ... # `df` is a spark dataframe\n\n    # create a converter from `df`\n    # it will materialize `df` to cache dir.\n    converter = make_spark_converter(df)\n\n    # make a tensorflow dataset from `converter`\n    with converter.make_tf_dataset() as dataset:\n        # the `dataset` is `tf.data.Dataset` object\n        # dataset transformation can be done if needed\n        dataset = dataset.map(...)\n        # we can train/evaluate model on the `dataset`\n        model.fit(dataset)\n        # when exiting the context, the reader of the dataset will be closed\n\n    # delete the cached files of the dataframe.\n    converter.delete()\n\nThe minimalist example below assumes the definition of a ``Net`` class and\n``train`` and ``test`` functions, included in\n`pytorch_example.py <https://github.com/uber/petastorm/blob/master/examples/mnist/pytorch_example.py>`_,\nand a Spark DataFrame containing a feature column followed by a label column.\n\n.. code-block:: python\n\n    from petastorm.spark import SparkDatasetConverter, make_spark_converter\n\n    # specify a cache dir first.\n    # the dir is used to save materialized spark dataframe files\n    spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, 'hdfs:/...')\n\n    df_train, df_test = ... # `df_train` and `df_test` are spark dataframes\n    model = Net()\n\n    # create a converter_train from `df_train`\n    # it will materialize `df_train` to cache dir. (the same for df_test)\n    converter_train = make_spark_converter(df_train)\n    converter_test = make_spark_converter(df_test)\n\n    # make a pytorch dataloader from `converter_train`\n    with converter_train.make_torch_dataloader() as dataloader_train:\n        # the `dataloader_train` is `torch.utils.data.DataLoader` object\n        # we can train model using the `dataloader_train`\n        train(model, dataloader_train, ...)\n        # when exiting the context, the reader of the dataset will be closed\n\n    # the same for `converter_test`\n    with converter_test.make_torch_dataloader() as dataloader_test:\n        test(model, dataloader_test, ...)\n\n    # delete the cached files of the dataframes.\n    converter_train.delete()\n    converter_test.delete()\n\n\nAnalyzing petastorm datasets using PySpark and SQL\n--------------------------------------------------\n\nA Petastorm dataset can be read into a Spark DataFrame using PySpark, where you can\nuse a wide range of Spark tools to analyze and manipulate the dataset.\n\n.. code-block:: python\n\n   # Create a dataframe object from a parquet file\n   dataframe = spark.read.parquet(dataset_url)\n\n   # Show a schema\n   dataframe.printSchema()\n\n   # Count all\n   dataframe.count()\n\n   # Show a single column\n   dataframe.select('id').show()\n\nSQL can be used to query a Petastorm dataset:\n\n.. code-block:: python\n\n   spark.sql(\n      'SELECT count(id) '\n      'from parquet.`file:///tmp/hello_world_dataset`').collect()\n\nYou can find a full code sample here: `pyspark_hello_world.py <https://github.com/uber/petastorm/blob/master/examples/hello_world/petastorm_dataset/pyspark_hello_world.py>`_,\n\nNon Petastorm Parquet Stores\n----------------------------\nPetastorm can also be used to read data directly from Apache Parquet stores. To achieve that, use\n``make_batch_reader`` (and not ``make_reader``). The following table summarizes the differences\n``make_batch_reader`` and ``make_reader`` functions.\n\n\n==================================================================  =====================================================\n``make_reader``                                                     ``make_batch_reader``\n==================================================================  =====================================================\nOnly Petastorm datasets (created using materializes_dataset)        Any Parquet store (some native Parquet column types\n                                                                    are not supported yet.\n------------------------------------------------------------------  -----------------------------------------------------\nThe reader returns one record at a time.                            The reader returns batches of records. The size of the\n                                                                    batch is not fixed and defined by Parquet row-group\n                                                                    size.\n------------------------------------------------------------------  -----------------------------------------------------\nPredicates passed to ``make_reader`` are evaluated per single row.  Predicates passed to ``make_batch_reader`` are evaluated per batch.\n==================================================================  =====================================================\n\n\nTroubleshooting\n---------------\n\nSee the Troubleshooting_ page and please submit a ticket_ if you can't find an\nanswer.\n\n\nPublications\n------------\n\n1. Gruener, R., Cheng, O., and Litvin, Y. (2018) *Introducing Petastorm: Uber ATG's Data Access Library for Deep Learning*. URL: https://eng.uber.com/petastorm/\n\n\n.. _Troubleshooting: docs/troubleshoot.rst\n.. _ticket: https://github.com/uber/petastorm/issues/new\n.. _Development: docs/development.rst\n\nHow to Contribute\n=================\n\nWe prefer to receive contributions in the form of GitHub pull requests. Please send pull requests against the ``github.com/uber/petastorm`` repository.\n\n- If you are looking for some ideas on what to contribute, check out `github issues <https://github.com/uber/petastorm/issues>`_ and comment on the issue.\n- If you have an idea for an improvement, or you'd like to report a bug but don't have time to fix it please a `create a github issue <https://github.com/uber/petastorm/issues/new>`_.\n\nTo contribute a patch:\n\n- Break your work into small, single-purpose patches if possible. It's much harder to merge in a large change with a lot of disjoint features.\n- Submit the patch as a GitHub pull request against the master branch. For a tutorial, see the GitHub guides on forking a repo and sending a pull request.\n- Include a detailed describtion of the proposed change in the pull request.\n- Make sure that your code passes the unit tests. You can find instructions how to run the unit tests `here <https://github.com/uber/petastorm/blob/master/docs/development.rst>`_.\n- Add new unit tests for your code.\n\nThank you in advance for your contributions!\n\n\nSee the Development_ for development related information.\n\n\n.. inclusion-marker-end-do-not-remove\n   Place contents above here if they should also appear in read-the-docs.\n   Contents below are already part of the read-the-docs table of contents.\n\n"}, "pillow": {"file_name": "python-pillow/Pillow/README.rst", "raw_text": "Pillow\n======\n\nPython Imaging Library (Fork)\n-----------------------------\n\nPillow is the friendly PIL fork by `Alex Clark and Contributors <https://github.com/python-pillow/Pillow/graphs/contributors>`_. PIL is the Python Imaging Library by Fredrik Lundh and Contributors. As of 2019, Pillow development is `supported by Tidelift <https://tidelift.com/subscription/pkg/pypi-pillow?utm_source=pypi-pillow&utm_medium=readme&utm_campaign=enterprise>`_.\n\n.. start-badges\n\n.. list-table::\n    :stub-columns: 1\n\n    * - docs\n      - |docs|\n    * - tests\n      - |linux| |macos| |windows| |gha_lint| |gha| |gha_windows| |gha_docker| |coverage|\n    * - package\n      - |zenodo| |tidelift| |version| |downloads|\n    * - social\n      - |gitter| |twitter|\n\n.. end-badges\n\nMore Information\n----------------\n\n- `Documentation <https://pillow.readthedocs.io/>`_\n\n  - `Installation <https://pillow.readthedocs.io/en/latest/installation.html>`_\n  - `Handbook <https://pillow.readthedocs.io/en/latest/handbook/index.html>`_\n\n- `Contribute <https://github.com/python-pillow/Pillow/blob/master/.github/CONTRIBUTING.md>`_\n\n  - `Issues <https://github.com/python-pillow/Pillow/issues>`_\n  - `Pull requests <https://github.com/python-pillow/Pillow/pulls>`_\n\n- `Changelog <https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst>`_\n\n  - `Pre-fork <https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst#pre-fork>`_\n\nReport a Vulnerability\n----------------------\n\nTo report a security vulnerability, please follow the procedure described in the `Tidelift security policy <https://tidelift.com/docs/security>`_.\n\n.. |docs| image:: https://readthedocs.org/projects/pillow/badge/?version=latest\n   :target: https://pillow.readthedocs.io/?badge=latest\n   :alt: Documentation Status\n\n.. |linux| image:: https://img.shields.io/travis/python-pillow/Pillow/master.svg?label=Linux%20build\n   :target: https://travis-ci.org/python-pillow/Pillow\n   :alt: Travis CI build status (Linux)\n\n.. |macos| image:: https://img.shields.io/travis/python-pillow/pillow-wheels/master.svg?label=macOS%20build\n   :target: https://travis-ci.org/python-pillow/pillow-wheels\n   :alt: Travis CI build status (macOS)\n\n.. |windows| image:: https://img.shields.io/appveyor/build/python-pillow/Pillow/master.svg?label=Windows%20build\n   :target: https://ci.appveyor.com/project/python-pillow/Pillow\n   :alt: AppVeyor CI build status (Windows)\n\n.. |gha_lint| image:: https://github.com/python-pillow/Pillow/workflows/Lint/badge.svg\n   :target: https://github.com/python-pillow/Pillow/actions?query=workflow%3ALint\n   :alt: GitHub Actions build status (Lint)\n\n.. |gha_docker| image:: https://github.com/python-pillow/Pillow/workflows/Test%20Docker/badge.svg\n   :target: https://github.com/python-pillow/Pillow/actions?query=workflow%3A%22Test+Docker%22\n   :alt: GitHub Actions build status (Test Docker)\n\n.. |gha| image:: https://github.com/python-pillow/Pillow/workflows/Test/badge.svg\n   :target: https://github.com/python-pillow/Pillow/actions?query=workflow%3ATest\n   :alt: GitHub Actions build status (Test Linux and macOS)\n\n.. |gha_windows| image:: https://github.com/python-pillow/Pillow/workflows/Test%20Windows/badge.svg\n   :target: https://github.com/python-pillow/Pillow/actions?query=workflow%3A%22Test+Windows%22\n   :alt: GitHub Actions build status (Test Windows)\n\n.. |coverage| image:: https://codecov.io/gh/python-pillow/Pillow/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/python-pillow/Pillow\n   :alt: Code coverage\n\n.. |zenodo| image:: https://zenodo.org/badge/17549/python-pillow/Pillow.svg\n   :target: https://zenodo.org/badge/latestdoi/17549/python-pillow/Pillow\n\n.. |tidelift| image:: https://tidelift.com/badges/package/pypi/Pillow?style=flat\n   :target: https://tidelift.com/subscription/pkg/pypi-pillow?utm_source=pypi-pillow&utm_medium=badge\n\n.. |version| image:: https://img.shields.io/pypi/v/pillow.svg\n   :target: https://pypi.org/project/Pillow/\n   :alt: Latest PyPI version\n\n.. |downloads| image:: https://img.shields.io/pypi/dm/pillow.svg\n   :target: https://pypi.org/project/Pillow/\n   :alt: Number of PyPI downloads\n\n.. |gitter| image:: https://badges.gitter.im/python-pillow/Pillow.svg\n   :target: https://gitter.im/python-pillow/Pillow?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge\n   :alt: Join the chat at https://gitter.im/python-pillow/Pillow\n\n.. |twitter| image:: https://img.shields.io/badge/tweet-on%20Twitter-00aced.svg\n   :target: https://twitter.com/PythonPillow\n   :alt: Follow on https://twitter.com/PythonPillow\n"}, "plotly": {"file_name": "plotly/plotly.py/README.md", "raw_text": "# plotly.py\n\n<table>\n    <tr>\n        <td>Latest Release</td>\n        <td>\n            <a href=\"https://pypi.org/project/plotly/\"/>\n            <img src=\"https://badge.fury.io/py/plotly.svg\"/>\n        </td>\n    </tr>\n    <tr>\n        <td>User forum</td>\n        <td>\n            <a href=\"https://community.plot.ly\"/>\n            <img src=\"https://img.shields.io/badge/help_forum-discourse-blue.svg\"/>\n        </td>\n    </tr>\n    <tr>\n        <td>PyPI Downloads</td>\n        <td>\n            <a href=\"https://pepy.tech/project/plotly\"/>\n            <img src=\"https://pepy.tech/badge/plotly/month\"/>\n        </td>\n    </tr>\n    <tr>\n        <td>License</td>\n        <td>\n            <a href=\"https://opensource.org/licenses/MIT\"/>\n            <img src=\"https://img.shields.io/badge/License-MIT-yellow.svg\"/>\n        </td>\n    </tr>\n</table>\n\n## Quickstart\n\n`pip install plotly==4.6.0`\n\nInside [Jupyter notebook](https://jupyter.org/install) (installable with `pip install \"notebook>=5.3\" \"ipywidgets>=7.2\"`):\n\n```python\nimport plotly.graph_objects as go\nfig = go.Figure()\nfig.add_trace(go.Scatter(y=[2, 1, 4, 3]))\nfig.add_trace(go.Bar(y=[1, 4, 3, 2]))\nfig.update_layout(title = 'Hello Figure')\nfig.show()\n```\n\nSee the [Python documentation](https://plot.ly/python/) for more examples.\n\nRead about what's new in [plotly.py v4](https://medium.com/plotly/plotly-py-4-0-is-here-offline-only-express-first-displayable-anywhere-fc444e5659ee)\n\n## Overview\n\n[plotly.py](https://plot.ly/python) is an interactive, open-source, and browser-based graphing library for Python :sparkles:\n\nBuilt on top of [plotly.js](https://github.com/plotly/plotly.js), `plotly.py` is a high-level, declarative charting library. plotly.js ships with over 30 chart types, including scientific charts, 3D graphs, statistical charts, SVG maps, financial charts, and more.\n\n`plotly.py` is [MIT Licensed](packages/python/chart-studio/LICENSE.txt). Plotly graphs can be viewed in Jupyter notebooks, standalone HTML files, or hosted online using [Chart Studio Cloud](https://chart-studio.plot.ly/feed/).\n\n[Contact us](https://plot.ly/products/consulting-and-oem/) for consulting, dashboard development, application integration, and feature additions.\n\n<p align=\"center\">\n    <a href=\"https://plot.ly/python\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/cldougl/plot_images/add_r_img/plotly_2017.png\">\n</a></p>\n\n---\n\n- [Online Documentation](https://plot.ly/python)\n- [Contributing to plotly](contributing.md)\n- [Changelog](CHANGELOG.md)\n- [Code of Conduct](CODE_OF_CONDUCT.md)\n- [Version 4 Migration Guide](https://plot.ly/python/next/v4-migration/)\n- [New! Announcing Dash 1.0](https://medium.com/plotly/welcoming-dash-1-0-0-f3af4b84bae)\n- [Community forum](https://community.plot.ly/c/api/python)\n\n---\n\n## Installation\n\nplotly.py may be installed using pip...\n\n```\npip install plotly==4.6.0\n```\n\nor conda.\n\n```\nconda install -c plotly plotly=4.6.0\n```\n\n### Jupyter Notebook Support\n\nFor use in the Jupyter Notebook, install the `notebook` and `ipywidgets`\npackages using pip...\n\n```\npip install \"notebook>=5.3\" \"ipywidgets==7.5\"\n```\n\nor conda.\n\n```\nconda install \"notebook>=5.3\" \"ipywidgets=7.5\"\n```\n\n### JupyterLab Support (Python 3.5+)\n\nFor use in JupyterLab, install the `jupyterlab` and `ipywidgets`\npackages using pip...\n\n```\npip install jupyterlab==1.2 \"ipywidgets==7.5\"\n```\n\nor conda.\n\n```\nconda install jupyterlab=1.2\nconda install \"ipywidgets=7.5\"\n```\n\nThen run the following commands to install the required JupyterLab extensions (note that this will require [`node`](https://nodejs.org/) to be installed):\n\n```\n# Avoid \"JavaScript heap out of memory\" errors during extension installation\n# (OS X/Linux)\nexport NODE_OPTIONS=--max-old-space-size=4096\n# (Windows)\nset NODE_OPTIONS=--max-old-space-size=4096\n\n# Jupyter widgets extension\njupyter labextension install @jupyter-widgets/jupyterlab-manager@1.1 --no-build\n\n# FigureWidget support\njupyter labextension install plotlywidget@4.6.0 --no-build\n\n# and jupyterlab renderer support\njupyter labextension install jupyterlab-plotly@4.6.0 --no-build\n\n# Build extensions (must be done to activate extensions since --no-build is used above)\njupyter lab build\n\n# Unset NODE_OPTIONS environment variable\n# (OS X/Linux)\nunset NODE_OPTIONS\n# (Windows)\nset NODE_OPTIONS=\n```\n\n### Static Image Export\n\nplotly.py supports static image export using the `to_image` and `write_image`\nfunctions in the `plotly.io` package. This functionality requires the\ninstallation of the plotly [orca](https://github.com/plotly/orca) command line utility and the\n[`psutil`](https://github.com/giampaolo/psutil) Python package.\n\nThese dependencies can both be installed using conda:\n\n```\nconda install -c plotly plotly-orca==1.3.1 psutil\n```\n\nOr, `psutil` can be installed using pip...\n\n```\npip install psutil\n```\n\nand orca can be installed according to the instructions in the [orca README](https://github.com/plotly/orca).\n\n#### Troubleshooting\n\n##### Wrong Executable found\n\nIf you get an error message stating that the `orca` executable that was found is not valid, this may be because another executable with the same name was found on your system. Please specify the complete path to the Plotly-Orca binary that you downloaded (for instance in the Miniconda folder) with the following command:\n\n`plotly.io.orca.config.executable = '/home/your_name/miniconda3/bin/orca'`\n\n### Extended Geo Support\n\nSome plotly.py features rely on fairly large geographic shape files. The county\nchoropleth figure factory is one such example. These shape files are distributed as a\nseparate `plotly-geo` package. This package can be installed using pip...\n\n```\npip install plotly-geo==1.0.0\n```\n\nor conda\n\n```\nconda install -c plotly plotly-geo=1.0.0\n```\n\n### Chart Studio support\n\nThe `chart-studio` package can be used to upload plotly figures to Plotly's Chart\nStudio Cloud or On-Prem service. This package can be installed using pip...\n\n```\npip install chart-studio==1.0.0\n```\n\nor conda\n\n```\nconda install -c plotly chart-studio=1.0.0\n```\n\n## Migration\n\nIf you're migrating from plotly.py v3 to v4, please check out the [Version 4 migration guide](https://plot.ly/python/next/v4-migration/)\n\nIf you're migrating from plotly.py v2 to v3, please check out the [Version 3 migration guide](migration-guide.md)\n\n## Copyright and Licenses\n\nCode and documentation copyright 2019 Plotly, Inc.\n\nCode released under the [MIT license](packages/python/chart-studio/LICENSE.txt).\n\nDocs released under the [Creative Commons license](https://github.com/plotly/documentation/blob/source/LICENSE).\n"}, "pomegranate": {"file_name": "jmschrei/pomegranate/README.md", "raw_text": "<img src=\"https://github.com/jmschrei/pomegranate/blob/master/docs/logo/pomegranate-logo.png\" width=300>\n\n[![Build Status](https://travis-ci.org/jmschrei/pomegranate.svg?branch=master)](https://travis-ci.org/jmschrei/pomegranate) ![Build Status](https://ci.appveyor.com/api/projects/status/github/jmschrei/pomegranate?svg=True) [![Documentation Status](https://readthedocs.org/projects/pomegranate/badge/?version=latest)](http://pomegranate.readthedocs.io/en/latest/?badge=latest) [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jmschrei/pomegranate/master)\n\nPlease consider citing the [**JMLR-MLOSS Manuscript**](http://jmlr.org/papers/volume18/17-636/17-636.pdf) if you've used pomegranate in your academic work!\n\npomegranate is a package for building probabilistic models in Python that is implemented in Cython for speed. A primary focus of pomegranate is to merge the easy-to-use API of scikit-learn with the modularity of probabilistic modeling to allow users to specify complicated models without needing to worry about implementation details. The models implemented here are built from the ground up with big data processing in mind and so natively support features like multi-threaded parallelism and out-of-core processing. Click on the binder badge above to interactively play with the tutorials!\n\n### Installation\n\npomegranate is pip-installable using `pip install pomegranate` and conda-installable using `conda install pomegranate`. If neither work, more detailed installation instructions can be found [here](http://pomegranate.readthedocs.io/en/latest/install.html).\n\n### Models\n\n* [Probability Distributions](http://pomegranate.readthedocs.io/en/latest/Distributions.html)\n* [General Mixture Models](http://pomegranate.readthedocs.io/en/latest/GeneralMixtureModel.html)\n* [Hidden Markov Models](http://pomegranate.readthedocs.io/en/latest/HiddenMarkovModel.html)\n* [Naive Bayes and Bayes Classifiers](http://pomegranate.readthedocs.io/en/latest/NaiveBayes.html)\n* [Markov Chains](http://pomegranate.readthedocs.io/en/latest/MarkovChain.html)\n* [Discrete Bayesian Networks](http://pomegranate.readthedocs.io/en/latest/BayesianNetwork.html)\n* [Discrete Markov Networks](https://pomegranate.readthedocs.io/en/latest/MarkovNetwork.html)\n\nThe discrete Bayesian networks also support novel work on structure learning in the presence of constraints through a constraint graph. These constraints can dramatically speed up structure learning through the use of loose general prior knowledge, and can frequently make the exact learning task take only polynomial time instead of exponential time. See the [PeerJ manuscript](https://peerj.com/articles/cs-122/) for the theory and the [pomegranate tutorial](https://github.com/jmschrei/pomegranate/blob/master/tutorials/B_Model_Tutorial_4b_Bayesian_Network_Structure_Learning.ipynb) for the practical usage! \n\nTo support the above algorithms, it has efficient implementations of the following:\n\n* Kmeans/Kmeans++/Kmeans||\n* Factor Graphs\n\n### Features\n\n* [sklearn-like API](https://pomegranate.readthedocs.io/en/latest/api.html)\n* [Multi-threaded Training](http://pomegranate.readthedocs.io/en/latest/parallelism.html)\n* [BLAS/GPU Acceleration](http://pomegranate.readthedocs.io/en/latest/gpu.html)\n* [Out-of-Core Learning](http://pomegranate.readthedocs.io/en/latest/ooc.html)\n* [Data Generators and IO](https://pomegranate.readthedocs.io/en/latest/io.html)\n* [Semi-supervised Learning](http://pomegranate.readthedocs.io/en/latest/semisupervised.html)\n* [Missing Value Support](http://pomegranate.readthedocs.io/en/latest/nan.html)\n* [Customized Callbacks](http://pomegranate.readthedocs.io/en/latest/callbacks.html)\n\nPlease take a look at the [tutorials folder](https://github.com/jmschrei/pomegranate/tree/master/tutorials), which includes several tutorials on how to effectively use pomegranate!\n\nSee [the website](http://pomegranate.readthedocs.org/en/latest/) for extensive documentation, API references, and FAQs about each of the models and supported features.\n\nNo good project is done alone, and so I'd like to thank all the previous contributors to YAHMM, and all the current contributors to pomegranate, including the graduate students who share my office I annoy on a regular basis by bouncing ideas off of.\n\n### Dependencies\n\npomegranate requires:\n\n```\n- Cython (only if building from source)\n- NumPy\n- SciPy\n- NetworkX\n- joblib\n```\n\nTo run the tests, you also must have `nose` installed.\n\n## Contributing\n\nIf you would like to contribute a feature then fork the master branch (fork the release if you are fixing a bug). Be sure to run the tests before changing any code. You'll need to have [nosetests](https://github.com/nose-devs/nose) installed. The following command will run all the tests:\n\n```\npython setup.py test\n```\n\nLet us know what you want to do just in case we're already working on an implementation of something similar. This way we can avoid any needless duplication of effort. Also, please don't forget to add tests for any new functions.\n\n"}, "protobuf": {"file_name": "protocolbuffers/protobuf/README.md", "raw_text": "Protocol Buffers - Google's data interchange format\n===================================================\n\nCopyright 2008 Google Inc.\n\nhttps://developers.google.com/protocol-buffers/\n\nOverview\n--------\n\nProtocol Buffers (a.k.a., protobuf) are Google's language-neutral,\nplatform-neutral, extensible mechanism for serializing structured data. You\ncan find [protobuf's documentation on the Google Developers site](https://developers.google.com/protocol-buffers/).\n\nThis README file contains protobuf installation instructions. To install\nprotobuf, you need to install the protocol compiler (used to compile .proto\nfiles) and the protobuf runtime for your chosen programming language.\n\nProtocol Compiler Installation\n------------------------------\n\nThe protocol compiler is written in C++. If you are using C++, please follow\nthe [C++ Installation Instructions](src/README.md) to install protoc along\nwith the C++ runtime.\n\nFor non-C++ users, the simplest way to install the protocol compiler is to\ndownload a pre-built binary from our release page:\n\n  [https://github.com/protocolbuffers/protobuf/releases](https://github.com/protocolbuffers/protobuf/releases)\n\nIn the downloads section of each release, you can find pre-built binaries in\nzip packages: protoc-$VERSION-$PLATFORM.zip. It contains the protoc binary\nas well as a set of standard .proto files distributed along with protobuf.\n\nIf you are looking for an old version that is not available in the release\npage, check out the maven repo here:\n\n  [https://repo1.maven.org/maven2/com/google/protobuf/protoc/](https://repo1.maven.org/maven2/com/google/protobuf/protoc/)\n\nThese pre-built binaries are only provided for released versions. If you want\nto use the github master version at HEAD, or you need to modify protobuf code,\nor you are using C++, it's recommended to build your own protoc binary from\nsource.\n\nIf you would like to build protoc binary from source, see the [C++ Installation\nInstructions](src/README.md).\n\nProtobuf Runtime Installation\n-----------------------------\n\nProtobuf supports several different programming languages. For each programming\nlanguage, you can find instructions in the corresponding source directory about\nhow to install protobuf runtime for that specific language:\n\n| Language                             | Source                                                      | Ubuntu | MacOS | Windows |\n|--------------------------------------|-------------------------------------------------------------|--------|-------|---------|\n| C++ (include C++ runtime and protoc) | [src](src)                                                  | [![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-cpp_distcheck.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fcpp_distcheck%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-bazel.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fbazel%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-dist_install.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fdist_install%2Fcontinuous) | [![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-cpp.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fcpp%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-cpp_distcheck.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fcpp_distcheck%2Fcontinuous) | [![Build status](https://ci.appveyor.com/api/projects/status/73ctee6ua4w2ruin?svg=true)](https://ci.appveyor.com/project/protobuf/protobuf) |\n| Java                                 | [java](java)                                                | [![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-java_compatibility.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fjava_compatibility%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-java_jdk7.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fjava_jdk7%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-java_oracle7.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fjava_oracle7%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-java_linkage_monitor.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fjava_linkage_monitor%2Fcontinuous) | | |\n| Python                               | [python](python)                                            | [![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-python27.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fpython27%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-python33.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fpython33%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-python34.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fpython34%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-python35.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fpython35%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-python36.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fpython36%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-python37.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fpython37%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-python_compatibility.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fpython_compatibility%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-python27_cpp.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fpython27_cpp%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-python33_cpp.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fpython33_cpp%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-python34_cpp.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fpython34_cpp%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-python35_cpp.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fpython35_cpp%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-python36_cpp.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fpython36_cpp%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-python37_cpp.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fpython37_cpp%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-python-release.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fpython_release%2Fcontinuous) | [![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-python.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fpython%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-python_cpp.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fpython_cpp%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-python-release.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fpython_release%2Fcontinuous) | [![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/windows-python-release.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fwindows%2Fpython_release%2Fcontinuous) |\n| Objective-C                          | [objectivec](objectivec)                                    | | [![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-objectivec_cocoapods_integration.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fobjectivec_cocoapods_integration%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-objectivec_ios_debug.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fobjectivec_ios_debug%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-objectivec_ios_release.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fobjectivec_ios_release%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-objectivec_osx.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fobjectivec_osx%2Fcontinuous) | |\n| C#                                   | [csharp](csharp)                                            | [![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-csharp.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fcsharp%2Fcontinuous) | | [![Build status](https://ci.appveyor.com/api/projects/status/73ctee6ua4w2ruin?svg=true)](https://ci.appveyor.com/project/protobuf/protobuf)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/windows-csharp-release.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fwindows%2Fcsharp_release%2Fcontinuous) |\n| JavaScript                           | [js](js)                                                    | [![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-javascript.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fjavascript%2Fcontinuous) | [![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-javascript.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fjavascript%2Fcontinuous) | |\n| Ruby                                 | [ruby](ruby)                                                | [![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-ruby23.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fruby23%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-ruby24.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fruby24%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-ruby25.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fruby25%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-ruby26.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fruby26%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-ruby-release.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fruby_release%2Fcontinuous) | [![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-ruby23.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fruby23%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-ruby24.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fruby24%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-ruby25.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fruby25%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-ruby26.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fruby26%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-ruby-release.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fruby_release%2Fcontinuous) | |\n| Go                                   | [golang/protobuf](https://github.com/golang/protobuf)       | | | |\n| PHP                                  | [php](php)                                                  | [![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-php_all.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2Fphp_all%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/linux-32-bit.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fubuntu%2F32-bit%2Fcontinuous) | [![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-php5.6_mac.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fphp5.6_mac%2Fcontinuous)<br/>[![Build status](https://storage.googleapis.com/protobuf-kokoro-results/status-badge/macos-php7.0_mac.png)](https://fusion.corp.google.com/projectanalysis/current/KOKORO/prod:protobuf%2Fgithub%2Fmaster%2Fmacos%2Fphp7.0_mac%2Fcontinuous) | |\n| Dart                                 | [dart-lang/protobuf](https://github.com/dart-lang/protobuf) | [![Build Status](https://travis-ci.org/dart-lang/protobuf.svg?branch=master)](https://travis-ci.org/dart-lang/protobuf) | | |\n\nQuick Start\n-----------\n\nThe best way to learn how to use protobuf is to follow the tutorials in our\ndeveloper guide:\n\nhttps://developers.google.com/protocol-buffers/docs/tutorials\n\nIf you want to learn from code examples, take a look at the examples in the\n[examples](examples) directory.\n\nDocumentation\n-------------\n\nThe complete documentation for Protocol Buffers is available via the\nweb at:\n\nhttps://developers.google.com/protocol-buffers/\n"}, "publish": {"file_name": "fiaas/publish/README.rst", "raw_text": "..\n  Copyright 2017-2019 The FIAAS Authors\n\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n\npublish - Tool to create a release of a Python package\n======================================================\n\n|Codacy Quality Badge| |Codacy Coverage Badge|\n\n\n.. |Codacy Quality Badge| image:: https://api.codacy.com/project/badge/Grade/bd7d31c7ceac43eb81884b2adc4ba3ed\n    :target: https://www.codacy.com/app/fiaas/publish?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=fiaas/publish&amp;utm_campaign=Badge_Grade\n.. |Codacy Coverage Badge| image:: https://api.codacy.com/project/badge/Coverage/bd7d31c7ceac43eb81884b2adc4ba3ed\n    :target: https://www.codacy.com/app/fiaas/publish?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=fiaas/publish&amp;utm_campaign=Badge_Coverage\n\npublish is a tool to package and release a python project. It will create a changelog and upload artifacts to Github and PyPI.\n\nIt is created for and by the `FIAAS project`_, and used for most of our projects.\n\n.. _`FIAAS project`: https://github.com/fiaas\n\n\nUsage\n-----\n\nIn order to use publish, you must first install it::\n\n    pip install publish\n\n\nUnder the covers, publish uses github-release_ and twine_ to do most of the work, and those tools require credentials for Github and PyPI to be available in environment variables::\n\n    export GITHUB_TOKEN=gh-token\n    export TWINE_USERNAME=pypi-user\n    export TWINE_PASSWORD=pypi-pass\n\nIn order to know where to upload the artifacts, you must specify an organization, and a repository::\n\n    publish fiaas k8s\n\n\nBefore uploading anything, publish will verify that the current checkout is suitable to be released, and checks the following items:\n\n* Are all files either ignored or in version control?\n* Is every change committed?\n* Is the currently checked out code tagged with an annotated tag?\n* Does that tag use the convention ``v<major>.<minor>.<bugfix>``?\n\nIf the answer to all of these is yes, the name of the tag is used as the version to release. A changelog is generated from the git log, source tarballs and wheels are built, the release is created in Github and PyPI, and the files are uploaded.\n\nWhen uploading a release to Github, the changelog is attached to the release automatically.\n\nIn order for the changelog to be attached to the release on PyPI, it needs to be included in the long description generated by ``setup.py``. To help with this, the changelog is written to a file, and the name of the file is available in an environment variable called ``CHANGELOG_FILE``. Append the contents of this file to your long description, and it will be included in the description on PyPI.\n\n.. _github-release: https://github.com/j0057/github-release\n.. _twine: https://github.com/pypa/twine\n"}, "pyarrow": {"file_name": "apache/arrow/README.md", "raw_text": "<!---\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\n# Apache Arrow\n\n[![Build Status](https://ci.appveyor.com/api/projects/status/github/apache/arrow/branch/master?svg=true)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/branch/master)\n[![Coverage Status](https://codecov.io/gh/apache/arrow/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/arrow?branch=master)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/arrow.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:arrow)\n[![License](http://img.shields.io/:license-Apache%202-blue.svg)](https://github.com/apache/arrow/blob/master/LICENSE.txt)\n[![Twitter Follow](https://img.shields.io/twitter/follow/apachearrow.svg?style=social&label=Follow)](https://twitter.com/apachearrow)\n\n## Powering In-Memory Analytics\n\nApache Arrow is a development platform for in-memory analytics. It contains a\nset of technologies that enable big data systems to process and move data fast.\n\nMajor components of the project include:\n\n - [The Arrow Columnar In-Memory Format](https://github.com/apache/arrow/tree/master/format)\n - [C++ libraries](https://github.com/apache/arrow/tree/master/cpp)\n - [C bindings using GLib](https://github.com/apache/arrow/tree/master/c_glib)\n - [C# .NET libraries](https://github.com/apache/arrow/tree/master/csharp)\n - [Gandiva](https://github.com/apache/arrow/tree/master/cpp/src/gandiva): an [LLVM](https://llvm.org)-based Arrow expression compiler, part of the C++ codebase\n - [Go libraries](https://github.com/apache/arrow/tree/master/go)\n - [Java libraries](https://github.com/apache/arrow/tree/master/java)\n - [JavaScript libraries](https://github.com/apache/arrow/tree/master/js)\n - [Plasma Object Store](https://github.com/apache/arrow/tree/master/cpp/src/plasma): a\n   shared-memory blob store, part of the C++ codebase\n - [Python libraries](https://github.com/apache/arrow/tree/master/python)\n - [R libraries](https://github.com/apache/arrow/tree/master/r)\n - [Ruby libraries](https://github.com/apache/arrow/tree/master/ruby)\n - [Rust libraries](https://github.com/apache/arrow/tree/master/rust)\n\nArrow is an [Apache Software Foundation](https://www.apache.org) project. Learn more at\n[arrow.apache.org](https://arrow.apache.org).\n\n## What's in the Arrow libraries?\n\nThe reference Arrow libraries contain a number of distinct software components:\n\n- Columnar vector and table-like containers (similar to data frames) supporting\n  flat or nested types\n- Fast, language agnostic metadata messaging layer (using Google's Flatbuffers\n  library)\n- Reference-counted off-heap buffer memory management, for zero-copy memory\n  sharing and handling memory-mapped files\n- IO interfaces to local and remote filesystems\n- Self-describing binary wire formats (streaming and batch/file-like) for\n  remote procedure calls (RPC) and\n  interprocess communication (IPC)\n- Integration tests for verifying binary compatibility between the\n  implementations (e.g. sending data from Java to C++)\n- Conversions to and from other in-memory data structures\n\n## How to Contribute\n\nPlease read our latest [project contribution guide][5].\n\n## Getting involved\n\nEven if you do not plan to contribute to Apache Arrow itself or Arrow\nintegrations in other projects, we'd be happy to have you involved:\n\n- Join the mailing list: send an email to\n  [dev-subscribe@arrow.apache.org][1]. Share your ideas and use cases for the\n  project.\n- [Follow our activity on JIRA][3]\n- [Learn the format][2]\n- Contribute code to one of the reference implementations\n\n[1]: mailto:dev-subscribe@arrow.apache.org\n[2]: https://github.com/apache/arrow/tree/master/format\n[3]: https://issues.apache.org/jira/browse/ARROW\n[4]: https://github.com/apache/arrow\n[5]: https://github.com/apache/arrow/blob/master/docs/source/developers/contributing.rst\n"}, "pydot": {"file_name": "pydot/pydot/README.md", "raw_text": "[![Build Status](https://www.travis-ci.com/pydot/pydot.svg?branch=master)](https://www.travis-ci.com/pydot/pydot)\n[![PyPI](https://img.shields.io/pypi/v/pydot.svg)](https://pypi.org/project/pydot/)\n\n\nAbout\n=====\n\n`pydot`:\n\n  - is an interface to [Graphviz][1]\n  - can parse and dump into the [DOT language][2] used by GraphViz,\n  - is written in pure Python,\n\nand [`networkx`][3] can convert its graphs to `pydot`.\nDevelopment occurs at [GitHub][11] (under branch `dev`),\nwhere you can report issues and contribute code.\n\n\nInstallation\n============\n\nFrom [PyPI][4] using [`pip`][5]:\n\n`pip install pydot`\n\nFrom source:\n\n`python setup.py install`\n\n\nDependencies\n============\n\n- [`pyparsing`][6]: used only for *loading* DOT files,\n  installed automatically during `pydot` installation.\n\n- GraphViz: used to render graphs as PDF, PNG, SVG, etc.\n  Should be installed separately, using your system's\n  [package manager][7], something similar (e.g., [MacPorts][8]),\n  or from [its source][9].\n\n\nLicense\n=======\n\nDistributed under an [MIT license][10].\n\n[1]: https://www.graphviz.org\n[2]: https://en.wikipedia.org/wiki/DOT_%28graph_description_language%29\n[3]: https://github.com/networkx/networkx\n[4]: https://pypi.python.org/pypi\n[5]: https://github.com/pypa/pip\n[6]: https://github.com/pyparsing/pyparsing\n[7]: https://en.wikipedia.org/wiki/Package_manager\n[8]: https://www.macports.org\n[9]: https://github.com/ellson/graphviz\n[10]: https://github.com/pydot/pydot/blob/master/LICENSE\n[11]: https://github.com/pydot/pydot\n"}, "pyforest": {"file_name": "8080labs/pyforest/README.md", "raw_text": "# pyforest - feel the bliss of automated imports\n\n### From the makers of [bamboolib](https://bamboolib.com)\n\nWriting the same imports over and over again is below your capacity. Let pyforest do the job for you.\n\n\nWith pyforest you can use all your favorite Python libraries without importing them before.\nIf you use a package that is not imported yet, pyforest imports the package for you and adds the code to the first Jupyter cell. If you don't use a library, it won't be imported.\n\n\n- [Demo in Jupyter Notebook](#demo-in-jupyter-notebook)\n- [Scenario](#scenario)\n- [Using pyforest](#using-pyforest)\n- [Installation](#installation)\n- [FAQs](#frequently-asked-questions)\n- [Contributing](#contributing)\n- [About](#about)\n\n\n\n## Demo in Jupyter Notebook\n![demo](examples/assets/pyforest_demo_in_jupyter_notebook.gif)\n\n\n## Scenario\n\nYou are a Data Scientist who works with Python. Every day you start multiple new Jupyter notebooks because you want to explore some data or validate a hypothesis.\n\nDuring your work, you use many different libraries like `pandas`, `matplotlib`, `seaborn`, `numpy` or `sklearn`. However, before you can start with the actual work, you always need to import your libraries.\n\n\nThere are several __problems__ with this. Admittedly, they are small but they add up over time.\n- It is boring because the imports are mostly the same. This is below your capacity.\n- Missing imports disrupt the natural flow of your work.\n- Sometimes, you may even need to look up the exact import statements. For example, `import matplotlib.pyplot as plt` or `from sklearn.ensemble import GradientBoostingRegressor`\n\n__What if you could just focus on using the libraries?__\n\npyforest offers the following __solution__:\n- You can use all your libraries like you usually do. If a library is not imported yet, pyforest will import it and add the import statement to the first Jupyter cell.\n- If a library is not used, it won't be imported.\n- Your notebooks stay reproducible and sharable without you wasting a thought on imports.\n\n\n## Using pyforest\n\nAfter you [installed](#installation) pyforest and its Jupyter extension, you can __use your favorite Python Data Science commands like you normally would - just without writing imports__.\n\nFor example, if you want to read a CSV with pandas:\n\n```python\ndf = pd.read_csv(\"titanic.csv\")\n```\n\npyforest will automatically import pandas for you and add the import statement to the first cell:\n```python\nimport pandas as pd\n```\n\n\n__Which libraries are available?__\n- We aim to add all popular Python Data Science libraries which should account for >99% of your daily imports. For example, we already added `pandas` as `pd`, `numpy` as `np`, `seaborn` as `sns`, `matplotlib.pyplot` as `plt`, or `OneHotEncoder` from `sklearn` and many more. In addition, there are also helper modules like `os`, `re`, `tqdm`, or `Path` from `pathlib`.\n- You can see an overview of all currently available imports [here](src/pyforest/_imports.py)\n- If you are missing an import, you can either __add the import to your user specific pyforest imports__ as described in the [FAQs](#frequently-asked-questions) or you can open a pull request for the official [pyforest imports](src/pyforest/_imports.py)\n\n> In order to gather all the most important names, we need your help. Please open a pull request and add the [imports](src/pyforest/_imports.py) that we are still missing.\n\n\n## Installation\n\n> You need Python 3.6 or above because we love f-strings.\n\nFrom the terminal (or Anaconda prompt in Windows), enter:\n\n```bash\npip install --upgrade pyforest\npython -m pyforest install_extensions\n```\n\nPlease make sure to restart any running Jupyter server so that the javascript extension can be loaded properly.\n\nAlso, please note that this will add pyforest to your IPython default startup settings. If you do not want this, you can disable the auto_import as described in the [FAQs](#frequently-asked-questions) below.\n\n\n## Frequently Asked Questions\n\n- __\"How to add my own import statements without adding them to the package source code?\"__\n    - pyforest creates a file in your home directory at `~/.pyforest/user_imports.py` in which you can type any **explicit** import statements you want (e.g. `import pandas as pd`). Your own custom imports take precedence over any other pyforest imports. **Please note:** implicit imports (e.g. `from pandas import *`) won't work.\n\n- __\"Doesn't this slow down my Jupyter or Python startup process?\"__\n    - No, because the libraries will only be imported when you actually use them. Until you use them, the variables like `pd` are only pyforest placeholders.\n\n- __\"Why can't I just use the typical IPython import?\"__\n    - If you were to add all the libraries that pyforest includes, your startup time might take more than 30s.\n\n- __\"I don't have and don't need tensorflow. What will happen when I use pyforest?\"__\n    - Tensorflow is included in pyforest but pyforest does not install any dependencies. You need to install your libraries separately from pyforest. Afterwards, you can access the libraries via pyforest if they are included in the [pyforest imports](src/pyforest/_imports.py).\n\n- __\"Will the pyforest variables interfere with my own local variables?\"__\n    - No, never. pyforest will never mask or overwrite any of your local variables. You can use your variables like you would without pyforest. The worst thing that can happen is that you overwrite a pyforest placeholder and thus cannot use the placeholder any more (duh).\n\n- __\"What about auto-completion on lazily imported modules?\"__\n    - It works :) As soon as you start the auto-completion, pyforest will import the module and return the available symbols to your auto-completer.\n\n- __\"How to (temporarily) deactivate the auto_import in IPython and Jupyter?\"__\n    - Go to the directory `~/.ipython/profile_default/startup` and adjust or delete the `pyforest_autoimport.py` file. You will find further instructions in the file. If you don't use the auto_import, you will need to import pyforest at the beginning of your notebook via `import pyforest`\n\n- __\"How to (re)activate the pyforest auto_import?\"__\n    - Execute the following Python command in Jupyter, IPython or Python: `from pyforest.auto_import import setup; setup()`. Please note that the auto_import only works for Jupyter and IPython.\n\n- __\"Can I use pyforest outside of the Jupyter Notebook or Lab?\"__\n    - Technically, yes. However, this is not the intended use case. pyforest is aimed primarily for the use in a Jupyter Notebook or Lab. If you want to use pyforest in IPython or a Python script etc, please import it as follows `import pyforest`. Afterwards, you can get the currently active imports via `pyforest.active_imports()`\n\n- __\"Why is the project called pyforest?\"__\n    - pyforest is created to be the home for all Data Science packages - including pandas. And in which ecosystems do pandas live? :)\n\n\n## Contributing\nIf you'd like to contribute, a great place to look is the [issues marked with help-wanted](https://github.com/8080labs/pyforest/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22).\n\nIn order to gather all the most important names, we need your help. Please open a pull request and add the imports that we are still missing to the [pyforest imports](src/pyforest/_imports.py). You can also find the guidelines in the [pyforest imports file](src/pyforest/_imports.py)\n\n\n## About\npyforest is developed by [8080 Labs](https://8080labs.com). Our goal is to make Python Data Scientists 10x faster. If you like the speedup to your workflow, you might also be interested in our other project [bamboolib](https://bamboolib.com)\n"}, "pymc3": {"file_name": "pymc-devs/pymc3/README.rst", "raw_text": ".. image:: https://cdn.rawgit.com/pymc-devs/pymc3/master/docs/logos/svg/PyMC3_banner.svg\n    :height: 100px\n    :alt: PyMC3 logo\n    :align: center\n\n|Build Status| |Coverage| |NumFOCUS_badge| |Binder| |Dockerhub|\n\nPyMC3 is a Python package for Bayesian statistical modeling and Probabilistic Machine Learning\nfocusing on advanced Markov chain Monte Carlo (MCMC) and variational inference (VI)\nalgorithms. Its flexibility and extensibility make it applicable to a\nlarge suite of problems.\n\nCheck out the `getting started guide <http://docs.pymc.io/notebooks/getting_started>`__,  or\n`interact with live examples <https://mybinder.org/v2/gh/pymc-devs/pymc3/master?filepath=%2Fdocs%2Fsource%2Fnotebooks>`__\nusing Binder!\n\nFeatures\n========\n\n-  Intuitive model specification syntax, for example, ``x ~ N(0,1)``\n   translates to ``x = Normal('x',0,1)``\n-  **Powerful sampling algorithms**, such as the `No U-Turn\n   Sampler <http://www.jmlr.org/papers/v15/hoffman14a.html>`__, allow complex models\n   with thousands of parameters with little specialized knowledge of\n   fitting algorithms.\n-  **Variational inference**: `ADVI <http://www.jmlr.org/papers/v18/16-107.html>`__\n   for fast approximate posterior estimation as well as mini-batch ADVI\n   for large data sets.\n-  Relies on `Theano <http://deeplearning.net/software/theano/>`__ which provides:\n    *  Computation optimization and dynamic C compilation\n    *  Numpy broadcasting and advanced indexing\n    *  Linear algebra operators\n    *  Simple extensibility\n-  Transparent support for missing value imputation\n\nGetting started\n===============\n\nIf you already know about Bayesian statistics:\n----------------------------------------------\n\n\n-  `API quickstart guide <http://docs.pymc.io/notebooks/api_quickstart>`__\n-  The `PyMC3 tutorial <http://docs.pymc.io/notebooks/getting_started>`__\n-  `PyMC3 examples <https://docs.pymc.io/nb_examples/index.html>`__ and the `API reference <http://docs.pymc.io/api>`__\n\n\n\nLearn Bayesian statistics with a book together with PyMC3:\n----------------------------------------------------------\n\n-  `Probabilistic Programming and Bayesian Methods for Hackers <https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers>`__: Fantastic book with many applied code examples.\n-  `PyMC3 port of the book \"Doing Bayesian Data Analysis\" by John Kruschke <https://github.com/aloctavodia/Doing_bayesian_data_analysis>`__ as well as the `second edition <https://github.com/JWarmenhoven/DBDA-python>`__: Principled introduction to Bayesian data analysis.\n-  `PyMC3 port of the book \"Statistical Rethinking A Bayesian Course with Examples in R and Stan\" by Richard McElreath <https://github.com/pymc-devs/resources/tree/master/Rethinking>`__\n-  `PyMC3 port of the book \"Bayesian Cognitive Modeling\" by Michael Lee and EJ Wagenmakers <https://github.com/pymc-devs/resources/tree/master/BCM>`__: Focused on using Bayesian statistics in cognitive modeling.\n-  `Bayesian Analysis with Python  <https://www.packtpub.com/big-data-and-business-intelligence/bayesian-analysis-python-second-edition>`__ (second edition) by Osvaldo Martin: Great introductory book. (`code <https://github.com/aloctavodia/BAP>`__ and errata).\n\nPyMC3 talks\n-----------\n\nThere are also several talks on PyMC3 which are gathered in this `YouTube playlist <https://www.youtube.com/playlist?list=PL1Ma_1DBbE82OVW8Fz_6Ts1oOeyOAiovy>`__\n\nInstallation\n============\n\nThe latest release of PyMC3 can be installed from PyPI using ``pip``:\n\n::\n\n    pip install pymc3\n\n**Note:** Running ``pip install pymc`` will install PyMC 2.3, not PyMC3,\nfrom PyPI.\n\nOr via conda-forge:\n\n::\n\n    conda install -c conda-forge pymc3\n\nPlotting is done using `ArviZ <https://arviz-devs.github.io/arviz/>`__\nwhich may be installed separately, or along with PyMC3:\n\n::\n\n    pip install pymc3[plots]\n\nThe current development branch of PyMC3 can be installed from GitHub, also using ``pip``:\n\n::\n\n    pip install git+https://github.com/pymc-devs/pymc3\n\nTo ensure the development branch of Theano is installed alongside PyMC3\n(recommended), you can install PyMC3 using the ``requirements.txt``\nfile. This requires cloning the repository to your computer:\n\n::\n\n    git clone https://github.com/pymc-devs/pymc3\n    cd pymc3\n    pip install -r requirements.txt\n\nHowever, if a recent version of Theano has already been installed on\nyour system, you can install PyMC3 directly from GitHub.\n\nAnother option is to clone the repository and install PyMC3 using\n``python setup.py install`` or ``python setup.py develop``.\n\n\nDependencies\n============\n\nPyMC3 is tested on Python 3.6 and depends on Theano, NumPy,\nSciPy, and Pandas (see ``requirements.txt`` for version\ninformation).\n\nOptional\n--------\n\nIn addtion to the above dependencies, the GLM submodule relies on\n`Patsy <http://patsy.readthedocs.io/en/latest/>`__.\n\n\nCiting PyMC3\n============\n\nSalvatier J., Wiecki T.V., Fonnesbeck C. (2016) Probabilistic programming\nin Python using PyMC3. PeerJ Computer Science 2:e55\n`DOI: 10.7717/peerj-cs.55 <https://doi.org/10.7717/peerj-cs.55>`__.\n\nContact\n=======\n\nWe are using `discourse.pymc.io <https://discourse.pymc.io/>`__ as our main communication channel. You can also follow us on `Twitter @pymc_devs <https://twitter.com/pymc_devs>`__ for updates and other announcements.\n\nTo ask a question regarding modeling or usage of PyMC3 we encourage posting to our Discourse forum under the `\u201cQuestions\u201d Category <https://discourse.pymc.io/c/questions>`__. You can also suggest feature in the `\u201cDevelopment\u201d Category <https://discourse.pymc.io/c/development>`__.\n\nTo report an issue with PyMC3 please use the `issue tracker <https://github.com/pymc-devs/pymc3/issues>`__.\n\nFinally, if you need to get in touch for non-technical information about the project, `send us an e-mail <pymc.devs@gmail.com>`__.\n\nLicense\n=======\n\n`Apache License, Version\n2.0 <https://github.com/pymc-devs/pymc3/blob/master/LICENSE>`__\n\n\nSoftware using PyMC3\n====================\n\n- `Exoplanet <https://github.com/dfm/exoplanet>`__: a toolkit for modeling of transit and/or radial velocity observations of exoplanets and other astronomical time series.\n- `Bambi <https://github.com/bambinos/bambi>`__: BAyesian Model-Building Interface (BAMBI) in Python.\n- `pymc3_models <https://github.com/parsing-science/pymc3_models>`__: Custom PyMC3 models built on top of the scikit-learn API.\n- `PMProphet <https://github.com/luke14free/pm-prophet>`__: PyMC3 port of Facebook's Prophet model for timeseries modeling\n- `webmc3 <https://github.com/AustinRochford/webmc3>`__: A web interface for exploring PyMC3 traces\n- `sampled <https://github.com/ColCarroll/sampled>`__: Decorator for PyMC3 models.\n- `NiPyMC <https://github.com/PsychoinformaticsLab/nipymc>`__: Bayesian mixed-effects modeling of fMRI data in Python.\n- `beat <https://github.com/hvasbath/beat>`__: Bayesian Earthquake Analysis Tool.\n\nPlease contact us if your software is not listed here.\n\nPapers citing PyMC3\n===================\n\nSee `Google Scholar <https://scholar.google.de/scholar?oi=bibs&hl=en&authuser=1&cites=6936955228135731011>`__ for a continuously updated list.\n\nContributors\n============\n\nSee the `GitHub contributor\npage <https://github.com/pymc-devs/pymc3/graphs/contributors>`__\n\nSupport\n=======\n\nPyMC3 is a non-profit project under NumFOCUS umbrella. If you want to support PyMC3 financially, you can donate `here <https://numfocus.salsalabs.org/donate-to-pymc3/index.html>`__.\n\nSponsors\n========\n\n|NumFOCUS|\n\n|Quantopian|\n\n|ODSC|\n\n.. |Binder| image:: https://mybinder.org/badge_logo.svg\n   :target: https://mybinder.org/v2/gh/pymc-devs/pymc3/master?filepath=%2Fdocs%2Fsource%2Fnotebooks\n.. |Build Status| image:: https://travis-ci.org/pymc-devs/pymc3.svg?branch=master\n   :target: https://travis-ci.org/pymc-devs/pymc3\n.. |Coverage| image:: https://codecov.io/gh/pymc-devs/pymc3/branch/master/graph/badge.svg\n  :target: https://codecov.io/gh/pymc-devs/pymc3\n.. |Dockerhub| image:: https://img.shields.io/docker/automated/pymc/pymc3.svg\n  :target: https://hub.docker.com/r/pymc/pymc3\n.. |NumFOCUS| image:: https://www.numfocus.org/wp-content/uploads/2017/03/1457562110.png\n   :target: http://www.numfocus.org/\n.. |Quantopian| image:: https://raw.githubusercontent.com/pymc-devs/pymc3/master/docs/quantopianlogo.jpg\n   :target: https://quantopian.com\n.. |NumFOCUS_badge| image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A\n   :target: http://www.numfocus.org/\n.. |ODSC| image:: https://raw.githubusercontent.com/pymc-devs/pymc3/master/docs/odsc_logo.png\n   :target: https://odsc.com\n"}, "pyro": {"file_name": "pyro-ppl/pyro/README.md", "raw_text": "<div align=\"center\">\n  <a href=\"http://pyro.ai\"> <img width=\"220px\" height=\"220px\" src=\"docs/source/_static/img/pyro_logo_with_text.png\"></a>\n</div>\n\n-----------------------------------------\n\n[![Build Status](https://travis-ci.com/pyro-ppl/pyro.svg?branch=dev)](https://travis-ci.com/pyro-ppl/pyro)\n[![codecov.io](https://codecov.io/github/pyro-ppl/pyro/branch/dev/graph/badge.svg)](https://codecov.io/github/pyro-ppl/pyro)\n[![Latest Version](https://badge.fury.io/py/pyro-ppl.svg)](https://pypi.python.org/pypi/pyro-ppl)\n[![Documentation Status](https://readthedocs.org/projects/pyro-ppl/badge/?version=dev)](http://pyro-ppl.readthedocs.io/en/stable/?badge=dev)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3056/badge)](https://bestpractices.coreinfrastructure.org/projects/3056)\n\n[Getting Started](http://pyro.ai/examples) |\n[Documentation](http://docs.pyro.ai/) |\n[Community](http://forum.pyro.ai/) |\n[Contributing](https://github.com/pyro-ppl/pyro/blob/master/CONTRIBUTING.md)\n\nPyro is a flexible, scalable deep probabilistic programming library built on PyTorch.  Notably, it was designed with these principles in mind:\n\n- **Universal**: Pyro is a universal PPL - it can represent any computable probability distribution.\n- **Scalable**: Pyro scales to large data sets with little overhead compared to hand-written code.\n- **Minimal**: Pyro is agile and maintainable. It is implemented with a small core of powerful, composable abstractions.\n- **Flexible**: Pyro aims for automation when you want it, control when you need it. This is accomplished through high-level abstractions to express generative and inference models, while allowing experts easy-access to customize inference.\n\nPyro is developed and maintained by [Uber AI Labs](http://uber.ai) and community contributors.\nFor more information, check out our [blog post](http://eng.uber.com/pyro).\n\n## Installing\n\n### Installing a stable Pyro release\n\n**Install using pip:**\n\nPyro supports Python 3.4+.\n\n```sh\npip install pyro-ppl\n```\n\n**Install from source:**\n```sh\ngit clone git@github.com:pyro-ppl/pyro.git\ncd pyro\ngit checkout master  # master is pinned to the latest release\npip install .\n```\n\n**Install with extra packages:**\n\nTo install the dependencies required to run the probabilistic models included in the `examples`/`tutorials` directories, please use the following command:\n```sh\npip install pyro-ppl[extras] \n```\nMake sure that the models come from the same release version of the [Pyro source code](https://github.com/pyro-ppl/pyro/releases) as you have installed.\n\n### Installing Pyro dev branch\n\nFor recent features you can install Pyro from source.\n\n**Install using pip:**\n\n```sh\npip install git+https://github.com/pyro-ppl/pyro.git\n```\n\nor, with the `extras` dependency to run the probabilistic models included in the `examples`/`tutorials` directories:\n```sh\npip install git+https://github.com/pyro-ppl/pyro.git#egg=project[extras]\n```\n\n**Install from source:**\n\n```sh\ngit clone https://github.com/pyro-ppl/pyro\ncd pyro\npip install .  # pip install .[extras] for running models in examples/tutorials\n```\n\n## Running Pyro from a Docker Container\n\nRefer to the instructions [here](docker/README.md).\n\n## Citation\nIf you use Pyro, please consider citing:\n```\n@article{bingham2019pyro,\n  author    = {Eli Bingham and\n               Jonathan P. Chen and\n               Martin Jankowiak and\n               Fritz Obermeyer and\n               Neeraj Pradhan and\n               Theofanis Karaletsos and\n               Rohit Singh and\n               Paul A. Szerlip and\n               Paul Horsfall and\n               Noah D. Goodman},\n  title     = {Pyro: Deep Universal Probabilistic Programming},\n  journal   = {J. Mach. Learn. Res.},\n  volume    = {20},\n  pages     = {28:1--28:6},\n  year      = {2019},\n  url       = {http://jmlr.org/papers/v20/18-403.html}\n}\n```\n"}, "pyspark": {"file_name": "apache/spark/README.md", "raw_text": "# Apache Spark\n\nSpark is a unified analytics engine for large-scale data processing. It provides\nhigh-level APIs in Scala, Java, Python, and R, and an optimized engine that\nsupports general computation graphs for data analysis. It also supports a\nrich set of higher-level tools including Spark SQL for SQL and DataFrames,\nMLlib for machine learning, GraphX for graph processing,\nand Structured Streaming for stream processing.\n\n<https://spark.apache.org/>\n\n[![Jenkins Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)\n[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)\n[![PySpark Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)\n\n\n## Online Documentation\n\nYou can find the latest Spark documentation, including a programming\nguide, on the [project web page](https://spark.apache.org/documentation.html).\nThis README file only contains basic setup instructions.\n\n## Building Spark\n\nSpark is built using [Apache Maven](https://maven.apache.org/).\nTo build Spark and its example programs, run:\n\n    ./build/mvn -DskipTests clean package\n\n(You do not need to do this if you downloaded a pre-built package.)\n\nMore detailed documentation is available from the project site, at\n[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).\n\nFor general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).\n\n## Interactive Scala Shell\n\nThe easiest way to start using Spark is through the Scala shell:\n\n    ./bin/spark-shell\n\nTry the following command, which should return 1,000,000,000:\n\n    scala> spark.range(1000 * 1000 * 1000).count()\n\n## Interactive Python Shell\n\nAlternatively, if you prefer Python, you can use the Python shell:\n\n    ./bin/pyspark\n\nAnd run the following command, which should also return 1,000,000,000:\n\n    >>> spark.range(1000 * 1000 * 1000).count()\n\n## Example Programs\n\nSpark also comes with several sample programs in the `examples` directory.\nTo run one of them, use `./bin/run-example <class> [params]`. For example:\n\n    ./bin/run-example SparkPi\n\nwill run the Pi example locally.\n\nYou can set the MASTER environment variable when running examples to submit\nexamples to a cluster. This can be a mesos:// or spark:// URL,\n\"yarn\" to run on YARN, and \"local\" to run\nlocally with one thread, or \"local[N]\" to run locally with N threads. You\ncan also use an abbreviated class name if the class is in the `examples`\npackage. For instance:\n\n    MASTER=spark://host:7077 ./bin/run-example SparkPi\n\nMany of the example programs print usage help if no params are given.\n\n## Running Tests\n\nTesting first requires [building Spark](#building-spark). Once Spark is built, tests\ncan be run using:\n\n    ./dev/run-tests\n\nPlease see the guidance on how to\n[run tests for a module, or individual tests](https://spark.apache.org/developer-tools.html#individual-tests).\n\nThere is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md\n\n## A Note About Hadoop Versions\n\nSpark uses the Hadoop core library to talk to HDFS and other Hadoop-supported\nstorage systems. Because the protocols have changed in different versions of\nHadoop, you must build Spark against the same version that your cluster runs.\n\nPlease refer to the build documentation at\n[\"Specifying the Hadoop Version and Enabling YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)\nfor detailed guidance on building for a particular distribution of Hadoop, including\nbuilding for particular Hive and Hive Thriftserver distributions.\n\n## Configuration\n\nPlease refer to the [Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)\nin the online documentation for an overview on how to configure Spark.\n\n## Contributing\n\nPlease review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)\nfor information on how to get started contributing to the project.\n"}, "qgrid": {"file_name": "quantopian/qgrid/README.rst", "raw_text": ".. image:: https://media.quantopian.com/logos/open_source/qgrid-logo-03.png\n    :target: https://qgrid.readthedocs.io\n    :width: 190px\n    :align: center\n    :alt: qgrid\n\n=====\nqgrid\n=====\nQgrid is a Jupyter notebook widget which uses `SlickGrid <https://github.com/mleibman/SlickGrid>`_ to render pandas\nDataFrames within a Jupyter notebook. This allows you to explore your DataFrames with intuitive scrolling, sorting, and\nfiltering controls, as well as edit your DataFrames by double clicking cells.\n\nQgrid was developed for use in `Quantopian's hosted research environment\n<https://www.quantopian.com/posts/qgrid-now-available-in-research-an-interactive-grid-for-sorting-and-filtering-dataframes?utm_source=github&utm_medium=web&utm_campaign=qgrid-repo>`_\nand is available for use in that environment as of June 2018.\nQuantopian also offers a `fully managed service for professionals <https://factset.quantopian.com>`_\nthat includes Qgrid, Zipline, Alphalens, Pyfolio, FactSet data, and more.\n\nAnnouncements: Qgrid Webinar\n----------------------------\nQgrid author Tim Shawver recently did a live webinar about Qgrid, and the recording of the webinar is `now available on YouTube <https://www.youtube.com/watch?v=AsJJpgwIX0Q>`_.\n\nThis talk will be interesting both for people that are new to Qgrid, as well as longtime fans that are interested in learning more about the project.\n\nDemo\n----\nClick the badge below to try out the latest beta of qgrid in Quantopian's hosted research environment. If you're already signed into Quantopian you'll be brought directly to the demo notebook. Otherwise you'll be prompted to register (it's free):\n\n.. image:: https://img.shields.io/badge/launch-quantopian-red.svg?colorB=d33015\n    :target: https://www.quantopian.com/clone_notebook?id=5b2baee1b3d6870048620188&utm_source=github&utm_medium=web&utm_campaign=qgrid-repo\n|\nClick the badge below to try out qgrid using binder:\n\n.. image:: https://beta.mybinder.org/badge.svg\n    :target: https://mybinder.org/v2/gh/quantopian/qgrid-notebooks/master?filepath=index.ipynb\n|\nClick the following badge to try out qgrid in Jupyterlab, also using binder:\n\n.. image:: https://mybinder.org/badge.svg\n    :target: https://mybinder.org/v2/gh/quantopian/qgrid-notebooks/master?urlpath=lab\n|\n*For both binder links, you'll see a brief loading screen while a server is being created for you in the cloud.  This shouldn't take more than a minute, and usually completes in under 10 seconds.*\n\n*The binder demos generally will be using the most recent stable release of qgrid, so features that were added in a recent beta version may not be available in those demos.*\n\nFor people who would rather not go to another page to try out qgrid for real, here's the tldr; version:\n\n        .. figure:: docs/images/filtering_demo.gif\n         :align: left\n         :target: docs/images/filtering_demo.gif\n         :width: 200px\n\n          A brief demo showing filtering, editing, and the `get_changed_df()` method\n\nAPI Documentation\n-----------------\nAPI documentation is hosted on `readthedocs <http://qgrid.readthedocs.io/en/latest/>`_.\n\nInstallation\n------------\n\nInstalling with pip::\n\n  pip install qgrid\n  jupyter nbextension enable --py --sys-prefix qgrid\n\n  # only required if you have not enabled the ipywidgets nbextension yet\n  jupyter nbextension enable --py --sys-prefix widgetsnbextension\n\nInstalling with conda::\n\n  # only required if you have not added conda-forge to your channels yet\n  conda config --add channels conda-forge\n\n  conda install qgrid\n\nJupyterlab Installation\n-----------------------\n\nFirst, go through the normal installation steps above as you normally would when using qgrid in the notebook.\nIf you haven't already install jupyterlab and enabled ipywidgets, do that first with the following lines::\n\n  pip install jupyterlab\n  jupyter labextension install @jupyter-widgets/jupyterlab-manager\n\nInstall the qgrid-jupyterlab extension and enable::\n\n  jupyter labextension install qgrid2\n\nAt this point if you run jupyter lab normally with the 'jupyter lab' command, you should be\nable to use qgrid in notebooks as you normally would.\n\n*Please Note: Jupyterlab support has been tested with jupyterlab 0.30.5 and jupyterlab-manager 0.31.3, so if you're\nhaving trouble, try installing those versions. Feel free to file an issue if you find that qgrid isn't working\nwith a newer version of either dependency.*\n\nWhat's New\n----------\n**Column-specific options (as of 1.1.0)**:\nThanks to a significant `PR from the community <https://github.com/quantopian/qgrid/pull/191>`_, Qgrid users now have the ability to set a number of options on a per column basis.  This allows you to do things like explicitly specify which column should be sortable, editable, etc.  For example, if you wanted to prevent editing on all columns except for a column named `'A'`, you could do the following::\n\n    col_opts = { 'editable': False }\n    col_defs = { 'A': { 'editable': True } }\n    qgrid.show_grid(df, column_options=col_opts, column_definitions=col_defs)\n\nSee the updated `show_grid <https://qgrid.readthedocs.io/en/v1.1.0/#qgrid.show_grid>`_ documentation for more information.\n\n**Disable editing on a per-row basis (as of 1.1.0)**:\nThis feature can be thought of as the first row-specific option that qgrid supports.  In particular it allows a user to specify, using python code, whether or not a particular row should be editable. For example, to make it so only rows in the grid where the `'status'` column is set to `'active'` are editable, you might use the following code::\n\n    def can_edit_row(row):\n        return row['status'] == 'active'\n\n    qgrid.show_grid(df, row_edit_callback=can_edit_row)\n\n**New API methods for dynamically updating an existing qgrid widget (as of 1.1.0)**:\nAdds the following new methods, which can be used to update the state of an existing Qgrid widget without having to call `show_grid` to completely rebuild the widget:\n\n    - `edit_cell <https://qgrid.readthedocs.io/en/latest/#qgrid.QgridWidget.edit_cell>`_\n    - `change_selection <https://qgrid.readthedocs.io/en/latest/#qgrid.QgridWidget.change_selection>`_\n    - `toggle_editable <https://qgrid.readthedocs.io/en/latest/#qgrid.QgridWidget.toggle_editable>`_\n    - `change_grid_option <https://qgrid.readthedocs.io/en/latest/#qgrid.QgridWidget.change_grid_option>`_ (experimental)\n\n**Improved MultiIndex Support (as of 1.0.6-beta.6)**:\nQgrid now displays multi-indexed DataFrames with some of the index cells merged for readability, as is normally done when viewing DataFrames as a static html table.  The following image shows qgrid displaying a multi-indexed DataFrame that was returned from Quantopian's `Pipeline API <https://www.quantopian.com/tutorials/pipeline?utm_source=github&utm_medium=web&utm_campaign=qgrid-repo>`_:\n\n.. figure:: https://s3.amazonaws.com/quantopian-forums/pipeline_with_qgrid.png\n         :align: left\n         :target: https://s3.amazonaws.com/quantopian-forums/pipeline_with_qgrid.png\n         :width: 100px\n\nDependencies\n------------\n\nQgrid runs on `Python 2 or 3 <https://www.python.org/downloads/>`_.  You'll also need\n`pip <https://pypi.python.org/pypi/pip>`_ for the installation steps below.\n\nQgrid depends on the following three Python packages:\n\n    `Jupyter notebook <https://github.com/jupyter/notebook>`_\n      This is the interactive Python environment in which qgrid runs.\n\n    `ipywidgets <https://github.com/ipython/ipywidgets>`_\n      In order for Jupyter notebooks to be able to run widgets, you have to also install this ipywidgets package.\n      It's maintained by the Jupyter organization, the same people who created Jupyter notebook.\n\n    `Pandas <http://pandas.pydata.org/>`_\n      A powerful data analysis / manipulation library for Python.  Qgrid requires that the data to be rendered as an\n      interactive grid be provided in the form of a pandas DataFrame.\n\nThese are listed in `requirements.txt <https://github.com/quantopian/qgrid/blob/master/requirements.txt>`_\nand will be automatically installed (if necessary) when qgrid is installed via pip.\n\nCompatibility\n-------------\n\n=================  ===========================  ==============================  ==============================\n qgrid             IPython / Jupyter notebook   ipywidgets                      Jupyterlab\n=================  ===========================  ==============================  ==============================\n 0.2.0             2.x                          N/A                             N/A\n 0.3.x             3.x                          N/A                             N/A\n 0.3.x             4.0                          4.0.x                           N/A\n 0.3.x             4.1                          4.1.x                           N/A\n 0.3.2             4.2                          5.x                             N/A\n 0.3.3             5.x                          6.x                             N/A\n 1.0.x             5.x                          7.x                             0.30.x\n=================  ===========================  ==============================  ==============================\n\n\nRunning the demo notebooks locally\n----------------------------------\n\nThere are a couple of demo notebooks in the `qgrid-notebooks <https://github.com/quantopian/qgrid-notebooks/>`_ repository\nwhich will help you get familiar with the functionality that qgrid provides. Here are the steps to clone the\nqgrid-notebooks repository and open a demo notebook:\n\n#. Install qgrid by following the instructions in the `Installation`_ section above, if you haven't already\n\n#. Clone the qgrid-notebooks repository from GitHub::\n\n    git clone https://github.com/quantopian/qgrid-notebooks.git\n\n#. Install the dev requirements for the repository and start the notebook server::\n\n    cd qgrid-notebooks\n    pip install -r requirements_dev.txt\n    jupyter notebook\n\n#. Click on one of the two notebooks (`index.ipynb <https://github.com/quantopian/qgrid-notebooks/blob/master/index.ipynb>`_ or `experimental.ipynb <https://github.com/quantopian/qgrid-notebooks/blob/master/experimental.ipynb>`_) that you see listed in the notebook UI in your browser.\n\nRunning from source & testing your changes\n------------------------------------------\n\nIf you'd like to contribute to qgrid, or just want to be able to modify the source code for your own purposes, you'll\nwant to clone this repository and run qgrid from your local copy of the repository.  The following steps explain how\nto do this.\n\n#. Clone the repository from GitHub and ``cd`` into the top-level directory::\n\n    git clone https://github.com/quantopian/qgrid.git\n    cd qgrid\n\n#. Install the current project in `editable <https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs>`_\n   mode::\n\n    pip install -e .\n\n#. Install the node packages that qgrid depends on and build qgrid's javascript using webpack::\n\n    cd js && npm install .\n\n#. Install and enable qgrid's javascript in your local jupyter notebook environment::\n\n    jupyter nbextension install --py --symlink --sys-prefix qgrid && jupyter nbextension enable --py --sys-prefix qgrid\n\n#. If desired, install the labextension::\n\n    jupyter labextension link js/\n\n#. Run the notebook as you normally would with the following command::\n\n    jupyter notebook\n\nManually testing server-side changes\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIf the code you need to change is in qgrid's python code, then restart the kernel of the notebook you're in and\nrerun any qgrid cells to see your changes take effect.\n\nManually testing client-side changes\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIf the code you need to change is in qgrid's javascript or css code, repeat step 3 to rebuild qgrid's npm package,\nthen refresh the browser tab where you're viewing your notebook to see your changes take effect.\n\nRunning automated tests\n^^^^^^^^^^^^^^^^^^^^^^^\nThere is a small python test suite which can be run locally by running the command ``pytest`` in the root folder\nof the repository.\n\nBuilding docs\n^^^^^^^^^^^^^\nThe read-the-docs page is generated using sphinx. If you change any doc strings or want to add something to the\nread-the-docs page, you can preview your changes locally before submitting a PR using the following commands::\n\n    pip install sphinx sphinx_rtd_theme\n    cd docs && make html\n\nThis will result in the ``docs/_build/html`` folder being populated with a new version of the read-the-docs site. If\nyou open the ``index.html`` file in your browser, you should be able to preview your changes.\n\nEvents API\n----------\nAs of qgrid 1.0.3 there are new ``on`` and ``off`` methods in qgrid which can be used to attach/detach event handlers. They're available on both the ``qgrid`` module (see `qgrid.on <https://qgrid.readthedocs.io/en/latest/#qgrid.on>`_), and on individual QgridWidget instances (see `qgrid.QgridWidget.on <https://qgrid.readthedocs.io/en/latest/#qgrid.QgridWidget.on>`_). Previously the only way to listen for events was to use undocumented parts of the API.\n\nHaving the ability to attach event handlers allows us to do some interesting things in terms of using qgrid in conjunction with other widgets/visualizations. One example is using qgrid to filter a DataFrame that's also being displayed by another visualization.\n\nIf you previously used the ``observe`` method to respond to qgrid events, lets see how your code might be updated to use the new ``on`` method::\n\n    # Before upgrading to 1.0.3\n    def handle_df_change(change):\n        print(change['new'])\n\n    qgrid_widget.observe(handle_df_change, names=['_df'])\n\nWhen you upgrade to 1.0.3, you have more granular control over which events you do an don't listen to, but you can also replicate the previous behavior of calling ``print`` every time the state of the internal DataFrame is changed. Here's what that would look like using the new ``on`` method::\n\n    # After upgrading to 1.0.3\n    def handle_json_updated(event, qgrid_widget):\n        # exclude 'viewport_changed' events since that doesn't change the DataFrame\n        if (event['triggered_by'] != 'viewport_changed'):\n            print(qgrid_widget.get_changed_df())\n\n    qgrid_widget.on('json_updated', handle_json_updated)\n\nSee the `events notebook <https://mybinder.org/v2/gh/quantopian/qgrid-notebooks/master?filepath=events.ipynb>`_ for more examples of using these new API methods.\n\nFor people who would rather not go to another page to try out the events notebook, here are a couple of gifs to give you an idea of what you can do with it.\n\nThe first gif shows how you can use qgrid to filter the data that's being shown by a matplotlib scatter plot:\n\n        .. figure:: docs/images/linked_to_scatter.gif\n         :align: left\n         :target: docs/images/linked_to_scatter.gif\n         :width: 600px\n\n          A brief demo showing qgrid hooked up to a matplotlib plot\n\nThe second gif shows how you can move qgrid to a separate view in JupyterLab, which makes it more convenient\nto use in conjunction with other visualizations (in this case, a couple of ``Output`` widgets):\n\n        .. figure:: docs/images/events_api.gif\n         :align: left\n         :target: docs/images/events_api.gif\n         :width: 600px\n\n          A brief demo showing qgrid's events api\n\nContinuing to use qgrid 0.3.3\n-----------------------------\nIf you're looking for the installation and usage instructions for qgrid 0.3.3 and the sample notebook that goes\nalong with it, please see the `qgrid 0.3.3 tag <https://github.com/quantopian/qgrid/tree/v0.3.3>`_ in this\nrepository. The installation steps will be mostly the same. The only difference is that when you run \"pip install\"\nyou'll have to explicitly specify that you want to install version 0.3.3, like this::\n\n  pip install qgrid==0.3.3\n\nIf you're looking for the API docs, you can find them on the\n`readthedocs page for qgrid 0.3.3 <http://qgrid.readthedocs.io/en/v0.3.3/>`_.\n\nIf you're looking for the demo notebook for 0.3.3, it's still availabe `in nbviewer\n<http://nbviewer.jupyter.org/gist/TimShawver/8fcef51dd3c222ed25306c002ab89b60>`_.\n\nQgrid 0.3.3 is not compatible with ipywidgets 7, so if you need support for ipywidgets 7, you'll need to use\nqgrid 1.0.\n\nContributing\n------------\nAll contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome. See the\n`Running from source & testing your changes`_ section above for more details on local qgrid development.\n\nIf you are looking to start working with the qgrid codebase, navigate to the GitHub issues tab and start looking\nthrough interesting issues.\n\nFeel free to ask questions by submitting an issue with your question.\n"}, "rapidfuzz": {"file_name": "maxbachmann/rapidfuzz/README.md", "raw_text": "<h1 align=\"center\">\n<img src=\"https://raw.githubusercontent.com/maxbachmann/rapidfuzz/master/.github/RapidFuzz.svg?sanitize=true\" alt=\"RapidFuzz\" width=\"400\">\n</h1>\n<h4 align=\"center\">Rapid fuzzy string matching in Python and C++ using the Levenshtein Distance</h4>\n\n<p align=\"center\">\n  <a href=\"https://github.com/maxbachmann/rapidfuzz/actions\">\n    <img src=\"https://github.com/maxbachmann/rapidfuzz/workflows/Build/badge.svg\"\n         alt=\"Continous Integration\">\n  </a>\n  <a href=\"https://pypi.org/project/rapidfuzz/\">\n    <img src=\"https://img.shields.io/pypi/v/rapidfuzz\"\n         alt=\"PyPI package version\">\n  </a>\n  <a href=\"https://anaconda.org/conda-forge/rapidfuzz\">\n    <img src=\"https://img.shields.io/conda/vn/conda-forge/rapidfuzz.svg\"\n         alt=\"Conda Version\">\n  </a>\n  <a href=\"https://www.python.org\">\n    <img src=\"https://img.shields.io/pypi/pyversions/rapidfuzz\"\n         alt=\"Python versions\">\n  </a>\n  <a href=\"https://github.com/maxbachmann/rapidfuzz/blob/dev/LICENSE\">\n    <img src=\"https://img.shields.io/github/license/maxbachmann/rapidfuzz\"\n         alt=\"GitHub license\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"#description\">Description</a> \u2022\n  <a href=\"#installation\">Installation</a> \u2022\n  <a href=\"#usage\">Usage</a> \u2022\n  <a href=\"#license\">License</a>\n</p>\n\n---\n\n## Description\nRapidFuzz is a fast string matching library for Python and C++, which is using the string similarity calculations from [FuzzyWuzzy](https://github.com/seatgeek/fuzzywuzzy). However there are two aspects that set RapidFuzz apart from FuzzyWuzzy:\n1) It is MIT licensed so it can be used whichever License you might want to choose for your project, while you're forced to adopt the GPL license when using FuzzyWuzzy\n2) It is mostly written in C++ and on top of this comes with a lot of Algorithmic improvements to make string matching even faster, while still providing the same results. More details on these performance improvements in form of benchmarks can be found [here](https://github.com/maxbachmann/rapidfuzz/blob/master/Benchmarks.md)\n\n\n## Installation\nRapidFuzz can be installed using [pip](https://pypi.org/project/rapidfuzz/)\n```bash\n$ pip install rapidfuzz\n```\n\nThere are pre-built binaries (wheels) for `RapidFuzz` and its dependencies for MacOS (10.9 and later), Linux x86_64 and Windows.\n\nFor any other architecture/os `RapidFuzz` can be installed from the source distribution. To do so, a C++14 capable compiler must be installed before running the `pip install rapidfuzz` command. While Linux and MacOs usually come with a compiler it is required to install [C++-Buildtools](https://visualstudio.microsoft.com/visual-cpp-build-tools) on Windows.\n\n\n## Usage\n```console\n> from rapidfuzz import fuzz\n> from rapidfuzz import process\n```\n\n### Simple Ratio\n```console\n> fuzz.ratio(\"this is a test\", \"this is a test!\")\n96.55171966552734\n```\n\n### Partial Ratio\n```console\n> fuzz.partial_ratio(\"this is a test\", \"this is a test!\")\n100.0\n```\n\n### Token Sort Ratio\n```console\n> fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")\n90.90908813476562\n> fuzz.token_sort_ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")\n100.0\n```\n\n### Token Set Ratio\n```console\n> fuzz.token_sort_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")\n83.8709716796875\n> fuzz.token_set_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")\n100.0\n```\n\n### Process\n```console\n> choices = [\"Atlanta Falcons\", \"New York Jets\", \"New York Giants\", \"Dallas Cowboys\"]\n> process.extract(\"new york jets\", choices, limit=2)\n[('new york jets', 100), ('new york giants', 78.57142639160156)]\n> process.extractOne(\"cowboys\", choices)\n(\"dallas cowboys\", 90)\n```\n\n## License\nRapidFuzz is licensed under the MIT license since I believe that everyone should be able to use it without being forced to adopt the GPL license. Thats why the library is based on an older version of fuzzywuzzy that was MIT licensed as well.\nThis old version of fuzzywuzzy can be found [here](https://github.com/seatgeek/fuzzywuzzy/tree/4bf28161f7005f3aa9d4d931455ac55126918df7).\n"}, "ray": {"file_name": "ray-project/ray/README.rst", "raw_text": ".. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png\n\n.. image:: https://travis-ci.com/ray-project/ray.svg?branch=master\n    :target: https://travis-ci.com/ray-project/ray\n\n.. image:: https://readthedocs.org/projects/ray/badge/?version=latest\n    :target: http://docs.ray.io/en/latest/?badge=latest\n\n|\n\n\n**Ray is a fast and simple framework for building and running distributed applications.**\n\nRay is packaged with the following libraries for accelerating machine learning workloads:\n\n- `Tune`_: Scalable Hyperparameter Tuning\n- `RLlib`_: Scalable Reinforcement Learning\n- `RaySGD <https://docs.ray.io/en/latest/raysgd/raysgd.html>`__: Distributed Training Wrappers\n\nInstall Ray with: ``pip install ray``. For nightly wheels, see the\n`Installation page <https://docs.ray.io/en/latest/installation.html>`__.\n\n**NOTE:** `We are deprecating Python 2 support soon.`_\n\n.. _`We are deprecating Python 2 support soon.`: https://github.com/ray-project/ray/issues/6580\n\nQuick Start\n-----------\n\nExecute Python functions in parallel.\n\n.. code-block:: python\n\n    import ray\n    ray.init()\n\n    @ray.remote\n    def f(x):\n        return x * x\n\n    futures = [f.remote(i) for i in range(4)]\n    print(ray.get(futures))\n\nTo use Ray's actor model:\n\n.. code-block:: python\n\n\n    import ray\n    ray.init()\n\n    @ray.remote\n    class Counter(object):\n        def __init__(self):\n            self.n = 0\n\n        def increment(self):\n            self.n += 1\n\n        def read(self):\n            return self.n\n\n    counters = [Counter.remote() for i in range(4)]\n    [c.increment.remote() for c in counters]\n    futures = [c.read.remote() for c in counters]\n    print(ray.get(futures))\n\n\nRay programs can run on a single machine, and can also seamlessly scale to large clusters. To execute the above Ray script in the cloud, just download `this configuration file <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/aws/example-full.yaml>`__, and run:\n\n``ray submit [CLUSTER.YAML] example.py --start``\n\nRead more about `launching clusters <https://docs.ray.io/en/latest/autoscaling.html>`_.\n\nTune Quick Start\n----------------\n\n.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/tune-wide.png\n\n`Tune`_ is a library for hyperparameter tuning at any scale.\n\n- Launch a multi-node distributed hyperparameter sweep in less than 10 lines of code.\n- Supports any deep learning framework, including PyTorch, TensorFlow, and Keras.\n- Visualize results with `TensorBoard <https://www.tensorflow.org/get_started/summaries_and_tensorboard>`__.\n- Choose among scalable SOTA algorithms such as `Population Based Training (PBT)`_, `Vizier's Median Stopping Rule`_, `HyperBand/ASHA`_.\n- Tune integrates with many optimization libraries such as `Facebook Ax <http://ax.dev>`_, `HyperOpt <https://github.com/hyperopt/hyperopt>`_, and `Bayesian Optimization <https://github.com/fmfn/BayesianOptimization>`_ and enables you to scale them transparently.\n\nTo run this example, you will need to install the following:\n\n.. code-block:: bash\n\n    $ pip install ray[tune] torch torchvision filelock\n\n\nThis example runs a parallel grid search to train a Convolutional Neural Network using PyTorch.\n\n.. code-block:: python\n\n\n    import torch.optim as optim\n    from ray import tune\n    from ray.tune.examples.mnist_pytorch import (\n        get_data_loaders, ConvNet, train, test)\n\n\n    def train_mnist(config):\n        train_loader, test_loader = get_data_loaders()\n        model = ConvNet()\n        optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"])\n        for i in range(10):\n            train(model, optimizer, train_loader)\n            acc = test(model, test_loader)\n            tune.track.log(mean_accuracy=acc)\n\n\n    analysis = tune.run(\n        train_mnist, config={\"lr\": tune.grid_search([0.001, 0.01, 0.1])})\n\n    print(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))\n\n    # Get a dataframe for analyzing trial results.\n    df = analysis.dataframe()\n\nIf TensorBoard is installed, automatically visualize all trial results:\n\n.. code-block:: bash\n\n    tensorboard --logdir ~/ray_results\n\n.. _`Tune`: https://docs.ray.io/en/latest/tune.html\n.. _`Population Based Training (PBT)`: https://docs.ray.io/en/latest/tune-schedulers.html#population-based-training-pbt\n.. _`Vizier's Median Stopping Rule`: https://docs.ray.io/en/latest/tune-schedulers.html#median-stopping-rule\n.. _`HyperBand/ASHA`: https://docs.ray.io/en/latest/tune-schedulers.html#asynchronous-hyperband\n\nRLlib Quick Start\n-----------------\n\n.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/rllib-wide.jpg\n\n`RLlib`_ is an open-source library for reinforcement learning built on top of Ray that offers both high scalability and a unified API for a variety of applications.\n\n.. code-block:: bash\n\n  pip install tensorflow  # or tensorflow-gpu\n  pip install ray[rllib]  # also recommended: ray[debug]\n\n.. code-block:: python\n\n    import gym\n    from gym.spaces import Discrete, Box\n    from ray import tune\n\n    class SimpleCorridor(gym.Env):\n        def __init__(self, config):\n            self.end_pos = config[\"corridor_length\"]\n            self.cur_pos = 0\n            self.action_space = Discrete(2)\n            self.observation_space = Box(0.0, self.end_pos, shape=(1, ))\n\n        def reset(self):\n            self.cur_pos = 0\n            return [self.cur_pos]\n\n        def step(self, action):\n            if action == 0 and self.cur_pos > 0:\n                self.cur_pos -= 1\n            elif action == 1:\n                self.cur_pos += 1\n            done = self.cur_pos >= self.end_pos\n            return [self.cur_pos], 1 if done else 0, done, {}\n\n    tune.run(\n        \"PPO\",\n        config={\n            \"env\": SimpleCorridor,\n            \"num_workers\": 4,\n            \"env_config\": {\"corridor_length\": 5}})\n\n.. _`RLlib`: https://docs.ray.io/en/latest/rllib.html\n\n\nMore Information\n----------------\n\n- `Documentation`_\n- `Tutorial`_\n- `Blog`_\n- `Ray paper`_\n- `Ray HotOS paper`_\n- `RLlib paper`_\n- `Tune paper`_\n\n.. _`Documentation`: http://docs.ray.io/en/latest/index.html\n.. _`Tutorial`: https://github.com/ray-project/tutorial\n.. _`Blog`: https://ray-project.github.io/\n.. _`Ray paper`: https://arxiv.org/abs/1712.05889\n.. _`Ray HotOS paper`: https://arxiv.org/abs/1703.03924\n.. _`RLlib paper`: https://arxiv.org/abs/1712.09381\n.. _`Tune paper`: https://arxiv.org/abs/1807.05118\n\nGetting Involved\n----------------\n\n- `ray-dev@googlegroups.com`_: For discussions about development or any general\n  questions.\n- `StackOverflow`_: For questions about how to use Ray.\n- `GitHub Issues`_: For reporting bugs and feature requests.\n- `Pull Requests`_: For submitting code contributions.\n- `Meetup Group`_: Join our meetup group.\n- `Community Slack`_: Join our Slack workspace.\n- `Twitter`_: Follow updates on Twitter.\n\n.. _`ray-dev@googlegroups.com`: https://groups.google.com/forum/#!forum/ray-dev\n.. _`GitHub Issues`: https://github.com/ray-project/ray/issues\n.. _`StackOverflow`: https://stackoverflow.com/questions/tagged/ray\n.. _`Pull Requests`: https://github.com/ray-project/ray/pulls\n.. _`Meetup Group`: https://www.meetup.com/Bay-Area-Ray-Meetup/\n.. _`Community Slack`: https://forms.gle/9TSdDYUgxYs8SA9e8\n.. _`Twitter`: https://twitter.com/raydistributed\n"}, "requests": {"file_name": "psf/requests/README.md", "raw_text": "\n\n<span align=\"center\">\n\n<pre>\n    <a href=\"https://requests.readthedocs.io/\"><img src=\"https://raw.githubusercontent.com/psf/requests/master/ext/requests-logo.png\" align=\"center\" /></a>\n    \n    <div align=\"left\">\n    <p></p>\n    <code> Python 3.7.4 (default, Sep  7 2019, 18:27:02)</code>\n    <code> >>> <strong>import requests</strong></code>\n    <code> >>> r = requests.get('https://api.github.com/repos/psf/requests')</code>\n    <code> >>> r.json()[\"description\"]</code>\n    <code> 'A simple, yet elegant HTTP library.'</code>\n    </div>\n\n    <p align=\"center\">\nThis software has been designed for you, with much joy,\nby <a href=\"https://kennethreitz.org/\">Kenneth Reitz</a> & is protected by The <a href=\"https://www.python.org/psf/\">Python Software Foundation</a>.\n   </p>\n</pre>\n\n</span>\n\n<p>&nbsp;</p><p>&nbsp;</p>\n\n<p align=\"center\"><strong>Requests</strong> is an elegant and simple HTTP library for Python, built with \u2665.</p>\n\n<p>&nbsp;</p>\n\n```pycon\n>>> import requests\n>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))\n>>> r.status_code\n200\n>>> r.headers['content-type']\n'application/json; charset=utf8'\n>>> r.encoding\n'utf-8'\n>>> r.text\n'{\"type\":\"User\"...'\n>>> r.json()\n{'disk_usage': 368627, 'private_gists': 484, ...}\n```\n\n\n\n---------------------------------------------------------------------\n\n<p>&nbsp;</p>\n\nRequests allows you to send HTTP/1.1 requests extremely easily. There\u2019s no need to manually add query strings to your URLs, or to form-encode your `PUT` & `POST` data \u2014 but nowadays, just use the `json` method!\n\n\nRequests is **the most downloaded Python package today**, pulling in around `14M downloads / week`\u2014 according to GitHub, Requests is currently [depended upon](https://github.com/psf/requests/network/dependents?package_id=UGFja2FnZS01NzA4OTExNg%3D%3D) by `367_296` repositories. You may certainly put your trust in this code.\n\n\n<p>&nbsp;</p>\n<p align=\"center\"><a href=\"https://pepy.tech/project/requests\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/e1dedc9f5ce5cd6b6c699f33d2e812daadcf3645/68747470733a2f2f706570792e746563682f62616467652f7265717565737473\" alt=\"Downloads\" data-canonical-src=\"https://pepy.tech/badge/requests\" style=\"max-width:100%;\"></a>\n<a href=\"https://pypi.org/project/requests/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/6d78aeec0a9a1cfe147ad064bfb99069e298e29b/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f72657175657374732e737667\" alt=\"image\" data-canonical-src=\"https://img.shields.io/pypi/pyversions/requests.svg\" style=\"max-width:100%;\"></a>\n<a href=\"https://github.com/psf/requests/graphs/contributors\"><img src=\"https://camo.githubusercontent.com/a70ea15870b38bba9203b969f6a6b7e7845fbb8a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f7073662f72657175657374732e737667\" alt=\"image\" data-canonical-src=\"https://img.shields.io/github/contributors/psf/requests.svg\" style=\"max-width:100%;\"></a></p>\n\n<p>&nbsp;</p>\n\n<h2 align=\"center\">Supported Features & Best\u2013Practices</h2>\n\nRequests is ready for the demands of building robust and reliable HTTP\u2013speak applications, for the needs of today.\n\n<pre class=\"test\">\n         + International Domains and URLs       + Keep-Alive & Connection Pooling\n         + Sessions with Cookie Persistence     + Browser-style SSL Verification\n         + Basic & Digest Authentication        + Familiar `dict`\u2013like Cookies\n         + Automatic Decompression of Content   + Automatic Content Decoding\n         + Automatic Connection Pooling         + Unicode Response Bodies<super>*</super>\n         + Multi-part File Uploads              + SOCKS Proxy Support\n         + Connection Timeouts                  + Streaming Downloads\n         + Automatic honoring of `.netrc`       + Chunked HTTP Requests\n\n                            &, of course, rock\u2013solid stability!\n</pre>\n</div>\n\n<p align=\"center\">\n        \u2728 \ud83c\udf70 \u2728&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n</p>\n\n<p>&nbsp;</p>\n\nRequests Module Installation\n----------------------------\n\nThe recommended way to intall the `requests` module is to simply use [`pipenv`](https://pipenv.kennethreitz.org) (or `pip`, of\ncourse):\n\n```console\n$ pipenv install requests\nAdding requests to Pipfile's [packages]\u2026\n\u2714 Installation Succeeded\n\u2026\n```\n\nRequests officially supports Python 2.7 & 3.5+.\n\n-------------------------------------\n\n## P.S. \u2014\u00a0Documentation is available at [`requests.readthedocs.io`](https://requests.readthedocs.io/en/latest/).\n\n<p align=\"center\">\n        <a href=\"https://requests.readthedocs.io/\"><img src=\"https://raw.githubusercontent.com/psf/requests/master/ext/ss.png\" align=\"center\" /></a>\n</p>\n\n\n------------------\n\n\n<p>&nbsp;</p>\n\n<p align=\"center\">\n        <a href=\"https://kennethreitz.org/\"><img src=\"https://raw.githubusercontent.com/psf/requests/master/ext/kr.png\" align=\"center\" /></a>\n</p>\n\n<p>&nbsp;</p>\n\n<p align=\"center\">\n        <a href=\"https://www.python.org/psf/\"><img src=\"https://raw.githubusercontent.com/psf/requests/master/ext/psf.png\" align=\"center\" /></a>\n</p>\n"}, "s3fs": {"file_name": "dask/s3fs/README.rst", "raw_text": "s3fs\n====\n\n|Build Status| |Doc Status|\n\nS3FS builds on botocore_ to provide a convenient Python filesystem interface for S3.\n\nView the documentation_ for s3fs.\n\n.. _documentation: http://s3fs.readthedocs.io/en/latest/\n.. _botocore: https://botocore.readthedocs.io/en/latest/\n\n.. |Build Status| image:: https://travis-ci.org/dask/s3fs.svg?branch=master\n    :target: https://travis-ci.org/dask/s3fs\n    :alt: Build Status\n.. |Doc Status| image:: https://readthedocs.org/projects/s3fs/badge/?version=latest\n    :target: https://s3fs.readthedocs.io/en/latest/?badge=latest\n    :alt: Documentation Status\n"}, "scikit-learn": {"file_name": "scikit-learn/scikit-learn/README.rst", "raw_text": ".. -*- mode: rst -*-\n\n|Azure|_ |Travis|_ |Codecov|_ |CircleCI|_ |PythonVersion|_ |PyPi|_ |DOI|_\n\n.. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=master\n.. _Azure: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=master\n\n.. |Travis| image:: https://api.travis-ci.org/scikit-learn/scikit-learn.svg?branch=master\n.. _Travis: https://travis-ci.org/scikit-learn/scikit-learn\n\n.. |Codecov| image:: https://codecov.io/github/scikit-learn/scikit-learn/badge.svg?branch=master&service=github\n.. _Codecov: https://codecov.io/github/scikit-learn/scikit-learn?branch=master\n\n.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/master.svg?style=shield&circle-token=:circle-token\n.. _CircleCI: https://circleci.com/gh/scikit-learn/scikit-learn\n\n.. |PythonVersion| image:: https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8-blue\n.. _PythonVersion: https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8-blue\n\n.. |PyPi| image:: https://badge.fury.io/py/scikit-learn.svg\n.. _PyPi: https://badge.fury.io/py/scikit-learn\n\n.. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg\n.. _DOI: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn\n\nscikit-learn\n============\n\nscikit-learn is a Python module for machine learning built on top of\nSciPy and is distributed under the 3-Clause BSD license.\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nIt is currently maintained by a team of volunteers.\n\nWebsite: https://scikit-learn.org\n\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\nscikit-learn requires:\n\n- Python (>= 3.6)\n- NumPy (>= 1.13.3)\n- SciPy (>= 0.19.1)\n- joblib (>= 0.11)\n\n**Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.**\nscikit-learn 0.23 and later require Python 3.6 or newer.\n\nScikit-learn plotting capabilities (i.e., functions start with ``plot_``\nand classes end with \"Display\") require Matplotlib (>= 2.1.1). For running the\nexamples Matplotlib >= 2.1.1 is required. A few examples require\nscikit-image >= 0.13, a few examples require pandas >= 0.18.0, some examples\nrequire seaborn >= 0.9.0.\n\nUser installation\n~~~~~~~~~~~~~~~~~\n\nIf you already have a working installation of numpy and scipy,\nthe easiest way to install scikit-learn is using ``pip``   ::\n\n    pip install -U scikit-learn\n\nor ``conda``::\n\n    conda install scikit-learn\n\nThe documentation includes more detailed `installation instructions <https://scikit-learn.org/stable/install.html>`_.\n\n\nChangelog\n---------\n\nSee the `changelog <https://scikit-learn.org/dev/whats_new.html>`__\nfor a history of notable changes to scikit-learn.\n\nDevelopment\n-----------\n\nWe welcome new contributors of all experience levels. The scikit-learn\ncommunity goals are to be helpful, welcoming, and effective. The\n`Development Guide <https://scikit-learn.org/stable/developers/index.html>`_\nhas detailed information about contributing code, documentation, tests, and\nmore. We've included some basic information in this README.\n\nImportant links\n~~~~~~~~~~~~~~~\n\n- Official source code repo: https://github.com/scikit-learn/scikit-learn\n- Download releases: https://pypi.org/project/scikit-learn/\n- Issue tracker: https://github.com/scikit-learn/scikit-learn/issues\n\nSource code\n~~~~~~~~~~~\n\nYou can check the latest sources with the command::\n\n    git clone https://github.com/scikit-learn/scikit-learn.git\n\nContributing\n~~~~~~~~~~~~\n\nTo learn more about making a contribution to scikit-learn, please see our\n`Contributing guide\n<https://scikit-learn.org/dev/developers/contributing.html>`_.\n\nTesting\n~~~~~~~\n\nAfter installation, you can launch the test suite from outside the\nsource directory (you will need to have ``pytest`` >= 3.3.0 installed)::\n\n    pytest sklearn\n\nSee the web page https://scikit-learn.org/dev/developers/advanced_installation.html#testing\nfor more information.\n\n    Random number generation can be controlled during testing by setting\n    the ``SKLEARN_SEED`` environment variable.\n\nSubmitting a Pull Request\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBefore opening a Pull Request, have a look at the\nfull Contributing page to make sure your code complies\nwith our guidelines: https://scikit-learn.org/stable/developers/index.html\n\n\nProject History\n---------------\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nThe project is currently maintained by a team of volunteers.\n\n**Note**: `scikit-learn` was previously referred to as `scikits.learn`.\n\n\nHelp and Support\n----------------\n\nDocumentation\n~~~~~~~~~~~~~\n\n- HTML documentation (stable release): https://scikit-learn.org\n- HTML documentation (development version): https://scikit-learn.org/dev/\n- FAQ: https://scikit-learn.org/stable/faq.html\n\nCommunication\n~~~~~~~~~~~~~\n\n- Mailing list: https://mail.python.org/mailman/listinfo/scikit-learn\n- IRC channel: ``#scikit-learn`` at ``webchat.freenode.net``\n- Stack Overflow: https://stackoverflow.com/questions/tagged/scikit-learn\n- Website: https://scikit-learn.org\n\nCitation\n~~~~~~~~\n\nIf you use scikit-learn in a scientific publication, we would appreciate citations: https://scikit-learn.org/stable/about.html#citing-scikit-learn\n"}, "scipy": {"file_name": "scipy/scipy/README.rst", "raw_text": "SciPy\n=====\n\n.. image:: https://img.shields.io/travis/scipy/scipy/master.svg?label=Travis%20CI\n   :target: https://travis-ci.org/scipy/scipy/\n\n.. image:: https://img.shields.io/appveyor/ci/scipy/scipy/master.svg?label=AppVeyor\n   :target: https://ci.appveyor.com/project/scipy/scipy\n\n.. image:: https://img.shields.io/circleci/project/github/scipy/scipy/master.svg?label=CircleCI\n  :target: https://circleci.com/gh/scipy/scipy\n\n.. image:: https://dev.azure.com/scipy-org/SciPy/_apis/build/status/scipy.scipy?branchName=master\n  :target: https://dev.azure.com/scipy-org/SciPy/_build/latest?definitionId=1?branchName=master\n\nSciPy (pronounced \"Sigh Pie\") is open-source software for mathematics,\nscience, and engineering. It includes modules for statistics, optimization,\nintegration, linear algebra, Fourier transforms, signal and image processing,\nODE solvers, and more.\n\n- **Website:** https://www.scipy.org/\n- **Documentation:** https://docs.scipy.org/\n- **Mailing list:** https://scipy.org/scipylib/mailing-lists.html\n- **Source code:** https://github.com/scipy/scipy\n- **Bug reports:** https://github.com/scipy/scipy/issues\n- **Code of Conduct:** https://scipy.github.io/devdocs/dev/conduct/code_of_conduct.html\n- **Report a security vulnerability:** https://tidelift.com/docs/security\n\nSciPy depends on NumPy, which provides convenient and fast\nN-dimensional array manipulation. SciPy is built to work with\nNumPy arrays, and provides many user-friendly and efficient numerical routines,\nsuch as routines for numerical integration and optimization. Together, they\nrun on all popular operating systems, are quick to install, and are free of\ncharge. NumPy and SciPy are easy to use, but powerful enough to be depended\nupon by some of the world's leading scientists and engineers. If you need to\nmanipulate numbers on a computer and display or publish the results, give\nSciPy a try!\n\nFor the installation instructions, see INSTALL.rst.txt_.\n\nWe appreciate and welcome contributions. If you would like to take part in\nSciPy development, take a look at the file CONTRIBUTING.rst_.\n\n\n.. _CONTRIBUTING.rst:  https://github.com/scipy/scipy/blob/master/CONTRIBUTING.rst\n.. _INSTALL.rst.txt:   https://github.com/scipy/scipy/blob/master/INSTALL.rst.txt\n"}, "scrapy": {"file_name": "scrapy/scrapy/README.rst", "raw_text": "======\nScrapy\n======\n\n.. image:: https://img.shields.io/pypi/v/Scrapy.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: PyPI Version\n\n.. image:: https://img.shields.io/pypi/pyversions/Scrapy.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: Supported Python Versions\n\n.. image:: https://img.shields.io/travis/scrapy/scrapy/master.svg\n   :target: https://travis-ci.org/scrapy/scrapy\n   :alt: Build Status\n\n.. image:: https://img.shields.io/badge/wheel-yes-brightgreen.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: Wheel Status\n\n.. image:: https://img.shields.io/codecov/c/github/scrapy/scrapy/master.svg\n   :target: https://codecov.io/github/scrapy/scrapy?branch=master\n   :alt: Coverage report\n\n.. image:: https://anaconda.org/conda-forge/scrapy/badges/version.svg\n   :target: https://anaconda.org/conda-forge/scrapy\n   :alt: Conda Version\n\n\nOverview\n========\n\nScrapy is a fast high-level web crawling and web scraping framework, used to\ncrawl websites and extract structured data from their pages. It can be used for\na wide range of purposes, from data mining to monitoring and automated testing.\n\nCheck the Scrapy homepage at https://scrapy.org for more information,\nincluding a list of features.\n\nRequirements\n============\n\n* Python 3.5+\n* Works on Linux, Windows, macOS, BSD\n\nInstall\n=======\n\nThe quick way::\n\n    pip install scrapy\n\nSee the install section in the documentation at\nhttps://docs.scrapy.org/en/latest/intro/install.html for more details.\n\nDocumentation\n=============\n\nDocumentation is available online at https://docs.scrapy.org/ and in the ``docs``\ndirectory.\n\nReleases\n========\n\nYou can check https://docs.scrapy.org/en/latest/news.html for the release notes.\n\nCommunity (blog, twitter, mail list, IRC)\n=========================================\n\nSee https://scrapy.org/community/ for details.\n\nContributing\n============\n\nSee https://docs.scrapy.org/en/master/contributing.html for details.\n\nCode of Conduct\n---------------\n\nPlease note that this project is released with a Contributor Code of Conduct\n(see https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md).\n\nBy participating in this project you agree to abide by its terms.\nPlease report unacceptable behavior to opensource@scrapinghub.com.\n\nCompanies using Scrapy\n======================\n\nSee https://scrapy.org/companies/ for a list.\n\nCommercial Support\n==================\n\nSee https://scrapy.org/support/ for details.\n"}, "seaborn": {"file_name": "mwaskom/seaborn/README.md", "raw_text": "seaborn: statistical data visualization\n=======================================\n\n<div class=\"row\">\n\n<a href=https://seaborn.pydata.org/examples/scatterplot_matrix.html>\n<img src=\"https://seaborn.pydata.org/_static/scatterplot_matrix_thumb.png\" height=\"135\" width=\"135\">\n</a>\n\n<a href=https://seaborn.pydata.org/examples/errorband_lineplots.html>\n<img src=\"https://seaborn.pydata.org/_static/errorband_lineplots_thumb.png\" height=\"135\" width=\"135\">\n</a>\n\n<a href=https://seaborn.pydata.org/examples/different_scatter_variables.html>\n<img src=\"https://seaborn.pydata.org/_static/different_scatter_variables_thumb.png\" height=\"135\" width=\"135\">\n</a>\n\n<a href=https://seaborn.pydata.org/examples/many_facets.html>\n<img src=\"https://seaborn.pydata.org/_static/many_facets_thumb.png\" height=\"135\" width=\"135\">\n</a>\n\n<a href=https://seaborn.pydata.org/examples/structured_heatmap.html>\n<img src=\"https://seaborn.pydata.org/_static/structured_heatmap_thumb.png\" height=\"135\" width=\"135\">\n</a>\n\n<a href=https://seaborn.pydata.org/examples/horizontal_boxplot.html>\n<img src=\"https://seaborn.pydata.org/_static/horizontal_boxplot_thumb.png\" height=\"135\" width=\"135\">\n</a>\n\n</div>\n\n--------------------------------------\n\n[![PyPI Version](https://img.shields.io/pypi/v/seaborn.svg)](https://pypi.org/project/seaborn/)\n[![License](https://img.shields.io/pypi/l/seaborn.svg)](https://github.com/mwaskom/seaborn/blob/master/LICENSE)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.592845.svg)](https://doi.org/10.5281/zenodo.592845)\n[![Build Status](https://travis-ci.org/mwaskom/seaborn.svg?branch=master)](https://travis-ci.org/mwaskom/seaborn)\n[![Code Coverage](https://codecov.io/gh/mwaskom/seaborn/branch/master/graph/badge.svg)](https://codecov.io/gh/mwaskom/seaborn)\n\nSeaborn is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics.\n\n\nDocumentation\n-------------\n\nOnline documentation is available at [seaborn.pydata.org](https://seaborn.pydata.org).\n\nThe docs include a [tutorial](https://seaborn.pydata.org/tutorial.html), [example gallery](https://seaborn.pydata.org/examples/index.html), [API reference](https://seaborn.pydata.org/api.html), and other useful information.\n\n\nDependencies\n------------\n\nSeaborn supports Python 3.6+ and no longer supports Python 2.\n\nInstallation requires [numpy](https://numpy.org/), [scipy](https://www.scipy.org/), [pandas](https://pandas.pydata.org/), and [matplotlib](https://matplotlib.org/). Some functions will optionally use [statsmodels](https://www.statsmodels.org/) if it is installed.\n\n\nInstallation\n------------\n\nThe latest stable release (and older versions) can be installed from PyPI:\n\n    pip install seaborn\n\nYou may instead want to use the development version from Github:\n\n    pip install git+https://github.com/mwaskom/seaborn.git#egg=seaborn\n\n\nTesting\n-------\n\nTo test the code, run `make test` in the source directory. This will exercise both the unit tests and docstring examples (using `pytest`).\n\nThe doctests require a network connection (unless all example datasets are cached), but the unit tests can be run offline with `make unittests`. Run `make coverage` to generate a test coverage report and `make lint` to check code style consistency.\n\n \nDevelopment\n-----------\n\nSeaborn development takes place on Github: https://github.com/mwaskom/seaborn\n\nPlease submit bugs that you encounter to the [issue tracker](https://github.com/mwaskom/seaborn/issues) with a reproducible example demonstrating the problem. Questions about usage are more at home on StackOverflow, where there is a [seaborn tag](https://stackoverflow.com/questions/tagged/seaborn).\n\n"}, "shap": {"file_name": "slundberg/shap/README.md", "raw_text": "\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_header.png\" width=\"800\" />\n</p>\n\n---\n<a href=\"https://travis-ci.org/slundberg/shap\"><img src=\"https://travis-ci.org/slundberg/shap.svg?branch=master\"></a>\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/slundberg/shap/master)\n\n**SHAP (SHapley Additive exPlanations)** is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see [papers](#citations) for details and citations).\n\n<!--**SHAP (SHapley Additive exPlanations)** is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see our [papers](#citations) for details and citations).-->\n\n\n\n## Install\n\nShap can be installed from either [PyPI](https://pypi.org/project/shap) or [conda-forge](https://anaconda.org/conda-forge/shap):\n\n<pre>\npip install shap\n<i>or</i>\nconda install -c conda-forge shap\n</pre>\n\n## Tree ensemble example with TreeExplainer (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)\n\nWhile SHAP can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (see our [Nature MI paper](https://rdcu.be/b0z70)). Fast C++ implementations are supported for *XGBoost*, *LightGBM*, *CatBoost*, *scikit-learn* and *pyspark* tree models:\n\n```python\nimport xgboost\nimport shap\n\n# load JS visualization code to notebook\nshap.initjs()\n\n# train XGBoost model\nX,y = shap.datasets.boston()\nmodel = xgboost.train({\"learning_rate\": 0.01}, xgboost.DMatrix(X, label=y), 100)\n\n# explain the model's predictions using SHAP\n# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)\n\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\nshap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])\n```\n\n<p align=\"center\">\n  <img width=\"811\" src=\"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png\" />\n</p>\n\n<!--If you want to use `matplotlib` backend in place of javascript, you can do so as shown below. You can also rotate the feature names using `text_rotation` parameter, if your dataset has really long feature names.\n```python\n%matplotlib inline\nimport xgboost\nimport shap\n\n# train XGBoost model\nX,y = shap.datasets.boston()\n\nmodel = xgboost.train({\"learning_rate\": 0.01}, xgboost.DMatrix(X, label=y), 100)\n# explain the model's predictions using SHAP\n# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)\n# visualize the first prediction's explanation using matplotlib (no javascript needed)\n# rotate the annotations so that they are legible when you have really long attribute names\nshap.force_plot(\n    explainer.expected_value, \n    shap_values[0, :], \n    X.iloc[0, :], \n    matplotlib=True, \n    text_rotation=30, \n    show=True\n)\n```\n\n<p align=\"center\">\n  <img width=\"811\" src=\"https://raw.githubusercontent.com/vatsan/shap/master/docs/artwork/force_plot_matplotlib_rotate.png\" />\n</p>-->\n\nThe above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue (these force plots are introduced in our [Nature BME paper](https://rdcu.be/baVbR)).\n\nIf we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset (in the notebook this plot is interactive):\n\n```python\n# visualize the training set predictions\nshap.force_plot(explainer.expected_value, shap_values, X)\n```\n\n<p align=\"center\">\n  <img width=\"811\" src=\"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png\" />\n</p>\n\nTo understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature's responsibility for a change in the model output, the plot below represents the change in predicted house price as RM (the average number of rooms per house in an area) changes. Vertical dispersion at a single value of RM represents interaction effects with other features. To help reveal these interactions `dependence_plot` automatically selects another feature for coloring. In this case coloring by RAD (index of accessibility to radial highways) highlights that the average number of rooms per house has less impact on home price for areas with a high RAD value.\n\n```python\n# create a dependence plot to show the effect of a single feature across the whole dataset\nshap.dependence_plot(\"RM\", shap_values, X)\n```\n\n<p align=\"center\">\n  <img width=\"544\" src=\"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dependence_plot.png\" />\n</p>\n\n\nTo get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). This reveals for example that a high LSTAT (% lower status of the population) lowers the predicted home price.\n\n```python\n# summarize the effects of all the features\nshap.summary_plot(shap_values, X)\n```\n\n<p align=\"center\">\n  <img width=\"483\" src=\"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot.png\" />\n</p>\n\nWe can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):\n\n```python\nshap.summary_plot(shap_values, X, plot_type=\"bar\")\n```\n\n<p align=\"center\">\n  <img width=\"470\" src=\"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot_bar.png\" />\n</p>\n\n## Deep learning example with DeepExplainer (TensorFlow/Keras models)\n\nDeep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with [DeepLIFT](https://arxiv.org/abs/1704.02685) described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. TensorFlow models and Keras models using the TensorFlow backend are supported (there is also preliminary support for PyTorch):\n\n```python\n# ...include code from https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n\nimport shap\nimport numpy as np\n\n# select a set of background examples to take an expectation over\nbackground = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]\n\n# explain predictions of the model on four images\ne = shap.DeepExplainer(model, background)\n# ...or pass tensors directly\n# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)\nshap_values = e.shap_values(x_test[1:5])\n\n# plot the feature attributions\nshap.image_plot(shap_values, -x_test[1:5])\n```\n\n<p align=\"center\">\n  <img width=\"820\" src=\"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png\" />\n</p>\n\nThe plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model's output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the 'zero' image the blank middle is important, while for the 'four' image the lack of a connection on top makes it a four instead of a nine.\n\n\n## Deep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)\n\nExpected gradients combines ideas from [Integrated Gradients](https://arxiv.org/abs/1703.01365), SHAP, and [SmoothGrad](https://arxiv.org/abs/1706.03825) into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities.\n\n```python\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nimport keras.backend as K\nimport numpy as np\nimport json\nimport shap\n\n# load pre-trained model and choose two images to explain\nmodel = VGG16(weights='imagenet', include_top=True)\nX,y = shap.datasets.imagenet50()\nto_explain = X[[39,41]]\n\n# load the ImageNet class names\nurl = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\nfname = shap.datasets.cache(url)\nwith open(fname) as f:\n    class_names = json.load(f)\n\n# explain how the input to the 7th layer of the model explains the top two classes\ndef map2layer(x, layer):\n    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n    return K.get_session().run(model.layers[layer].input, feed_dict)\ne = shap.GradientExplainer(\n    (model.layers[7].input, model.layers[-1].output),\n    map2layer(X, 7),\n    local_smoothing=0 # std dev of smoothing noise\n)\nshap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n\n# get the names for the classes\nindex_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n\n# plot the explanations\nshap.image_plot(shap_values, to_explain, index_names)\n```\n\n<p align=\"center\">\n  <img width=\"500\" src=\"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png\" />\n</p>\n\nPredictions for two input images are explained in the plot above. Red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. By using `ranked_outputs=2` we explain only the two most likely classes for each input (this spares us from explaining all 1,000 classes).\n\n## Model agnostic example with KernelExplainer (explains any function)\n\nKernel SHAP uses a specially-weighted local linear regression to estimate SHAP values for any model. Below is a simple example for explaining a multi-class SVM on the classic iris dataset.\n\n```python\nimport sklearn\nimport shap\nfrom sklearn.model_selection import train_test_split\n\n# print the JS visualization code to the notebook\nshap.initjs()\n\n# train a SVM classifier\nX_train,X_test,Y_train,Y_test = train_test_split(*shap.datasets.iris(), test_size=0.2, random_state=0)\nsvm = sklearn.svm.SVC(kernel='rbf', probability=True)\nsvm.fit(X_train, Y_train)\n\n# use Kernel SHAP to explain test set predictions\nexplainer = shap.KernelExplainer(svm.predict_proba, X_train, link=\"logit\")\nshap_values = explainer.shap_values(X_test, nsamples=100)\n\n# plot the SHAP values for the Setosa output of the first instance\nshap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link=\"logit\")\n```\n<p align=\"center\">\n  <img width=\"810\" src=\"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png\" />\n</p>\n\nThe above explanation shows four features each contributing to push the model output from the base value (the average model output over the training dataset we passed) towards zero. If there were any features pushing the class label higher they would be shown in red.\n\nIf we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset. This is exactly what we do below for all the examples in the iris test set:\n\n```python\n# plot the SHAP values for the Setosa output of all instances\nshap.force_plot(explainer.expected_value[0], shap_values[0], X_test, link=\"logit\")\n```\n<p align=\"center\">\n  <img width=\"813\" src=\"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png\" />\n</p>\n\n## SHAP Interaction Values\n\nSHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented for tree models with `shap.TreeExplainer(model).shap_interaction_values(X)`. This returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. These values often reveal interesting hidden relationships, such as how the increased risk of death peaks for men at age 60 (see the NHANES notebook for details):\n\n<p align=\"center\">\n  <img width=\"483\" src=\"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png\" />\n</p>\n\n## Sample notebooks\n\nThe notebooks below demonstrate different use cases for SHAP. Look inside the notebooks directory of the repository if you want to try playing with the original notebooks yourself.\n\n### TreeExplainer\n\nAn implementation of Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees.\n\n- [**NHANES survival model with XGBoost and SHAP interaction values**](https://slundberg.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html) - Using mortality data from 20 years of followup this notebook demonstrates how to use XGBoost and `shap` to uncover complex risk factor relationships.\n\n- [**Census income classification with LightGBM**](https://slundberg.github.io/shap/notebooks/tree_explainer/Census%20income%20classification%20with%20LightGBM.html) - Using the standard adult census income dataset, this notebook trains a gradient boosting tree model with LightGBM and then explains predictions using `shap`.\n\n- [**League of Legends Win Prediction with XGBoost**](https://slundberg.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html) - Using a Kaggle dataset of 180,000 ranked matches from League of Legends we train and explain a gradient boosting tree model with XGBoost to predict if a player will win their match.\n\n### DeepExplainer\n\nAn implementation of Deep SHAP, a faster (but only approximate) algorithm to compute SHAP values for deep learning models that is based on connections between SHAP and the DeepLIFT algorithm.\n\n- [**MNIST Digit classification with Keras**](https://slundberg.github.io/shap/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html) - Using the MNIST handwriting recognition dataset, this notebook trains a neural network with Keras and then explains predictions using `shap`.\n\n- [**Keras LSTM for IMDB Sentiment Classification**](https://slundberg.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html) - This notebook trains an LSTM with Keras on the IMDB text sentiment analysis dataset and then explains predictions using `shap`. \n\n### GradientExplainer\n\nAn implementation of expected gradients to approximate SHAP values for deep learning models. It is based on connections between SHAP and the Integrated Gradients algorithm. GradientExplainer is slower than DeepExplainer and makes different approximation assumptions.\n\n- [**Explain an Intermediate Layer of VGG16 on ImageNet**](https://slundberg.github.io/shap/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.html) - This notebook demonstrates how to explain the output of a pre-trained VGG16 ImageNet model using an internal convolutional layer.\n\n### LinearExplainer\n\nFor a linear model with independent features we can analytically compute the exact SHAP values. We can also account for feature correlation if we are willing to estimate the feature covaraince matrix. LinearExplainer supports both of these options.\n\n- [**Sentiment Analysis with Logistic Regression**](https://slundberg.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html) - This notebook demonstrates how to explain a linear logistic regression sentiment analysis model.\n\n### KernelExplainer\n\nAn implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes not assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms.\n\n- [**Census income classification with scikit-learn**](https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20scikit-learn.html) - Using the standard adult census income dataset, this notebook trains a k-nearest neighbors classifier using scikit-learn and then explains predictions using `shap`.\n\n- [**ImageNet VGG16 Model with Keras**](https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html) - Explain the classic VGG16 convolutional nerual network's predictions for an image. This works by applying the model agnostic Kernel SHAP method to a super-pixel segmented image.\n\n- [**Iris classification**](https://slundberg.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html) - A basic demonstration using the popular iris species dataset. It explains predictions from six different models in scikit-learn using `shap`.\n\n## Documentation notebooks\n\nThese notebooks comprehensively demonstrate how to use specific functions and objects. \n\n- [`shap.decision_plot` and `shap.multioutput_decision_plot`](https://slundberg.github.io/shap/notebooks/plots/decision_plot.html)\n\n- [`shap.dependence_plot`](https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html)\n\n## Methods Unified by SHAP\n\n1. *LIME:* Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. \"Why should i trust you?: Explaining the predictions of any classifier.\" Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.\n\n2. *Shapley sampling values:* Strumbelj, Erik, and Igor Kononenko. \"Explaining prediction models and individual predictions with feature contributions.\" Knowledge and information systems 41.3 (2014): 647-665.\n\n3. *DeepLIFT:* Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. \"Learning important features through propagating activation differences.\" arXiv preprint arXiv:1704.02685 (2017).\n\n4. *QII:* Datta, Anupam, Shayak Sen, and Yair Zick. \"Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems.\" Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016.\n\n5. *Layer-wise relevance propagation:* Bach, Sebastian, et al. \"On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.\" PloS one 10.7 (2015): e0130140.\n\n6. *Shapley regression values:* Lipovetsky, Stan, and Michael Conklin. \"Analysis of regression in game theory approach.\" Applied Stochastic Models in Business and Industry 17.4 (2001): 319-330.\n\n7. *Tree interpreter:* Saabas, Ando. Interpreting random forests. http://blog.datadive.net/interpreting-random-forests/\n\n## Citations\n\nThe algorithms and visualizations used in this package came primarily out of research in [Su-In Lee's lab](https://suinlee.cs.washington.edu) at the University of Washington, and Microsoft Research. If you use SHAP in your research we would appreciate a citation to the appropriate paper(s):\n\n- For general use of SHAP you can read/cite our [NeurIPS paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) ([bibtex](https://raw.githubusercontent.com/slundberg/shap/master/docs/references/shap_nips.bib)). \n- For TreeExplainer you can read/cite our [Nature Machine Intelligence paper](https://www.nature.com/articles/s42256-019-0138-9) ([bibtex](https://raw.githubusercontent.com/slundberg/shap/master/docs/references/tree_explainer.bib); [free access](https://rdcu.be/b0z70)).\n- For `force_plot` visualizations and medical applications you can read/cite our [Nature Biomedical Engineering paper](https://www.nature.com/articles/s41551-018-0304-0) ([bibtex](https://raw.githubusercontent.com/slundberg/shap/master/docs/references/nature_bme.bib); [free access](https://rdcu.be/baVbR)).\n\n<img height=\"1\" width=\"1\" style=\"display:none\" src=\"https://www.facebook.com/tr?id=189147091855991&ev=PageView&noscript=1\" />\n"}, "skorch": {"file_name": "skorch-dev/skorch/README.rst", "raw_text": ".. image:: https://github.com/skorch-dev/skorch/blob/master/assets/skorch.svg\n   :width: 30%\n\n------------\n\n|build| |coverage| |docs| |powered|\n\nA scikit-learn compatible neural network library that wraps PyTorch.\n\n.. |build| image:: https://api.travis-ci.org/skorch-dev/skorch.svg?branch=master\n    :alt: Build Status\n    :scale: 100%\n    :target: https://travis-ci.org/skorch-dev/skorch?branch=master\n\n.. |coverage| image:: https://github.com/skorch-dev/skorch/blob/master/assets/coverage.svg\n    :alt: Test Coverage\n    :scale: 100%\n\n.. |docs| image:: https://readthedocs.org/projects/skorch/badge/?version=latest\n    :alt: Documentation Status\n    :scale: 100%\n    :target: https://skorch.readthedocs.io/en/latest/?badge=latest\n\n.. |powered| image:: https://github.com/skorch-dev/skorch/blob/master/assets/powered.svg\n    :alt: Powered by\n    :scale: 100%\n    :target: https://github.com/ottogroup/\n\n=========\nResources\n=========\n\n- `Documentation <https://skorch.readthedocs.io/en/latest/?badge=latest>`_\n- `Source Code <https://github.com/skorch-dev/skorch/>`_\n\n========\nExamples\n========\n\nTo see more elaborate examples, look `here\n<https://github.com/skorch-dev/skorch/tree/master/notebooks/README.md>`__.\n\n.. code:: python\n\n    import numpy as np\n    from sklearn.datasets import make_classification\n    from torch import nn\n    import torch.nn.functional as F\n\n    from skorch import NeuralNetClassifier\n\n\n    X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n    X = X.astype(np.float32)\n    y = y.astype(np.int64)\n\n    class MyModule(nn.Module):\n        def __init__(self, num_units=10, nonlin=F.relu):\n            super(MyModule, self).__init__()\n\n            self.dense0 = nn.Linear(20, num_units)\n            self.nonlin = nonlin\n            self.dropout = nn.Dropout(0.5)\n            self.dense1 = nn.Linear(num_units, 10)\n            self.output = nn.Linear(10, 2)\n\n        def forward(self, X, **kwargs):\n            X = self.nonlin(self.dense0(X))\n            X = self.dropout(X)\n            X = F.relu(self.dense1(X))\n            X = F.softmax(self.output(X), dim=-1)\n            return X\n\n\n    net = NeuralNetClassifier(\n        MyModule,\n        max_epochs=10,\n        lr=0.1,\n        # Shuffle training data on each epoch\n        iterator_train__shuffle=True,\n    )\n\n    net.fit(X, y)\n    y_proba = net.predict_proba(X)\n\nIn an sklearn Pipeline:\n\n.. code:: python\n\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n\n\n    pipe = Pipeline([\n        ('scale', StandardScaler()),\n        ('net', net),\n    ])\n\n    pipe.fit(X, y)\n    y_proba = pipe.predict_proba(X)\n\nWith grid search\n\n.. code:: python\n\n    from sklearn.model_selection import GridSearchCV\n\n\n    params = {\n        'lr': [0.01, 0.02],\n        'max_epochs': [10, 20],\n        'module__num_units': [10, 20],\n    }\n    gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy')\n\n    gs.fit(X, y)\n    print(gs.best_score_, gs.best_params_)\n\nskorch also provides many convenient features, among others:\n\n- `Learning rate schedulers <https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.LRScheduler>`_ (Warm restarts, cyclic LR and many more)\n- `Scoring using sklearn (and custom) scoring functions <https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.EpochScoring>`_\n- `Early stopping <https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.EarlyStopping>`_\n- `Checkpointing <https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.Checkpoint>`_\n- `Parameter freezing/unfreezing <https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.Freezer>`_\n- `Progress bar <https://skorch.readthedocs.io/en/stable/callbacks.html#skorch.callbacks.ProgressBar>`_ (for CLI as well as jupyter)\n- `Automatic inference of CLI parameters <https://github.com/skorch-dev/skorch/tree/master/examples/cli>`_\n\n============\nInstallation\n============\n\nskorch requires Python 3.5 or higher.\n\npip installation\n================\n\nTo install with pip, run:\n\n.. code:: bash\n\n    pip install -U skorch\n\nWe recommend to use a virtual environment for this.\n\nFrom source\n===========\n\nIf you would like to use the must recent additions to skorch or\nhelp development, you should install skorch from source:\n\n.. code:: bash\n\n    git clone https://github.com/skorch-dev/skorch.git\n    cd skorch\n    # install pytorch version for your system (see below)\n    python setup.py install\n\nUsing conda\n===========\n\nYou need a working conda installation. Get the correct miniconda for\nyour system from `here <https://conda.io/miniconda.html>`__.\n\nYou can also install skorch through the conda-forge channel. \nThe instructions for doing so are \navailable `here <https://github.com/conda-forge/skorch-feedstock>`__.\n**Note**: The conda channel is _not_ managed by the skorch maintainers.\n\nIf you do not want to use conda-forge, you may install skorch using:\n\n.. code:: bash\n\n    git clone https://github.com/skorch-dev/skorch.git\n    cd skorch\n    conda env create\n    source activate skorch\n    # install pytorch version for your system (see below)\n    python setup.py install\n\nIf you want to help developing, run:\n\n.. code:: bash\n\n    git clone https://github.com/skorch-dev/skorch.git\n    cd skorch\n    conda env create\n    source activate skorch\n    # install pytorch version for your system (see below)\n    conda install -c conda-forge --file requirements-dev.txt\n    python setup.py develop\n\n    py.test  # unit tests\n    pylint skorch  # static code checks\n\nUsing pip\n=========\n\nIf you just want to use skorch, use:\n\n.. code:: bash\n\n    git clone https://github.com/skorch-dev/skorch.git\n    cd skorch\n    # create and activate a virtual environment\n    pip install -r requirements.txt\n    # install pytorch version for your system (see below)\n    python setup.py install\n\nIf you want to help developing, run:\n\n.. code:: bash\n\n    git clone https://github.com/skorch-dev/skorch.git\n    cd skorch\n    # create and activate a virtual environment\n    pip install -r requirements.txt\n    # install pytorch version for your system (see below)\n    pip install -r requirements-dev.txt\n    python setup.py develop\n\n    py.test  # unit tests\n    pylint skorch  # static code checks\n\nPyTorch\n=======\n\nPyTorch is not covered by the dependencies, since the PyTorch version\nyou need is dependent on your system. For installation instructions\nfor PyTorch, visit the `PyTorch website\n<http://pytorch.org/>`__. skorch officially supports the following\nPyTorch versions:\n\n- 1.1.0\n- 1.2.0\n- 1.3.1\n- 1.4.0\n\nIn general, this should work (assuming CUDA 9):\n\n.. code:: bash\n\n    # using conda:\n    conda install pytorch cudatoolkit=9.0 -c pytorch\n    # using pip\n    pip install torch\n\n=============\nCommunication\n=============\n\n- `GitHub issues <https://github.com/skorch-dev/skorch/issues>`_: bug\n  reports, feature requests, install issues, RFCs, thoughts, etc.\n\n- Slack: We run the #skorch channel on the `PyTorch Slack server\n  <https://pytorch.slack.com/>`_, for which you can `request access\n  here <https://bit.ly/ptslack>`_.\n"}, "spacy": {"file_name": "explosion/spaCy/README.md", "raw_text": "<a href=\"https://explosion.ai\"><img src=\"https://explosion.ai/assets/img/logo.svg\" width=\"125\" height=\"125\" align=\"right\" /></a>\n\n# spaCy: Industrial-strength NLP\n\nspaCy is a library for advanced Natural Language Processing in Python and\nCython. It's built on the very latest research, and was designed from day one to\nbe used in real products. spaCy comes with\n[pretrained statistical models](https://spacy.io/models) and word vectors, and\ncurrently supports tokenization for **50+ languages**. It features\nstate-of-the-art speed, convolutional **neural network models** for tagging,\nparsing and **named entity recognition** and easy **deep learning** integration.\nIt's commercial open-source software, released under the MIT license.\n\n\ud83d\udcab **Version 2.2 out now!**\n[Check out the release notes here.](https://github.com/explosion/spaCy/releases)\n\n[![Azure Pipelines](<https://img.shields.io/azure-devops/build/explosion-ai/public/8/master.svg?logo=azure-pipelines&style=flat-square&label=build+(3.x)>)](https://dev.azure.com/explosion-ai/public/_build?definitionId=8)\n[![Travis Build Status](<https://img.shields.io/travis/explosion/spaCy/master.svg?style=flat-square&logo=travis-ci&logoColor=white&label=build+(2.7)>)](https://travis-ci.org/explosion/spaCy)\n[![Current Release Version](https://img.shields.io/github/release/explosion/spacy.svg?style=flat-square&logo=github)](https://github.com/explosion/spaCy/releases)\n[![pypi Version](https://img.shields.io/pypi/v/spacy.svg?style=flat-square&logo=pypi&logoColor=white)](https://pypi.org/project/spacy/)\n[![conda Version](https://img.shields.io/conda/vn/conda-forge/spacy.svg?style=flat-square&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/spacy)\n[![Python wheels](https://img.shields.io/badge/wheels-%E2%9C%93-4c1.svg?longCache=true&style=flat-square&logo=python&logoColor=white)](https://github.com/explosion/wheelwright/releases)\n[![PyPi downloads](https://img.shields.io/pypi/dm/spacy?style=flat-square&logo=pypi&logoColor=white)](https://pypi.org/project/spacy/)\n[![Conda downloads](https://img.shields.io/conda/dn/conda-forge/spacy?style=flat-square&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/spacy)\n[![Model downloads](https://img.shields.io/github/downloads/explosion/spacy-models/total?style=flat-square&label=model+downloads)](https://github.com/explosion/spacy-models/releases)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/ambv/black)\n[![spaCy on Twitter](https://img.shields.io/twitter/follow/spacy_io.svg?style=social&label=Follow)](https://twitter.com/spacy_io)\n\n## \ud83d\udcd6 Documentation\n\n| Documentation   |                                                                |\n| --------------- | -------------------------------------------------------------- |\n| [spaCy 101]     | New to spaCy? Here's everything you need to know!              |\n| [Usage Guides]  | How to use spaCy and its features.                             |\n| [New in v2.2]   | New features, backwards incompatibilities and migration guide. |\n| [API Reference] | The detailed reference for spaCy's API.                        |\n| [Models]        | Download statistical language models for spaCy.                |\n| [Universe]      | Libraries, extensions, demos, books and courses.               |\n| [Changelog]     | Changes and version history.                                   |\n| [Contribute]    | How to contribute to the spaCy project and code base.          |\n\n[spacy 101]: https://spacy.io/usage/spacy-101\n[new in v2.2]: https://spacy.io/usage/v2-2\n[usage guides]: https://spacy.io/usage/\n[api reference]: https://spacy.io/api/\n[models]: https://spacy.io/models\n[universe]: https://spacy.io/universe\n[changelog]: https://spacy.io/usage#changelog\n[contribute]: https://github.com/explosion/spaCy/blob/master/CONTRIBUTING.md\n\n## \ud83d\udcac Where to ask questions\n\nThe spaCy project is maintained by [@honnibal](https://github.com/honnibal) and\n[@ines](https://github.com/ines), along with core contributors\n[@svlandeg](https://github.com/svlandeg) and\n[@adrianeboyd](https://github.com/adrianeboyd). Please understand that we won't\nbe able to provide individual support via email. We also believe that help is\nmuch more valuable if it's shared publicly, so that more people can benefit from\nit.\n\n| Type                     | Platforms                                              |\n| ------------------------ | ------------------------------------------------------ |\n| \ud83d\udea8 **Bug Reports**       | [GitHub Issue Tracker]                                 |\n| \ud83c\udf81 **Feature Requests**  | [GitHub Issue Tracker]                                 |\n| \ud83d\udc69\u200d\ud83d\udcbb **Usage Questions**   | [Stack Overflow] \u00b7 [Gitter Chat] \u00b7 [Reddit User Group] |\n| \ud83d\uddef **General Discussion** | [Gitter Chat] \u00b7 [Reddit User Group]                    |\n\n[github issue tracker]: https://github.com/explosion/spaCy/issues\n[stack overflow]: https://stackoverflow.com/questions/tagged/spacy\n[gitter chat]: https://gitter.im/explosion/spaCy\n[reddit user group]: https://www.reddit.com/r/spacynlp\n\n## Features\n\n- Non-destructive **tokenization**\n- **Named entity** recognition\n- Support for **50+ languages**\n- pretrained [statistical models](https://spacy.io/models) and word vectors\n- State-of-the-art speed\n- Easy **deep learning** integration\n- Part-of-speech tagging\n- Labelled dependency parsing\n- Syntax-driven sentence segmentation\n- Built in **visualizers** for syntax and NER\n- Convenient string-to-hash mapping\n- Export to numpy data arrays\n- Efficient binary serialization\n- Easy **model packaging** and deployment\n- Robust, rigorously evaluated accuracy\n\n\ud83d\udcd6 **For more details, see the\n[facts, figures and benchmarks](https://spacy.io/usage/facts-figures).**\n\n## Install spaCy\n\nFor detailed installation instructions, see the\n[documentation](https://spacy.io/usage).\n\n- **Operating system**: macOS / OS X \u00b7 Linux \u00b7 Windows (Cygwin, MinGW, Visual\n  Studio)\n- **Python version**: Python 2.7, 3.5+ (only 64 bit)\n- **Package managers**: [pip] \u00b7 [conda] (via `conda-forge`)\n\n[pip]: https://pypi.org/project/spacy/\n[conda]: https://anaconda.org/conda-forge/spacy\n\n### pip\n\nUsing pip, spaCy releases are available as source packages and binary wheels (as\nof `v2.0.13`).\n\n```bash\npip install spacy\n```\n\nTo install additional data tables for lemmatization in **spaCy v2.2+** you can\nrun `pip install spacy[lookups]` or install\n[`spacy-lookups-data`](https://github.com/explosion/spacy-lookups-data)\nseparately. The lookups package is needed to create blank models with\nlemmatization data, and to lemmatize in languages that don't yet come with\npretrained models and aren't powered by third-party libraries.\n\nWhen using pip it is generally recommended to install packages in a virtual\nenvironment to avoid modifying system state:\n\n```bash\npython -m venv .env\nsource .env/bin/activate\npip install spacy\n```\n\n### conda\n\nThanks to our great community, we've finally re-added conda support. You can now\ninstall spaCy via `conda-forge`:\n\n```bash\nconda install -c conda-forge spacy\n```\n\nFor the feedstock including the build recipe and configuration, check out\n[this repository](https://github.com/conda-forge/spacy-feedstock). Improvements\nand pull requests to the recipe and setup are always appreciated.\n\n### Updating spaCy\n\nSome updates to spaCy may require downloading new statistical models. If you're\nrunning spaCy v2.0 or higher, you can use the `validate` command to check if\nyour installed models are compatible and if not, print details on how to update\nthem:\n\n```bash\npip install -U spacy\npython -m spacy validate\n```\n\nIf you've trained your own models, keep in mind that your training and runtime\ninputs must match. After updating spaCy, we recommend **retraining your models**\nwith the new version.\n\n\ud83d\udcd6 **For details on upgrading from spaCy 1.x to spaCy 2.x, see the\n[migration guide](https://spacy.io/usage/v2#migrating).**\n\n## Download models\n\nAs of v1.7.0, models for spaCy can be installed as **Python packages**. This\nmeans that they're a component of your application, just like any other module.\nModels can be installed using spaCy's `download` command, or manually by\npointing pip to a path or URL.\n\n| Documentation          |                                                               |\n| ---------------------- | ------------------------------------------------------------- |\n| [Available Models]     | Detailed model descriptions, accuracy figures and benchmarks. |\n| [Models Documentation] | Detailed usage instructions.                                  |\n\n[available models]: https://spacy.io/models\n[models documentation]: https://spacy.io/docs/usage/models\n\n```bash\n# download best-matching version of specific model for your spaCy installation\npython -m spacy download en_core_web_sm\n\n# pip install .tar.gz archive from path or URL\npip install /Users/you/en_core_web_sm-2.2.0.tar.gz\npip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n```\n\n### Loading and using models\n\nTo load a model, use `spacy.load()` with the model name, a shortcut link or a\npath to the model data directory.\n\n```python\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"This is a sentence.\")\n```\n\nYou can also `import` a model directly via its full name and then call its\n`load()` method with no arguments.\n\n```python\nimport spacy\nimport en_core_web_sm\n\nnlp = en_core_web_sm.load()\ndoc = nlp(\"This is a sentence.\")\n```\n\n\ud83d\udcd6 **For more info and examples, check out the\n[models documentation](https://spacy.io/docs/usage/models).**\n\n## Compile from source\n\nThe other way to install spaCy is to clone its\n[GitHub repository](https://github.com/explosion/spaCy) and build it from\nsource. That is the common way if you want to make changes to the code base.\nYou'll need to make sure that you have a development environment consisting of a\nPython distribution including header files, a compiler,\n[pip](https://pip.pypa.io/en/latest/installing/),\n[virtualenv](https://virtualenv.pypa.io/en/latest/) and\n[git](https://git-scm.com) installed. The compiler part is the trickiest. How to\ndo that depends on your system. See notes on Ubuntu, OS X and Windows for\ndetails.\n\n```bash\n# make sure you are using the latest pip\npython -m pip install -U pip\ngit clone https://github.com/explosion/spaCy\ncd spaCy\n\npython -m venv .env\nsource .env/bin/activate\nexport PYTHONPATH=`pwd`\npip install -r requirements.txt\npython setup.py build_ext --inplace\n```\n\nCompared to regular install via pip, [requirements.txt](requirements.txt)\nadditionally installs developer dependencies such as Cython. For more details\nand instructions, see the documentation on\n[compiling spaCy from source](https://spacy.io/usage#source) and the\n[quickstart widget](https://spacy.io/usage#section-quickstart) to get the right\ncommands for your platform and Python version.\n\n### Ubuntu\n\nInstall system-level dependencies via `apt-get`:\n\n```bash\nsudo apt-get install build-essential python-dev git\n```\n\n### macOS / OS X\n\nInstall a recent version of [XCode](https://developer.apple.com/xcode/),\nincluding the so-called \"Command Line Tools\". macOS and OS X ship with Python\nand git preinstalled.\n\n### Windows\n\nInstall a version of the\n[Visual C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/)\nor [Visual Studio Express](https://visualstudio.microsoft.com/vs/express/) that\nmatches the version that was used to compile your Python interpreter. For\nofficial distributions these are VS 2008 (Python 2.7), VS 2010 (Python 3.4) and\nVS 2015 (Python 3.5).\n\n## Run tests\n\nspaCy comes with an [extensive test suite](spacy/tests). In order to run the\ntests, you'll usually want to clone the repository and build spaCy from source.\nThis will also install the required development dependencies and test utilities\ndefined in the `requirements.txt`.\n\nAlternatively, you can find out where spaCy is installed and run `pytest` on\nthat directory. Don't forget to also install the test utilities via spaCy's\n`requirements.txt`:\n\n```bash\npython -c \"import os; import spacy; print(os.path.dirname(spacy.__file__))\"\npip install -r path/to/requirements.txt\npython -m pytest <spacy-directory>\n```\n\nSee [the documentation](https://spacy.io/usage#tests) for more details and\nexamples.\n"}, "sqlalchemy": {"file_name": "sqlalchemy/sqlalchemy/README.rst", "raw_text": "SQLAlchemy\n==========\n\nThe Python SQL Toolkit and Object Relational Mapper\n\nIntroduction\n-------------\n\nSQLAlchemy is the Python SQL toolkit and Object Relational Mapper\nthat gives application developers the full power and\nflexibility of SQL. SQLAlchemy provides a full suite\nof well known enterprise-level persistence patterns,\ndesigned for efficient and high-performing database\naccess, adapted into a simple and Pythonic domain\nlanguage.\n\nMajor SQLAlchemy features include:\n\n* An industrial strength ORM, built\n  from the core on the identity map, unit of work,\n  and data mapper patterns.   These patterns\n  allow transparent persistence of objects\n  using a declarative configuration system.\n  Domain models\n  can be constructed and manipulated naturally,\n  and changes are synchronized with the\n  current transaction automatically.\n* A relationally-oriented query system, exposing\n  the full range of SQL's capabilities\n  explicitly, including joins, subqueries,\n  correlation, and most everything else,\n  in terms of the object model.\n  Writing queries with the ORM uses the same\n  techniques of relational composition you use\n  when writing SQL.  While you can drop into\n  literal SQL at any time, it's virtually never\n  needed.\n* A comprehensive and flexible system\n  of eager loading for related collections and objects.\n  Collections are cached within a session,\n  and can be loaded on individual access, all\n  at once using joins, or by query per collection\n  across the full result set.\n* A Core SQL construction system and DBAPI\n  interaction layer.  The SQLAlchemy Core is\n  separate from the ORM and is a full database\n  abstraction layer in its own right, and includes\n  an extensible Python-based SQL expression\n  language, schema metadata, connection pooling,\n  type coercion, and custom types.\n* All primary and foreign key constraints are\n  assumed to be composite and natural.  Surrogate\n  integer primary keys are of course still the\n  norm, but SQLAlchemy never assumes or hardcodes\n  to this model.\n* Database introspection and generation.  Database\n  schemas can be \"reflected\" in one step into\n  Python structures representing database metadata;\n  those same structures can then generate\n  CREATE statements right back out - all within\n  the Core, independent of the ORM.\n\nSQLAlchemy's philosophy:\n\n* SQL databases behave less and less like object\n  collections the more size and performance start to\n  matter; object collections behave less and less like\n  tables and rows the more abstraction starts to matter.\n  SQLAlchemy aims to accommodate both of these\n  principles.\n* An ORM doesn't need to hide the \"R\".   A relational\n  database provides rich, set-based functionality\n  that should be fully exposed.   SQLAlchemy's\n  ORM provides an open-ended set of patterns\n  that allow a developer to construct a custom\n  mediation layer between a domain model and\n  a relational schema, turning the so-called\n  \"object relational impedance\" issue into\n  a distant memory.\n* The developer, in all cases, makes all decisions\n  regarding the design, structure, and naming conventions\n  of both the object model as well as the relational\n  schema.   SQLAlchemy only provides the means\n  to automate the execution of these decisions.\n* With SQLAlchemy, there's no such thing as\n  \"the ORM generated a bad query\" - you\n  retain full control over the structure of\n  queries, including how joins are organized,\n  how subqueries and correlation is used, what\n  columns are requested.  Everything SQLAlchemy\n  does is ultimately the result of a developer-\n  initiated decision.\n* Don't use an ORM if the problem doesn't need one.\n  SQLAlchemy consists of a Core and separate ORM\n  component.   The Core offers a full SQL expression\n  language that allows Pythonic construction\n  of SQL constructs that render directly to SQL\n  strings for a target database, returning\n  result sets that are essentially enhanced DBAPI\n  cursors.\n* Transactions should be the norm.  With SQLAlchemy's\n  ORM, nothing goes to permanent storage until\n  commit() is called.  SQLAlchemy encourages applications\n  to create a consistent means of delineating\n  the start and end of a series of operations.\n* Never render a literal value in a SQL statement.\n  Bound parameters are used to the greatest degree\n  possible, allowing query optimizers to cache\n  query plans effectively and making SQL injection\n  attacks a non-issue.\n\nDocumentation\n-------------\n\nLatest documentation is at:\n\nhttp://www.sqlalchemy.org/docs/\n\nInstallation / Requirements\n---------------------------\n\nFull documentation for installation is at\n`Installation <http://www.sqlalchemy.org/docs/intro.html#installation>`_.\n\nGetting Help / Development / Bug reporting\n------------------------------------------\n\nPlease refer to the `SQLAlchemy Community Guide <http://www.sqlalchemy.org/support.html>`_.\n\nCode of Conduct\n---------------\n\nAbove all, SQLAlchemy places great emphasis on polite, thoughtful, and\nconstructive communication between users and developers.\nPlease see our current Code of Conduct at\n`Code of Conduct <http://www.sqlalchemy.org/codeofconduct.html>`_.\n\nLicense\n-------\n\nSQLAlchemy is distributed under the `MIT license\n<http://www.opensource.org/licenses/mit-license.php>`_.\n\n"}, "statsmodels": {"file_name": "statsmodels/statsmodels/README.rst", "raw_text": "|Travis Build Status| |Azure CI Build Status| |Appveyor Build Status| |Coveralls Coverage|\n\nAbout statsmodels\n=================\n\nstatsmodels is a Python package that provides a complement to scipy for\nstatistical computations including descriptive statistics and estimation\nand inference for statistical models.\n\n\nDocumentation\n=============\n\nThe documentation for the latest release is at\n\nhttps://www.statsmodels.org/stable/\n\nThe documentation for the development version is at\n\nhttps://www.statsmodels.org/dev/\n\nRecent improvements are highlighted in the release notes\n\nhttps://www.statsmodels.org/stable/release/version0.9.html\n\nBackups of documentation are available at https://statsmodels.github.io/stable/\nand https://statsmodels.github.io/dev/.\n\n\nMain Features\n=============\n\n* Linear regression models:\n\n  - Ordinary least squares\n  - Generalized least squares\n  - Weighted least squares\n  - Least squares with autoregressive errors\n  - Quantile regression\n  - Recursive least squares\n\n* Mixed Linear Model with mixed effects and variance components\n* GLM: Generalized linear models with support for all of the one-parameter\n  exponential family distributions\n* Bayesian Mixed GLM for Binomial and Poisson\n* GEE: Generalized Estimating Equations for one-way clustered or longitudinal data\n* Discrete models:\n\n  - Logit and Probit\n  - Multinomial logit (MNLogit)\n  - Poisson and Generalized Poisson regression\n  - Negative Binomial regression\n  - Zero-Inflated Count models\n\n* RLM: Robust linear models with support for several M-estimators.\n* Time Series Analysis: models for time series analysis\n\n  - Complete StateSpace modeling framework\n\n    - Seasonal ARIMA and ARIMAX models\n    - VARMA and VARMAX models\n    - Dynamic Factor models\n    - Unobserved Component models\n\n  - Markov switching models (MSAR), also known as Hidden Markov Models (HMM)\n  - Univariate time series analysis: AR, ARIMA\n  - Vector autoregressive models, VAR and structural VAR\n  - Vector error correction modle, VECM\n  - exponential smoothing, Holt-Winters\n  - Hypothesis tests for time series: unit root, cointegration and others\n  - Descriptive statistics and process models for time series analysis\n\n* Survival analysis:\n\n  - Proportional hazards regression (Cox models)\n  - Survivor function estimation (Kaplan-Meier)\n  - Cumulative incidence function estimation\n\n* Multivariate:\n\n  - Principal Component Analysis with missing data\n  - Factor Analysis with rotation\n  - MANOVA\n  - Canonical Correlation\n\n* Nonparametric statistics: Univariate and multivariate kernel density estimators\n* Datasets: Datasets used for examples and in testing\n* Statistics: a wide range of statistical tests\n\n  - diagnostics and specification tests\n  - goodness-of-fit and normality tests\n  - functions for multiple testing\n  - various additional statistical tests\n\n* Imputation with MICE, regression on order statistic and Gaussian imputation\n* Mediation analysis\n* Graphics includes plot functions for visual analysis of data and model results\n\n* I/O\n\n  - Tools for reading Stata .dta files, but pandas has a more recent version\n  - Table output to ascii, latex, and html\n\n* Miscellaneous models\n* Sandbox: statsmodels contains a sandbox folder with code in various stages of\n  development and testing which is not considered \"production ready\".  This covers\n  among others\n\n  - Generalized method of moments (GMM) estimators\n  - Kernel regression\n  - Various extensions to scipy.stats.distributions\n  - Panel data models\n  - Information theoretic measures\n\nHow to get it\n=============\nThe master branch on GitHub is the most up to date code\n\nhttps://www.github.com/statsmodels/statsmodels\n\nSource download of release tags are available on GitHub\n\nhttps://github.com/statsmodels/statsmodels/tags\n\nBinaries and source distributions are available from PyPi\n\nhttps://pypi.org/project/statsmodels/\n\nBinaries can be installed in Anaconda\n\nconda install statsmodels\n\n\nInstalling from sources\n=======================\n\nSee INSTALL.txt for requirements or see the documentation\n\nhttps://statsmodels.github.io/dev/install.html\n\nContributing\n============\nContributions in any form are welcome, including:\n\n* Documentation improvements\n* Additional tests\n* New features to existing models\n* New models\n\nhttps://statsmodels.github.io/dev/test_notes.html\n\nfor instructions on installing statsmodels in *editable* mode.\n\nLicense\n=======\n\nModified BSD (3-clause)\n\nDiscussion and Development\n==========================\n\nDiscussions take place on the mailing list\n\nhttps://groups.google.com/group/pystatsmodels\n\nand in the issue tracker. We are very interested in feedback\nabout usability and suggestions for improvements.\n\nBug Reports\n===========\n\nBug reports can be submitted to the issue tracker at\n\nhttps://github.com/statsmodels/statsmodels/issues\n\n.. |Travis Build Status| image:: https://travis-ci.org/statsmodels/statsmodels.svg?branch=master\n   :target: https://travis-ci.org/statsmodels/statsmodels\n.. |Azure CI Build Status| image:: https://dev.azure.com/statsmodels/statsmodels-testing/_apis/build/status/statsmodels.statsmodels?branch=master\n   :target: https://dev.azure.com/statsmodels/statsmodels-testing/_build/latest?definitionId=1&branch=master\n.. |Appveyor Build Status| image:: https://ci.appveyor.com/api/projects/status/gx18sd2wc63mfcuc/branch/master?svg=true\n   :target: https://ci.appveyor.com/project/josef-pkt/statsmodels/branch/master\n.. |Coveralls Coverage| image:: https://coveralls.io/repos/github/statsmodels/statsmodels/badge.svg?branch=master\n   :target: https://coveralls.io/github/statsmodels/statsmodels?branch=master\n"}, "streamz": {"file_name": "python-streamz/streamz/README.rst", "raw_text": "Streamz\n=======\n\n|Build Status| |Doc Status| |Version Status|\n\nStreamz helps you build pipelines to manage continuous streams of data. It is simple to use in simple cases, but also supports complex pipelines that involve branching, joining, flow control, feedback, back pressure, and so on.\n\nOptionally, Streamz can also work with both `Pandas <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html>`_ and `cuDF <https://docs.rapids.ai/api/cudf/stable/>`_ dataframes, to provide sensible streaming operations on continuous tabular data.\n\nTo learn more about how to use Streamz see documentation at `streamz.readthedocs.org <https://streamz.readthedocs.org>`_.\n\nLICENSE\n-------\n\nBSD-3 Clause\n\n.. |Build Status| image:: https://travis-ci.org/python-streamz/streamz.svg?branch=master\n   :target: https://travis-ci.org/python-streamz/streamz\n.. |Doc Status| image:: http://readthedocs.org/projects/streamz/badge/?version=latest\n   :target: http://streamz.readthedocs.org/en/latest/\n   :alt: Documentation Status\n.. |Version Status| image:: https://img.shields.io/pypi/v/streamz.svg\n   :target: https://pypi.python.org/pypi/streamz/\n"}, "sympy": {"file_name": "sympy/sympy/README.rst", "raw_text": "SymPy\n=====\n\n|pypi version| |Build status| |Gitter Badge| |Zenodo Badge| |codecov Badge|\n\n.. |pypi version| image:: https://img.shields.io/pypi/v/sympy.svg\n   :target: https://pypi.python.org/pypi/sympy\n.. |Build status| image:: https://secure.travis-ci.org/sympy/sympy.svg?branch=master\n   :target: https://travis-ci.org/sympy/sympy\n.. |Gitter Badge| image:: https://badges.gitter.im/Join%20Chat.svg\n   :alt: Join the chat at https://gitter.im/sympy/sympy\n   :target: https://gitter.im/sympy/sympy?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge\n.. |Zenodo Badge| image:: https://zenodo.org/badge/18918/sympy/sympy.svg\n   :target: https://zenodo.org/badge/latestdoi/18918/sympy/sympy\n.. |codecov Badge| image:: https://codecov.io/gh/sympy/sympy/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/sympy/sympy\n\nA Python library for symbolic mathematics.\n\nhttps://sympy.org/\n\nSee the AUTHORS file for the list of authors.\n\nAnd many more people helped on the SymPy mailing list, reported bugs, helped\norganize SymPy's participation in the Google Summer of Code, the Google Highly\nOpen Participation Contest, Google Code-In, wrote and blogged about SymPy...\n\nLicense: New BSD License (see the LICENSE file for details) covers all files\nin the sympy repository unless stated otherwise.\n\nOur mailing list is at\nhttps://groups.google.com/forum/?fromgroups#!forum/sympy.\n\nWe have community chat at `Gitter <https://gitter.im/sympy/sympy>`_. Feel free\nto ask us anything there. We have a very welcoming and helpful community.\n\n\nDownload\n--------\n\nThe recommended installation method is through Anaconda,\nhttps://www.anaconda.com/download/\n\nYou can also get the latest version of SymPy from\nhttps://pypi.python.org/pypi/sympy/\n\nTo get the git version do\n\n::\n\n    $ git clone git://github.com/sympy/sympy.git\n\nFor other options (tarballs, debs, etc.), see\nhttps://docs.sympy.org/dev/install.html.\n\nDocumentation and Usage\n-----------------------\n\nFor in-depth instructions on installation and building the documentation, see\nthe `SymPy Documentation Style Guide\n<https://docs.sympy.org/dev/documentation-style-guide.html>`_.\n\nEverything is at:\n\nhttps://docs.sympy.org/\n\nYou can generate everything at the above site in your local copy of SymPy by::\n\n    $ cd doc\n    $ make html\n\nThen the docs will be in `_build/html`. If you don't want to read that, here\nis a short usage:\n\nFrom this directory, start Python and:\n\n.. code-block:: python\n\n    >>> from sympy import Symbol, cos\n    >>> x = Symbol('x')\n    >>> e = 1/cos(x)\n    >>> print(e.series(x, 0, 10))\n    1 + x**2/2 + 5*x**4/24 + 61*x**6/720 + 277*x**8/8064 + O(x**10)\n\nSymPy also comes with a console that is a simple wrapper around the\nclassic python console (or IPython when available) that loads the\nSymPy namespace and executes some common commands for you.\n\nTo start it, issue::\n\n    $ bin/isympy\n\nfrom this directory, if SymPy is not installed or simply::\n\n    $ isympy\n\nif SymPy is installed.\n\nInstallation\n------------\n\nSymPy has a hard dependency on the `mpmath <http://mpmath.org/>`_\nlibrary (version >= 0.19).  You should install it first, please refer to\nthe mpmath installation guide:\n\nhttps://github.com/fredrik-johansson/mpmath#1-download--installation\n\nTo install SymPy using PyPI, run the following command::\n\n    $ pip install sympy\n\nTo install SymPy using Anaconda, run the following command::\n\n    $ conda install -c anaconda sympy\n\nTo install SymPy from GitHub source, first clone SymPy using ``git``::\n\n    $ git clone https://github.com/sympy/sympy.git\n\nThen, in the ``sympy`` repository that you cloned, simply run::\n\n    $ python setup.py install\n\nSee https://docs.sympy.org/dev/install.html for more information.\n\nContributing\n------------\n\nWe welcome contributions from anyone, even if you are new to open source. Please\nread our `Introduction to Contributing\n<https://github.com/sympy/sympy/wiki/Introduction-to-contributing>`_ page and\nthe `SymPy Documentation Style Guide\n<https://docs.sympy.org/dev/documentation-style-guide.html>`_. If you are new\nand looking for some way to contribute, a good place to start is to look at the\nissues tagged `Easy to Fix\n<https://github.com/sympy/sympy/issues?q=is%3Aopen+is%3Aissue+label%3A%22Easy+to+Fix%22>`_.\n\nPlease note that all participants in this project are expected to follow our\nCode of Conduct. By participating in this project you agree to abide by its\nterms. See `CODE_OF_CONDUCT.md <CODE_OF_CONDUCT.md>`_.\n\nTests\n-----\n\nTo execute all tests, run::\n\n    $./setup.py test\n\nin the current directory.\n\nFor the more fine-grained running of tests or doctests, use ``bin/test`` or\nrespectively ``bin/doctest``. The master branch is automatically tested by\nTravis CI.\n\nTo test pull requests, use `sympy-bot <https://github.com/sympy/sympy-bot>`_.\n\nRegenerate Experimental `\\LaTeX` Parser/Lexer\n---------------------------------------------\n\nThe parser and lexer generated with the `ANTLR4 <http://antlr4.org>`_ toolchain\nin `sympy/parsing/latex/_antlr` and checked into the repo. Presently, most\nusers should not need to regenerate these files, but if you plan to work on\nthis feature, you will need the `antlr4` command-line tool available. One way\nto get it is::\n\n    $ conda install -c conda-forge antlr=4.7\n\nAfter making changes to `sympy/parsing/latex/LaTeX.g4`, run::\n\n    $ ./setup.py antlr\n\nClean\n-----\n\nTo clean everything (thus getting the same tree as in the repository)::\n\n    $ ./setup.py clean\n\nYou can also clean things with git using::\n\n    $ git clean -Xdf\n\nwhich will clear everything ignored by ``.gitignore``, and::\n\n    $ git clean -df\n\nto clear all untracked files.  You can revert the most recent changes in git\nwith::\n\n    $ git reset --hard\n\nWARNING: The above commands will all clear changes you may have made, and you\nwill lose them forever. Be sure to check things with ``git status``, ``git\ndiff``, ``git clean -Xn`` and ``git clean -n`` before doing any of those.\n\nBugs\n----\n\nOur issue tracker is at https://github.com/sympy/sympy/issues.  Please report\nany bugs that you find.  Or, even better, fork the repository on GitHub and\ncreate a pull request.  We welcome all changes, big or small, and we will help\nyou make the pull request if you are new to git (just ask on our mailing list\nor Gitter).\n\nBrief History\n-------------\n\nSymPy was started by Ond\u0159ej \u010cert\u00edk in 2005, he wrote some code during the\nsummer, then he wrote some more code during summer 2006. In February 2007,\nFabian Pedregosa joined the project and helped fixed many things, contributed\ndocumentation and made it alive again. 5 students (Mateusz Paprocki, Brian\nJorgensen, Jason Gedge, Robert Schwarz, and Chris Wu) improved SymPy incredibly\nduring summer 2007 as part of the Google Summer of Code. Pearu Peterson\njoined the development during the summer 2007 and he has made SymPy much more\ncompetitive by rewriting the core from scratch, that has made it from 10x to\n100x faster. Jurjen N.E. Bos has contributed pretty-printing and other patches.\nFredrik Johansson has written mpmath and contributed a lot of patches.\n\nSymPy has participated in every Google Summer of Code since 2007. You can see\nhttps://github.com/sympy/sympy/wiki#google-summer-of-code for full details.\nEach year has improved SymPy by bounds. Most of SymPy's development has come\nfrom Google Summer of Code students.\n\nIn 2011, Ond\u0159ej \u010cert\u00edk stepped down as lead developer, with Aaron Meurer, who\nalso started as a Google Summer of Code student, taking his place. Ond\u0159ej\n\u010cert\u00edk is still active in the community but is too busy with work and family\nto play a lead development role.\n\nSince then, a lot more people have joined the development and some people have\nalso left. You can see the full list in doc/src/aboutus.rst, or online at:\n\nhttps://docs.sympy.org/dev/aboutus.html#sympy-development-team\n\nThe git history goes back to 2007 when development moved from svn to hg.  To\nsee the history before that point, look at https://github.com/sympy/sympy-old.\n\nYou can use git to see the biggest developers.  The command::\n\n     $ git shortlog -ns\n\nwill show each developer, sorted by commits to the project.  The command::\n\n     $ git shortlog -ns --since=\"1 year\"\n\nwill show the top developers from the last year.\n\nCitation\n--------\n\nTo cite SymPy in publications use\n\n    Meurer A, Smith CP, Paprocki M, \u010cert\u00edk O, Kirpichev SB, Rocklin M, Kumar A,\n    Ivanov S, Moore JK, Singh S, Rathnayake T, Vig S, Granger BE, Muller RP,\n    Bonazzi F, Gupta H, Vats S, Johansson F, Pedregosa F, Curry MJ, Terrel AR,\n    Rou\u010dka \u0160, Saboo A, Fernando I, Kulal S, Cimrman R, Scopatz A. (2017) SymPy:\n    symbolic computing in Python. *PeerJ Computer Science* 3:e103\n    https://doi.org/10.7717/peerj-cs.103\n\nA BibTeX entry for LaTeX users is\n\n.. code-block:: bibtex\n\n    @article{10.7717/peerj-cs.103,\n     title = {SymPy: symbolic computing in Python},\n     author = {Meurer, Aaron and Smith, Christopher P. and Paprocki, Mateusz and \\v{C}ert\\'{i}k, Ond\\v{r}ej and Kirpichev, Sergey B. and Rocklin, Matthew and Kumar, Amit and Ivanov, Sergiu and Moore, Jason K. and Singh, Sartaj and Rathnayake, Thilina and Vig, Sean and Granger, Brian E. and Muller, Richard P. and Bonazzi, Francesco and Gupta, Harsh and Vats, Shivam and Johansson, Fredrik and Pedregosa, Fabian and Curry, Matthew J. and Terrel, Andy R. and Rou\\v{c}ka, \\v{S}t\\v{e}p\\'{a}n and Saboo, Ashutosh and Fernando, Isuru and Kulal, Sumith and Cimrman, Robert and Scopatz, Anthony},\n     year = 2017,\n     month = Jan,\n     keywords = {Python, Computer algebra system, Symbolics},\n     abstract = {\n                SymPy is an open-source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become a popular symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select submodules. The supplementary material provides additional examples and further outlines details of the architecture and features of SymPy.\n             },\n     volume = 3,\n     pages = {e103},\n     journal = {PeerJ Computer Science},\n     issn = {2376-5992},\n     url = {https://doi.org/10.7717/peerj-cs.103},\n     doi = {10.7717/peerj-cs.103}\n    }\n\nSymPy is BSD licensed, so you are free to use it whatever you like, be it\nacademic, commercial, creating forks or derivatives, as long as you copy the\nBSD statement if you redistribute it (see the LICENSE file for details).  That\nsaid, although not required by the SymPy license, if it is convenient for you,\nplease cite SymPy when using it in your work and also consider contributing\nall your changes back, so that we can incorporate it and all of us will\nbenefit in the end.\n"}, "tensorflow": {"file_name": "tensorflow/tensorflow/README.md", "raw_text": "<div align=\"center\">\n  <img src=\"https://www.tensorflow.org/images/tf_logo_social.png\">\n</div>\n\n[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic)](https://badge.fury.io/py/tensorflow)\n[![PyPI](https://badge.fury.io/py/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n\n\n**`Documentation`** |\n------------------- |\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/) |\n\n[TensorFlow](https://www.tensorflow.org/) is an end-to-end open source platform\nfor machine learning. It has a comprehensive, flexible ecosystem of\n[tools](https://www.tensorflow.org/resources/tools),\n[libraries](https://www.tensorflow.org/resources/libraries-extensions), and\n[community](https://www.tensorflow.org/community) resources that lets\nresearchers push the state-of-the-art in ML and developers easily build and\ndeploy ML-powered applications.\n\nTensorFlow was originally developed by researchers and engineers working on the\nGoogle Brain team within Google's Machine Intelligence Research organization to\nconduct machine learning and deep neural networks research. The system is\ngeneral enough to be applicable in a wide variety of other domains, as well.\n\nTensorFlow provides stable [Python](https://www.tensorflow.org/api_docs/python)\nand [C++](https://www.tensorflow.org/api_docs/cc) APIs, as well as\nnon-guaranteed backward compatible API for\n[other languages](https://www.tensorflow.org/api_docs).\n\nKeep up-to-date with release announcements and security updates by subscribing\nto\n[announce@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce).\nSee all the [mailing lists](https://www.tensorflow.org/community/forums).\n\n## Install\n\nSee the [TensorFlow install guide](https://www.tensorflow.org/install) for the\n[pip package](https://www.tensorflow.org/install/pip), to\n[enable GPU support](https://www.tensorflow.org/install/gpu), use a\n[Docker container](https://www.tensorflow.org/install/docker), and\n[build from source](https://www.tensorflow.org/install/source).\n\nTo install the current release, which includes support for\n[CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and\nWindows)*:\n\n```\n$ pip install tensorflow\n```\n\nA smaller CPU-only package is also available:\n\n```\n$ pip install tensorflow-cpu\n```\n\nTo update TensorFlow to the latest version, add `--upgrade` flag to the above\ncommands.\n\n*Nightly binaries are available for testing using the\n[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and\n[tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPi.*\n\n#### *Try your first TensorFlow program*\n\n```shell\n$ python\n```\n\n```python\n>>> import tensorflow as tf\n>>> tf.add(1, 2).numpy()\n3\n>>> hello = tf.constant('Hello, TensorFlow!')\n>>> hello.numpy()\nb'Hello, TensorFlow!'\n```\n\nFor more examples, see the\n[TensorFlow tutorials](https://www.tensorflow.org/tutorials/).\n\n## Contribution guidelines\n\n**If you want to contribute to TensorFlow, be sure to review the\n[contribution guidelines](CONTRIBUTING.md). This project adheres to TensorFlow's\n[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.**\n\n**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, please see\n[TensorFlow Discuss](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss)\nfor general questions and discussion, and please direct specific questions to\n[Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**\n\nThe TensorFlow project strives to abide by generally accepted best practices in\nopen-source software development:\n\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)\n\n## Continuous build status\n\n### Official Builds\n\nBuild Type               | Status                                                                                                                                                                                                                                                                                                                                        | Artifacts\n------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------\n**Linux CPU**            | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html)                                                                                                                                                                        | [PyPI](https://pypi.org/project/tf-nightly/)\n**Linux GPU**            | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html)                                                                                                                                                              | [PyPI](https://pypi.org/project/tf-nightly-gpu/)\n**Linux XLA**            | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html)                                                                                                                                                                      | TBA\n**macOS**                | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html)                                                                                                                                                                  | [PyPI](https://pypi.org/project/tf-nightly/)\n**Windows CPU**          | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html)                                                                                                                                                                    | [PyPI](https://pypi.org/project/tf-nightly/)\n**Windows GPU**          | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html)                                                                                                                                                                    | [PyPI](https://pypi.org/project/tf-nightly-gpu/)\n**Android**              | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html)                                                                                                                                                                            | [![Download](https://api.bintray.com/packages/google/tensorflow/tensorflow/images/download.svg)](https://bintray.com/google/tensorflow/tensorflow/_latestVersion)\n**Raspberry Pi 0 and 1** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py2.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py2.html) [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html) | [Py2](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp27-none-linux_armv6l.whl) [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl)\n**Raspberry Pi 2 and 3** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py2.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py2.html) [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html) | [Py2](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp27-none-linux_armv7l.whl) [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl)\n\n### Community Supported Builds\n\nBuild Type                                                        | Status                                                                                                                                                                                        | Artifacts\n----------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------\n**Linux AMD ROCm GPU** Nightly                                    | [![Build Status](http://ml-ci.amd.com:21096/job/tensorflow-rocm-nightly/badge/icon)](http://ml-ci.amd.com:21096/job/tensorflow-rocm-nightly)                                                  | [Nightly](http://ml-ci.amd.com:21096/job/tensorflow-rocm-nightly/lastSuccessfulBuild/)\n**Linux AMD ROCm GPU** Stable Release                             | [![Build Status](http://ml-ci.amd.com:21096/job/tensorflow-rocm-release/badge/icon)](http://ml-ci.amd.com:21096/job/tensorflow-rocm-release/)                                                 | Release [1.15](http://ml-ci.amd.com:21096/job/tensorflow-rocm-release/lastSuccessfulBuild/) / [2.x](http://ml-ci.amd.com:21096/job/tensorflow-rocm-v2-release/lastSuccessfulBuild/)\n**Linux s390x** Nightly                                           | [![Build Status](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/badge/icon)](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/)                                                             | [Nightly](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/)\n**Linux s390x CPU** Stable Release                                | [![Build Status](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/badge/icon)](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/)                                      | [Release](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/)\n**Linux ppc64le CPU** Nightly                                     | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/)                                       | [Nightly](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Nightly_Artifact/)\n**Linux ppc64le CPU** Stable Release                              | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/)                       | Release [1.15](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/) / [2.x](https://powerci.osuosl.org/job/TensorFlow2_PPC64LE_CPU_Release_Build/)\n**Linux ppc64le GPU** Nightly                                     | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build/)                                       | [Nightly](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Nightly_Artifact/)\n**Linux ppc64le GPU** Stable Release                              | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/)                       | Release [1.15](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/) / [2.x](https://powerci.osuosl.org/job/TensorFlow2_PPC64LE_GPU_Release_Build/)\n**Linux CPU with Intel\u00ae MKL-DNN** Nightly                         | [![Build Status](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/badge/icon)](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/)                     | [Nightly](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/)\n**Linux CPU with Intel\u00ae MKL-DNN** Stable Release                  | ![Build Status](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-release-whl/badge/icon)                                                                                              | Release [1.15](https://pypi.org/project/intel-tensorflow/1.15.0/) / [2.x](https://pypi.org/project/intel-tensorflow/)\n**Red Hat\u00ae Enterprise Linux\u00ae 7.6 CPU & GPU** <br> Python 2.7, 3.6 | [![Build Status](https://jenkins-tensorflow.apps.ci.centos.org/buildStatus/icon?job=tensorflow-rhel7-3.6&build=2)](https://jenkins-tensorflow.apps.ci.centos.org/job/tensorflow-rhel7-3.6/2/) | [1.13.1 PyPI](https://tensorflow.pypi.thoth-station.ninja/index/)\n\n## Resources\n\n*   [TensorFlow.org](https://www.tensorflow.org)\n*   [TensorFlow Tutorials](https://www.tensorflow.org/tutorials/)\n*   [TensorFlow Official Models](https://github.com/tensorflow/models/tree/master/official)\n*   [TensorFlow Examples](https://github.com/tensorflow/examples)\n*   [TensorFlow in Practice from Coursera](https://www.coursera.org/specializations/tensorflow-in-practice)\n*   [TensorFlow: Data and Deployment from Coursera](https://www.coursera.org/specializations/tensorflow-data-and-deployment)\n*   [Getting Started with TensorFlow 2 from Coursera](https://www.coursera.org/learn/getting-started-with-tensor-flow2)\n*   [Intro to TensorFlow for Deep Learning from Udacity](https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187)\n*   [Introduction to TensorFlow Lite from Udacity](https://www.udacity.com/course/intro-to-tensorflow-lite--ud190)\n*   [TensorFlow Blog](https://blog.tensorflow.org)\n*   [Learn ML with TensorFlow](https://www.tensorflow.org/resources/learn-ml)\n*   [TensorFlow Twitter](https://twitter.com/tensorflow)\n*   [TensorFlow YouTube](https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ)\n*   [TensorFlow Roadmap](https://www.tensorflow.org/community/roadmap)\n*   [TensorFlow White Papers](https://www.tensorflow.org/about/bib)\n*   [TensorBoard Visualization Toolkit](https://github.com/tensorflow/tensorboard)\n\nLearn more about the\n[TensorFlow community](https://www.tensorflow.org/community) and how to\n[contribute](https://www.tensorflow.org/community/contribute).\n\n## License\n\n[Apache License 2.0](LICENSE)\n"}, "textblob": {"file_name": "sloria/TextBlob/README.rst", "raw_text": "\nTextBlob: Simplified Text Processing\n====================================\n\n.. image:: https://badgen.net/pypi/v/TextBlob\n    :target: https://pypi.org/project/textblob/\n    :alt: Latest version\n\n.. image:: https://badgen.net/travis/sloria/TextBlob/dev\n    :target: https://travis-ci.org/sloria/TextBlob\n    :alt: Travis-CI\n\nHomepage: `https://textblob.readthedocs.io/ <https://textblob.readthedocs.io/>`_\n\n`TextBlob` is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n\n\n.. code-block:: python\n\n    from textblob import TextBlob\n\n    text = '''\n    The titular threat of The Blob has always struck me as the ultimate movie\n    monster: an insatiably hungry, amoeba-like mass able to penetrate\n    virtually any safeguard, capable of--as a doomed doctor chillingly\n    describes it--\"assimilating flesh on contact.\n    Snide comparisons to gelatin be damned, it's a concept with the most\n    devastating of potential consequences, not unlike the grey goo scenario\n    proposed by technological theorists fearful of\n    artificial intelligence run rampant.\n    '''\n\n    blob = TextBlob(text)\n    blob.tags           # [('The', 'DT'), ('titular', 'JJ'),\n                        #  ('threat', 'NN'), ('of', 'IN'), ...]\n\n    blob.noun_phrases   # WordList(['titular threat', 'blob',\n                        #            'ultimate movie monster',\n                        #            'amoeba-like mass', ...])\n\n    for sentence in blob.sentences:\n        print(sentence.sentiment.polarity)\n    # 0.060\n    # -0.341\n\n    blob.translate(to=\"es\")  # 'La amenaza titular de The Blob...'\n\nTextBlob stands on the giant shoulders of `NLTK`_ and `pattern`_, and plays nicely with both.\n\nFeatures\n--------\n\n- Noun phrase extraction\n- Part-of-speech tagging\n- Sentiment analysis\n- Classification (Naive Bayes, Decision Tree)\n- Language translation and detection powered by Google Translate\n- Tokenization (splitting text into words and sentences)\n- Word and phrase frequencies\n- Parsing\n- `n`-grams\n- Word inflection (pluralization and singularization) and lemmatization\n- Spelling correction\n- Add new models or languages through extensions\n- WordNet integration\n\nGet it now\n----------\n::\n\n    $ pip install -U textblob\n    $ python -m textblob.download_corpora\n\nExamples\n--------\n\nSee more examples at the `Quickstart guide`_.\n\n.. _`Quickstart guide`: https://textblob.readthedocs.io/en/latest/quickstart.html#quickstart\n\n\nDocumentation\n-------------\n\nFull documentation is available at https://textblob.readthedocs.io/.\n\nRequirements\n------------\n\n- Python >= 2.7 or >= 3.4\n\nProject Links\n-------------\n\n- Docs: https://textblob.readthedocs.io/\n- Changelog: https://textblob.readthedocs.io/en/latest/changelog.html\n- PyPI: https://pypi.python.org/pypi/TextBlob\n- Issues: https://github.com/sloria/TextBlob/issues\n\nLicense\n-------\n\nMIT licensed. See the bundled `LICENSE <https://github.com/sloria/TextBlob/blob/master/LICENSE>`_ file for more details.\n\n.. _pattern: http://www.clips.ua.ac.be/pattern\n.. _NLTK: http://nltk.org/\n"}, "tokenizers": {"file_name": "huggingface/tokenizers/README.md", "raw_text": "<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png\" width=\"600\"/>\n    <br>\n<p>\n<p align=\"center\">\n    <img alt=\"Build\" src=\"https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg\">\n    <a href=\"https://github.com/huggingface/tokenizers/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue&cachedrop\">\n    </a>\n    </a>\n    <a href=\"https://pepy.tech/project/tokenizers/week\">\n        <img src=\"https://pepy.tech/badge/tokenizers/week\" />\n    </a>\n</p>\n\nProvides an implementation of today's most used tokenizers, with a focus on performance and\nversatility.\n\n## Main features:\n\n - Train new vocabularies and tokenize, using today's most used tokenizers.\n - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes\n   less than 20 seconds to tokenize a GB of text on a server's CPU.\n - Easy to use, but also extremely versatile.\n - Designed for research and production.\n - Normalization comes with alignments tracking. It's always possible to get the part of the\n   original sentence that corresponds to a given token.\n - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs.\n\n## Bindings\n\nWe provide bindings to the following languages (more to come!):\n  - [Rust](https://github.com/huggingface/tokenizers/tree/master/tokenizers) (Original implementation)\n  - [Python](https://github.com/huggingface/tokenizers/tree/master/bindings/python)\n  - [Node.js](https://github.com/huggingface/tokenizers/tree/master/bindings/node)\n \n## Quick examples using Python:\n\nStart using in a matter of seconds:\n\n```python\n# Tokenizers provides ultra-fast implementations of most current tokenizers:\n>>> from tokenizers import (ByteLevelBPETokenizer,\n                            CharBPETokenizer,\n                            SentencePieceBPETokenizer,\n                            BertWordPieceTokenizer)\n# Ultra-fast => they can encode 1GB of text in ~20sec on a standard server's CPU\n# Tokenizers can be easily instantiated from standard files\n>>> tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\nTokenizer(vocabulary_size=30522, model=BertWordPiece, add_special_tokens=True, unk_token=[UNK], \n          sep_token=[SEP], cls_token=[CLS], clean_text=True, handle_chinese_chars=True, \n          strip_accents=True, lowercase=True, wordpieces_prefix=##)\n\n# Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.\n# They also handle model's max input lengths as well as padding (to directly encode in padded batches)\n>>> output = tokenizer.encode(\"Hello, y'all! How are you \ud83d\ude01 ?\")\nEncoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing, original_str, normalized_str])\n>>> print(output.ids, output.tokens, output.offsets)\n[101, 7592, 1010, 1061, 1005, 2035, 999, 2129, 2024, 2017, 100, 1029, 102]\n['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n[(0, 0), (0, 5), (5, 6), (7, 8), (8, 9), (9, 12), (12, 13), (14, 17), (18, 21), (22, 25), (26, 27),\n (28, 29), (0, 0)]\n# Here is an example using the offsets mapping to retrieve the string corresponding to the 10th token:\n>>> output.original_str[output.offsets[10]]\n'\ud83d\ude01'\n```\n\nAnd training a new vocabulary is just as easy:\n\n```python\n# You can also train a BPE/Byte-levelBPE/WordPiece vocabulary on your own files\n>>> tokenizer = ByteLevelBPETokenizer()\n>>> tokenizer.train([\"wiki.test.raw\"], vocab_size=20000)\n[00:00:00] Tokenize words                 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   20993/20993\n[00:00:00] Count pairs                    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   20993/20993\n[00:00:03] Compute merges                 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   19375/19375\n```\n \n## Contributors\n  \n[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/0)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/0)[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/1)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/1)[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/2)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/2)[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/3)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/3)[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/4)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/4)[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/5)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/5)[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/6)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/6)[![](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/images/7)](https://sourcerer.io/fame/clmnt/huggingface/tokenizers/links/7)\n\n"}, "transformers": {"file_name": "huggingface/transformers/README.md", "raw_text": "<p align=\"center\">\n    <br>\n    <img src=\"https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png\" width=\"400\"/>\n    <br>\n<p>\n<p align=\"center\">\n    <a href=\"https://circleci.com/gh/huggingface/transformers\">\n        <img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/master\">\n    </a>\n    <a href=\"https://github.com/huggingface/transformers/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\">\n    </a>\n    <a href=\"https://huggingface.co/transformers/index.html\">\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/transformers/index.html.svg?down_color=red&down_message=offline&up_message=online\">\n    </a>\n    <a href=\"https://github.com/huggingface/transformers/releases\">\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\">\n    </a>\n</p>\n\n<h3 align=\"center\">\n<p>State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch\n</h3>\n\n\ud83e\udd17 Transformers (formerly known as `pytorch-transformers` and `pytorch-pretrained-bert`) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.\n\n[![](https://sourcerer.io/fame/clmnt/huggingface/transformers/images/0)](https://sourcerer.io/fame/clmnt/huggingface/transformers/links/0)[![](https://sourcerer.io/fame/clmnt/huggingface/transformers/images/1)](https://sourcerer.io/fame/clmnt/huggingface/transformers/links/1)[![](https://sourcerer.io/fame/clmnt/huggingface/transformers/images/2)](https://sourcerer.io/fame/clmnt/huggingface/transformers/links/2)[![](https://sourcerer.io/fame/clmnt/huggingface/transformers/images/3)](https://sourcerer.io/fame/clmnt/huggingface/transformers/links/3)[![](https://sourcerer.io/fame/clmnt/huggingface/transformers/images/4)](https://sourcerer.io/fame/clmnt/huggingface/transformers/links/4)[![](https://sourcerer.io/fame/clmnt/huggingface/transformers/images/5)](https://sourcerer.io/fame/clmnt/huggingface/transformers/links/5)[![](https://sourcerer.io/fame/clmnt/huggingface/transformers/images/6)](https://sourcerer.io/fame/clmnt/huggingface/transformers/links/6)[![](https://sourcerer.io/fame/clmnt/huggingface/transformers/images/7)](https://sourcerer.io/fame/clmnt/huggingface/transformers/links/7)\n\n### Features\n\n- As easy to use as pytorch-transformers\n- As powerful and concise as Keras\n- High performance on NLU and NLG tasks\n- Low barrier to entry for educators and practitioners\n\nState-of-the-art NLP for everyone\n- Deep learning researchers\n- Hands-on practitioners\n- AI/ML/NLP teachers and educators\n\nLower compute costs, smaller carbon footprint\n- Researchers can share trained models instead of always retraining\n- Practitioners can reduce compute time and production costs\n- 10 architectures with over 30 pretrained models, some in more than 100 languages\n\nChoose the right framework for every part of a model's lifetime\n- Train state-of-the-art models in 3 lines of code\n- Deep interoperability between TensorFlow 2.0 and PyTorch models\n- Move a single model between TF2.0/PyTorch frameworks at will\n- Seamlessly pick the right framework for training, evaluation, production\n\n\n| Section | Description |\n|-|-|\n| [Installation](#installation) | How to install the package |\n| [Model architectures](#model-architectures) | Architectures (with pretrained weights) |\n| [Online demo](#online-demo) | Experimenting with this repo\u2019s text generation capabilities |\n| [Quick tour: Usage](#quick-tour) | Tokenizers & models usage: Bert and GPT-2 |\n| [Quick tour: TF 2.0 and PyTorch ](#Quick-tour-TF-20-training-and-PyTorch-interoperability) | Train a TF 2.0 model in 10 lines of code, load it in PyTorch |\n| [Quick tour: pipelines](#quick-tour-of-pipelines) | Using Pipelines: Wrapper around tokenizer and models to use finetuned models |\n| [Quick tour: Fine-tuning/usage scripts](#quick-tour-of-the-fine-tuningusage-scripts) | Using provided scripts: GLUE, SQuAD and Text generation |\n| [Quick tour: Share your models ](#Quick-tour-of-model-sharing) | Upload and share your fine-tuned models with the community |\n| [Migrating from pytorch-transformers to transformers](#Migrating-from-pytorch-transformers-to-transformers) | Migrating your code from pytorch-transformers to transformers |\n| [Migrating from pytorch-pretrained-bert to pytorch-transformers](#Migrating-from-pytorch-pretrained-bert-to-transformers) | Migrating your code from pytorch-pretrained-bert to transformers |\n| [Documentation][(v2.5.0)](https://huggingface.co/transformers/v2.5.0)[(v2.4.0/v2.4.1)](https://huggingface.co/transformers/v2.4.0)[(v2.3.0)](https://huggingface.co/transformers/v2.3.0)[(v2.2.0/v2.2.1/v2.2.2)](https://huggingface.co/transformers/v2.2.0) [(v2.1.1)](https://huggingface.co/transformers/v2.1.1) [(v2.0.0)](https://huggingface.co/transformers/v2.0.0) [(v1.2.0)](https://huggingface.co/transformers/v1.2.0) [(v1.1.0)](https://huggingface.co/transformers/v1.1.0) [(v1.0.0)](https://huggingface.co/transformers/v1.0.0) [(master)](https://huggingface.co/transformers) | Full API documentation and more |\n\n## Installation\n\nThis repo is tested on Python 3.6+, PyTorch 1.0.0+ and TensorFlow 2.0.0-rc1\n\nYou should install \ud83e\udd17 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n\nCreate a virtual environment with the version of Python you're going to use and activate it.\n\nNow, if you want to use \ud83e\udd17 Transformers, you can install it with pip. If you'd like to play with the examples, you must install it from source.\n\n### With pip\n\nFirst you need to install one of, or both, TensorFlow 2.0 and PyTorch.\nPlease refer to [TensorFlow installation page](https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available) and/or [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) regarding the specific install command for your platform.\n\nWhen TensorFlow 2.0 and/or PyTorch has been installed, \ud83e\udd17 Transformers can be installed using pip as follows:\n\n```bash\npip install transformers\n```\n\n### From source\n\nHere also, you first need to install one of, or both, TensorFlow 2.0 and PyTorch.\nPlease refer to [TensorFlow installation page](https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available) and/or [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) regarding the specific install command for your platform.\n\nWhen TensorFlow 2.0 and/or PyTorch has been installed, you can install from source by cloning the repository and running:\n\n```bash\ngit clone https://github.com/huggingface/transformers\ncd transformers\npip install .\n```\n\nWhen you update the repository, you should upgrade the transformers installation and its dependencies as follows:\n\n```bash\ngit pull\npip install --upgrade .\n```\n\n### Run the examples\n\nExamples are included in the repository but are not shipped with the library.\n\nTherefore, in order to run the latest versions of the examples, you need to install from source, as described above.\n\nLook at the [README](https://github.com/huggingface/transformers/blob/master/examples/README.md) for how to run examples.\n\n### Tests\n\nA series of tests are included for the library and for some example scripts. Library tests can be found in the [tests folder](https://github.com/huggingface/transformers/tree/master/tests) and examples tests in the [examples folder](https://github.com/huggingface/transformers/tree/master/examples).\n\nDepending on which framework is installed (TensorFlow 2.0 and/or PyTorch), the irrelevant tests will be skipped. Ensure that both frameworks are installed if you want to execute all tests.\n\nHere's the easiest way to run tests for the library:\n\n```bash\npip install -e \".[testing]\"\nmake test\n```\n\nand for the examples:\n\n```bash\npip install -e \".[testing]\"\npip install -r examples/requirements.txt\nmake test-examples\n```\n\nFor details, refer to the [contributing guide](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md#tests).\n\n### Do you want to run a Transformer model on a mobile device?\n\nYou should check out our [`swift-coreml-transformers`](https://github.com/huggingface/swift-coreml-transformers) repo.\n\nIt contains a set of tools to convert PyTorch or TensorFlow 2.0 trained Transformer models (currently contains `GPT-2`, `DistilGPT-2`, `BERT`, and `DistilBERT`) to CoreML models that run on iOS devices.\n\nAt some point in the future, you'll be able to seamlessly move from pre-training or fine-tuning models to productizing them in CoreML, or prototype a model or an app in CoreML then research its hyperparameters or architecture from TensorFlow 2.0 and/or PyTorch. Super exciting!\n\n## Model architectures\n\n\ud83e\udd17 Transformers currently provides the following NLU/NLG architectures:\n\n1. **[BERT](https://huggingface.co/transformers/model_doc/bert.html)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\n2. **[GPT](https://huggingface.co/transformers/model_doc/gpt.html)** (from OpenAI) released with the paper [Improving Language Understanding by Generative Pre-Training](https://blog.openai.com/language-unsupervised/) by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.\n3. **[GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)** (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/) by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.\n4. **[Transformer-XL](https://huggingface.co/transformers/model_doc/transformerxl.html)** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n5. **[XLNet](https://huggingface.co/transformers/model_doc/xlnet.html)** (from Google/CMU) released with the paper [\u200bXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n6. **[XLM](https://huggingface.co/transformers/model_doc/xlm.html)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau.\n7. **[RoBERTa](https://huggingface.co/transformers/model_doc/roberta.html)** (from Facebook), released together with the paper a [Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n8. **[DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/master/examples/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/master/examples/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/master/examples/distillation) and a German version of DistilBERT.\n9. **[CTRL](https://huggingface.co/transformers/model_doc/ctrl.html)** (from Salesforce) released with the paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.\n10. **[CamemBERT](https://huggingface.co/transformers/model_doc/camembert.html)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n11. **[ALBERT](https://huggingface.co/transformers/model_doc/albert.html)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\n12. **[T5](https://huggingface.co/transformers/model_doc/t5.html)** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\n13. **[XLM-RoBERTa](https://huggingface.co/transformers/model_doc/xlmroberta.html)** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.\n14. **[MMBT](https://github.com/facebookresearch/mmbt/)** (from Facebook), released together with the paper a [Supervised Multimodal Bitransformers for Classifying Images and Text](https://arxiv.org/pdf/1909.02950.pdf) by Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, Davide Testuggine.\n15. **[FlauBERT](https://huggingface.co/transformers/model_doc/flaubert.html)** (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, Lo\u00efc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Beno\u00eet Crabb\u00e9, Laurent Besacier, Didier Schwab.\n16. **[BART](https://huggingface.co/transformers/model_doc/bart.html)** (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.\n17. **[ELECTRA](https://huggingface.co/transformers/model_doc/electra.html)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n18. **[DialoGPT](https://huggingface.co/transformers/model_doc/dialogpt.html)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n18. **[Other community models](https://huggingface.co/models)**, contributed by the [community](https://huggingface.co/users).\n19. Want to contribute a new model? We have added a **detailed guide and templates** to guide you in the process of adding a new model. You can find them in the [`templates`](./templates) folder of the repository. Be sure to check the [contributing guidelines](./CONTRIBUTING.md) and contact the maintainers or open an issue to collect feedbacks before starting your PR.\n\nThese implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 F1 on SQuAD for BERT Whole-Word-Masking, ~88 F1 on RocStories for OpenAI GPT, ~18.3 perplexity on WikiText 103 for Transformer-XL, ~0.916 Peason R coefficient on STS-B for XLNet). You can find more details on the performances in the Examples section of the [documentation](https://huggingface.co/transformers/examples.html).\n\n## Online demo\n\n**[Write With Transformer](https://transformer.huggingface.co)**, built by the Hugging Face team at transformer.huggingface.co, is the official demo of this repo\u2019s text generation capabilities.\nYou can use it to experiment with completions generated by `GPT2Model`, `TransfoXLModel`, and `XLNetModel`.\n\n> \u201c\ud83e\udd84 Write with transformer is to writing what calculators are to calculus.\u201d\n\n![write_with_transformer](https://transformer.huggingface.co/front/assets/thumbnail-large.png)\n\n## Quick tour\n\nLet's do a very quick overview of the model architectures in \ud83e\udd17 Transformers. Detailed examples for each model architecture (Bert, GPT, GPT-2, Transformer-XL, XLNet and XLM) can be found in the [full documentation](https://huggingface.co/transformers/).\n\n```python\nimport torch\nfrom transformers import *\n\n# Transformers has a unified API\n# for 10 transformer architectures and 30 pretrained weights.\n#          Model          | Tokenizer          | Pretrained weights shortcut\nMODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased'),\n          (OpenAIGPTModel,  OpenAIGPTTokenizer,  'openai-gpt'),\n          (GPT2Model,       GPT2Tokenizer,       'gpt2'),\n          (CTRLModel,       CTRLTokenizer,       'ctrl'),\n          (TransfoXLModel,  TransfoXLTokenizer,  'transfo-xl-wt103'),\n          (XLNetModel,      XLNetTokenizer,      'xlnet-base-cased'),\n          (XLMModel,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\n          (DistilBertModel, DistilBertTokenizer, 'distilbert-base-cased'),\n          (RobertaModel,    RobertaTokenizer,    'roberta-base'),\n          (XLMRobertaModel, XLMRobertaTokenizer, 'xlm-roberta-base'),\n         ]\n\n# To use TensorFlow 2.0 versions of the models, simply prefix the class names with 'TF', e.g. `TFRobertaModel` is the TF 2.0 counterpart of the PyTorch model `RobertaModel`\n\n# Let's encode some text in a sequence of hidden-states using each model:\nfor model_class, tokenizer_class, pretrained_weights in MODELS:\n    # Load pretrained model/tokenizer\n    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n    model = model_class.from_pretrained(pretrained_weights)\n\n    # Encode text\n    input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n    with torch.no_grad():\n        last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples\n\n# Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.\nBERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n                      BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]\n\n# All the classes for an architecture can be initiated from pretrained weights for this architecture\n# Note that additional weights added for fine-tuning are only initialized\n# and need to be trained on the down-stream task\npretrained_weights = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(pretrained_weights)\nfor model_class in BERT_MODEL_CLASSES:\n    # Load pretrained model/tokenizer\n    model = model_class.from_pretrained(pretrained_weights)\n\n    # Models can return full list of hidden-states & attentions weights at each layer\n    model = model_class.from_pretrained(pretrained_weights,\n                                        output_hidden_states=True,\n                                        output_attentions=True)\n    input_ids = torch.tensor([tokenizer.encode(\"Let's see all hidden-states and attentions on this text\")])\n    all_hidden_states, all_attentions = model(input_ids)[-2:]\n\n    # Models are compatible with Torchscript\n    model = model_class.from_pretrained(pretrained_weights, torchscript=True)\n    traced_model = torch.jit.trace(model, (input_ids,))\n\n    # Simple serialization for models and tokenizers\n    model.save_pretrained('./directory/to/save/')  # save\n    model = model_class.from_pretrained('./directory/to/save/')  # re-load\n    tokenizer.save_pretrained('./directory/to/save/')  # save\n    tokenizer = BertTokenizer.from_pretrained('./directory/to/save/')  # re-load\n\n    # SOTA examples for GLUE, SQUAD, text generation...\n```\n\n## Quick tour TF 2.0 training and PyTorch interoperability\n\nLet's do a quick example of how a TensorFlow 2.0 model can be trained in 12 lines of code with \ud83e\udd17 Transformers and then loaded in PyTorch for fast inspection/tests.\n\n```python\nimport tensorflow as tf\nimport tensorflow_datasets\nfrom transformers import *\n\n# Load dataset, tokenizer, model from pretrained model/vocabulary\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nmodel = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\ndata = tensorflow_datasets.load('glue/mrpc')\n\n# Prepare dataset for GLUE as a tf.data.Dataset instance\ntrain_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')\nvalid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='mrpc')\ntrain_dataset = train_dataset.shuffle(100).batch(32).repeat(2)\nvalid_dataset = valid_dataset.batch(64)\n\n# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n\n# Train and evaluate using tf.keras.Model.fit()\nhistory = model.fit(train_dataset, epochs=2, steps_per_epoch=115,\n                    validation_data=valid_dataset, validation_steps=7)\n\n# Load the TensorFlow model in PyTorch for inspection\nmodel.save_pretrained('./save/')\npytorch_model = BertForSequenceClassification.from_pretrained('./save/', from_tf=True)\n\n# Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task\nsentence_0 = \"This research was consistent with his findings.\"\nsentence_1 = \"His findings were compatible with this research.\"\nsentence_2 = \"His findings were not compatible with this research.\"\ninputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt')\ninputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt')\n\npred_1 = pytorch_model(inputs_1['input_ids'], token_type_ids=inputs_1['token_type_ids'])[0].argmax().item()\npred_2 = pytorch_model(inputs_2['input_ids'], token_type_ids=inputs_2['token_type_ids'])[0].argmax().item()\n\nprint(\"sentence_1 is\", \"a paraphrase\" if pred_1 else \"not a paraphrase\", \"of sentence_0\")\nprint(\"sentence_2 is\", \"a paraphrase\" if pred_2 else \"not a paraphrase\", \"of sentence_0\")\n```\n\n## Quick tour of the fine-tuning/usage scripts\n\n**Important**\nBefore running the fine-tuning scripts, please read the\n[instructions](#run-the-examples) on how to\nsetup your environment to run the examples.\n\nThe library comprises several example scripts with SOTA performances for NLU and NLG tasks:\n\n- `run_glue.py`: an example fine-tuning Bert, XLNet and XLM on nine different GLUE tasks (*sequence-level classification*)\n- `run_squad.py`: an example fine-tuning Bert, XLNet and XLM on the question answering dataset SQuAD 2.0 (*token-level classification*)\n- `run_generation.py`: an example using GPT, GPT-2, CTRL, Transformer-XL and XLNet for conditional language generation\n- other model-specific examples (see the documentation).\n\nHere are three quick usage examples for these scripts:\n\n### `run_glue.py`: Fine-tuning on GLUE tasks for sequence classification\n\nThe [General Language Understanding Evaluation (GLUE) benchmark](https://gluebenchmark.com/) is a collection of nine sentence- or sentence-pair language understanding tasks for evaluating and analyzing natural language understanding systems.\n\nBefore running anyone of these GLUE tasks you should download the\n[GLUE data](https://gluebenchmark.com/tasks) by running\n[this script](https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e)\nand unpack it to some directory `$GLUE_DIR`.\n\nYou should also install the additional packages required by the examples:\n\n```shell\npip install -r ./examples/requirements.txt\n```\n\n```shell\nexport GLUE_DIR=/path/to/glue\nexport TASK_NAME=MRPC\n\npython ./examples/run_glue.py \\\n    --model_type bert \\\n    --model_name_or_path bert-base-uncased \\\n    --task_name $TASK_NAME \\\n    --do_train \\\n    --do_eval \\\n    --data_dir $GLUE_DIR/$TASK_NAME \\\n    --max_seq_length 128 \\\n    --per_gpu_eval_batch_size=8   \\\n    --per_gpu_train_batch_size=8   \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 3.0 \\\n    --output_dir /tmp/$TASK_NAME/\n```\n\nwhere task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.\n\nThe dev set results will be present within the text file 'eval_results.txt' in the specified output_dir. In case of MNLI, since there are two separate dev sets, matched and mismatched, there will be a separate output folder called '/tmp/MNLI-MM/' in addition to '/tmp/MNLI/'.\n\n#### Fine-tuning XLNet model on the STS-B regression task\n\nThis example code fine-tunes XLNet on the STS-B corpus using parallel training on a server with 4 V100 GPUs.\nParallel training is a simple way to use several GPUs (but is slower and less flexible than distributed training, see below).\n\n```shell\nexport GLUE_DIR=/path/to/glue\n\npython ./examples/run_glue.py \\\n    --model_type xlnet \\\n    --model_name_or_path xlnet-large-cased \\\n    --do_train  \\\n    --do_eval   \\\n    --task_name=sts-b     \\\n    --data_dir=${GLUE_DIR}/STS-B  \\\n    --output_dir=./proc_data/sts-b-110   \\\n    --max_seq_length=128   \\\n    --per_gpu_eval_batch_size=8   \\\n    --per_gpu_train_batch_size=8   \\\n    --gradient_accumulation_steps=1 \\\n    --max_steps=1200  \\\n    --model_name=xlnet-large-cased   \\\n    --overwrite_output_dir   \\\n    --overwrite_cache \\\n    --warmup_steps=120\n```\n\nOn this machine we thus have a batch size of 32, please increase `gradient_accumulation_steps` to reach the same batch size if you have a smaller machine. These hyper-parameters should result in a Pearson correlation coefficient of `+0.917` on the development set.\n\n#### Fine-tuning Bert model on the MRPC classification task\n\nThis example code fine-tunes the Bert Whole Word Masking model on the Microsoft Research Paraphrase Corpus (MRPC) corpus using distributed training on 8 V100 GPUs to reach a F1 > 92.\n\n```bash\npython -m torch.distributed.launch --nproc_per_node 8 ./examples/run_glue.py   \\\n    --model_type bert \\\n    --model_name_or_path bert-large-uncased-whole-word-masking \\\n    --task_name MRPC \\\n    --do_train   \\\n    --do_eval   \\\n    --data_dir $GLUE_DIR/MRPC/   \\\n    --max_seq_length 128   \\\n    --per_gpu_eval_batch_size=8   \\\n    --per_gpu_train_batch_size=8   \\\n    --learning_rate 2e-5   \\\n    --num_train_epochs 3.0  \\\n    --output_dir /tmp/mrpc_output/ \\\n    --overwrite_output_dir   \\\n    --overwrite_cache \\\n```\n\nTraining with these hyper-parameters gave us the following results:\n\n```bash\n  acc = 0.8823529411764706\n  acc_and_f1 = 0.901702786377709\n  eval_loss = 0.3418912578906332\n  f1 = 0.9210526315789473\n  global_step = 174\n  loss = 0.07231863956341798\n```\n\n### `run_squad.py`: Fine-tuning on SQuAD for question-answering\n\nThis example code fine-tunes BERT on the SQuAD dataset using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 > 93 on SQuAD:\n\n```bash\npython -m torch.distributed.launch --nproc_per_node=8 ./examples/run_squad.py \\\n    --model_type bert \\\n    --model_name_or_path bert-large-uncased-whole-word-masking \\\n    --do_train \\\n    --do_eval \\\n    --train_file $SQUAD_DIR/train-v1.1.json \\\n    --predict_file $SQUAD_DIR/dev-v1.1.json \\\n    --learning_rate 3e-5 \\\n    --num_train_epochs 2 \\\n    --max_seq_length 384 \\\n    --doc_stride 128 \\\n    --output_dir ../models/wwm_uncased_finetuned_squad/ \\\n    --per_gpu_eval_batch_size=3   \\\n    --per_gpu_train_batch_size=3   \\\n```\n\nTraining with these hyper-parameters gave us the following results:\n\n```bash\npython $SQUAD_DIR/evaluate-v1.1.py $SQUAD_DIR/dev-v1.1.json ../models/wwm_uncased_finetuned_squad/predictions.json\n{\"exact_match\": 86.91579943235573, \"f1\": 93.1532499015869}\n```\n\nThis is the model provided as `bert-large-uncased-whole-word-masking-finetuned-squad`.\n\n### `run_generation.py`: Text generation with GPT, GPT-2, CTRL, Transformer-XL and XLNet\n\nA conditional generation script is also included to generate text from a prompt.\nThe generation script includes the [tricks](https://github.com/rusiaaman/XLNet-gen#methodology) proposed by Aman Rusia to get high-quality generation with memory models like Transformer-XL and XLNet (include a predefined text to make short inputs longer).\n\nHere is how to run the script with the small version of OpenAI GPT-2 model:\n\n```shell\npython ./examples/run_generation.py \\\n    --model_type=gpt2 \\\n    --length=20 \\\n    --model_name_or_path=gpt2 \\\n```\n\nand from the Salesforce CTRL model:\n```shell\npython ./examples/run_generation.py \\\n    --model_type=ctrl \\\n    --length=20 \\\n    --model_name_or_path=ctrl \\\n    --temperature=0 \\\n    --repetition_penalty=1.2 \\\n```\n\n## Quick tour of model sharing\n\nStarting with `v2.2.2`, you can now upload and share your fine-tuned models with the community, using the <abbr title=\"Command-line interface\">CLI</abbr> that's built-in to the library.\n\n**First, create an account on [https://huggingface.co/join](https://huggingface.co/join)**. Optionally, join an existing organization or create a new one. Then:\n\n```shell\ntransformers-cli login\n# log in using the same credentials as on huggingface.co\n```\nUpload your model:\n```shell\ntransformers-cli upload ./path/to/pretrained_model/\n\n# ^^ Upload folder containing weights/tokenizer/config\n# saved via `.save_pretrained()`\n\ntransformers-cli upload ./config.json [--filename folder/foobar.json]\n\n# ^^ Upload a single file\n# (you can optionally override its filename, which can be nested inside a folder)\n```\n\nIf you want your model to be namespaced by your organization name rather than your username, add the following flag to any command:\n```shell\n--organization organization_name\n```\n\nYour model will then be accessible through its identifier, a concatenation of your username (or organization name) and the folder name above:\n```python\n\"username/pretrained_model\"\n# or if an org:\n\"organization_name/pretrained_model\"\n```\n\n**Please add a README.md model card** to the repo under `model_cards/` with: model description, training params (dataset, preprocessing, hardware used, hyperparameters), evaluation results, intended uses & limitations, etc.\n\nYour model now has a page on huggingface.co/models \ud83d\udd25\n\nAnyone can load it from code:\n```python\ntokenizer = AutoTokenizer.from_pretrained(\"namespace/pretrained_model\")\nmodel = AutoModel.from_pretrained(\"namespace/pretrained_model\")\n```\n\nList all your files on S3:\n```shell\ntransformers-cli s3 ls\n```\n\nYou can also delete unneeded files:\n\n```shell\ntransformers-cli s3 rm \u2026\n```\n\n## Quick tour of pipelines\n\nNew in version `v2.3`: `Pipeline` are high-level objects which automatically handle tokenization, running your data through a transformers model\nand outputting the result in a structured object.\n\nYou can create `Pipeline` objects for the following down-stream tasks:\n\n - `feature-extraction`: Generates a tensor representation for the input sequence\n - `ner`: Generates named entity mapping for each word in the input sequence.\n - `sentiment-analysis`: Gives the polarity (positive / negative) of the whole input sequence.\n - `text-classification`: Initialize a `TextClassificationPipeline` directly, or see `sentiment-analysis` for an example.\n - `question-answering`: Provided some context and a question refering to the context, it will extract the answer to the question in the context.\n - `fill-mask`: Takes an input sequence containing a masked token (e.g. `<mask>`) and return list of most probable filled sequences, with their probabilities.\n - `summarization`\n - `translation_xx_to_yy`\n\n```python\nfrom transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\nnlp = pipeline('sentiment-analysis')\nnlp('We are very happy to include pipeline into the transformers repository.')\n>>> {'label': 'POSITIVE', 'score': 0.99893874}\n\n# Allocate a pipeline for question-answering\nnlp = pipeline('question-answering')\nnlp({\n    'question': 'What is the name of the repository ?',\n    'context': 'Pipeline have been included in the huggingface/transformers repository'\n})\n>>> {'score': 0.28756016668193496, 'start': 35, 'end': 59, 'answer': 'huggingface/transformers'}\n```\n\n## Migrating from pytorch-transformers to transformers\n\nHere is a quick summary of what you should take care of when migrating from `pytorch-transformers` to `transformers`.\n\n### Positional order of some models' keywords inputs (`attention_mask`, `token_type_ids`...) changed\n\nTo be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models **keywords inputs** (`attention_mask`, `token_type_ids`...) has been changed.\n\nIf you used to call the models with keyword names for keyword arguments, e.g. `model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)`, this should not cause any change.\n\nIf you used to call the models with positional inputs for keyword arguments, e.g. `model(inputs_ids, attention_mask, token_type_ids)`, you may have to double check the exact order of input arguments.\n\n\n## Migrating from pytorch-pretrained-bert to transformers\n\nHere is a quick summary of what you should take care of when migrating from `pytorch-pretrained-bert` to `transformers`.\n\n### Models always output `tuples`\n\nThe main breaking change when migrating from `pytorch-pretrained-bert` to `transformers` is that every model's forward method always outputs a `tuple` with various elements depending on the model and the configuration parameters.\n\nThe exact content of the tuples for each model is detailed in the models' docstrings and the [documentation](https://huggingface.co/transformers/).\n\nIn pretty much every case, you will be fine by taking the first element of the output as the output you previously used in `pytorch-pretrained-bert`.\n\nHere is a `pytorch-pretrained-bert` to `transformers` conversion example for a `BertForSequenceClassification` classification model:\n\n```python\n# Let's load our model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\n# If you used to have this line in pytorch-pretrained-bert:\nloss = model(input_ids, labels=labels)\n\n# Now just use this line in transformers to extract the loss from the output tuple:\noutputs = model(input_ids, labels=labels)\nloss = outputs[0]\n\n# In transformers you can also have access to the logits:\nloss, logits = outputs[:2]\n\n# And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', output_attentions=True)\noutputs = model(input_ids, labels=labels)\nloss, logits, attentions = outputs\n```\n\n### Using hidden states\n\nBy enabling the configuration option `output_hidden_states`, it was possible to retrieve the last hidden states of the encoder. In `pytorch-transformers` as well as `transformers` the return value has changed slightly: `all_hidden_states` now also includes the hidden state of the embeddings in addition to those of the encoding layers. This allows users to easily access the embeddings final state.\n\n### Serialization\n\nBreaking change in the `from_pretrained()` method:\n\n1. Models are now set in evaluation mode by default when instantiated with the `from_pretrained()` method. To train them, don't forget to set them back in training mode (`model.train()`) to activate the dropout modules.\n\n2. The additional `*input` and `**kwargs` arguments supplied to the `from_pretrained()` method used to be directly passed to the underlying model's class `__init__()` method. They are now used to update the model configuration attribute instead, which can break derived model classes built based on the previous `BertForSequenceClassification` examples. We are working on a way to mitigate this breaking change in [#866](https://github.com/huggingface/transformers/pull/866) by forwarding the the model's `__init__()` method (i) the provided positional arguments and (ii) the keyword arguments which do not match any configuration class attributes.\n\nAlso, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method `save_pretrained(save_directory)` if you were using any other serialization method before.\n\nHere is an example:\n\n```python\n### Let's load a model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n### Do some stuff to our model and tokenizer\n# Ex: add new tokens to the vocabulary and embeddings of our model\ntokenizer.add_tokens(['[SPECIAL_TOKEN_1]', '[SPECIAL_TOKEN_2]'])\nmodel.resize_token_embeddings(len(tokenizer))\n# Train our model\ntrain(model)\n\n### Now let's save our model and tokenizer to a directory\nmodel.save_pretrained('./my_saved_model_directory/')\ntokenizer.save_pretrained('./my_saved_model_directory/')\n\n### Reload the model and the tokenizer\nmodel = BertForSequenceClassification.from_pretrained('./my_saved_model_directory/')\ntokenizer = BertTokenizer.from_pretrained('./my_saved_model_directory/')\n```\n\n### Optimizers: BertAdam & OpenAIAdam are now AdamW, schedules are standard PyTorch schedules\n\nThe two optimizers previously included, `BertAdam` and `OpenAIAdam`, have been replaced by a single `AdamW` optimizer which has a few differences:\n\n- it only implements weights decay correction,\n- schedules are now externals (see below),\n- gradient clipping is now also external (see below).\n\nThe new optimizer `AdamW` matches PyTorch `Adam` optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping.\n\nThe schedules are now standard [PyTorch learning rate schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) and not part of the optimizer anymore.\n\nHere is a conversion examples from `BertAdam` with a linear warmup and decay schedule to `AdamW` and the same schedule:\n\n```python\n# Parameters:\nlr = 1e-3\nmax_grad_norm = 1.0\nnum_training_steps = 1000\nnum_warmup_steps = 100\nwarmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1\n\n### Previously BertAdam optimizer was instantiated like this:\noptimizer = BertAdam(model.parameters(), lr=lr, schedule='warmup_linear', warmup=warmup_proportion, t_total=num_training_steps)\n### and used like this:\nfor batch in train_data:\n    loss = model(batch)\n    loss.backward()\n    optimizer.step()\n\n### In Transformers, optimizer and schedules are splitted and instantiated like this:\noptimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n### and used like this:\nfor batch in train_data:\n    model.train()\n    loss = model(batch)\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n```\n\n## Citation\n\nWe now have a paper you can cite for the \ud83e\udd17 Transformers library:\n```bibtex\n@article{Wolf2019HuggingFacesTS,\n  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},\n  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.03771}\n}\n```\n"}, "Theano": {"file_name": "Theano/Theano/README.rst", "raw_text": "============================================================================================================\n`MILA will stop developing Theano <https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ>`_.\n============================================================================================================\n\n\nTo install the package, see this page:\n   http://deeplearning.net/software/theano/install.html\n\nFor the documentation, see the project website:\n   http://deeplearning.net/software/theano/\n\nRelated Projects:\n   https://github.com/Theano/Theano/wiki/Related-projects\n\nIt is recommended that you look at the documentation on the website, as it will be more current than the documentation included with the package.\n\nIn order to build the documentation yourself, you will need sphinx. Issue the following command:\n\n::\n\n   python ./doc/scripts/docgen.py\n\nDocumentation is built into ``html/``\n\nThe PDF of the documentation can be found at ``html/theano.pdf``\n\n================\nDIRECTORY LAYOUT\n================\n\n``Theano`` (current directory) is the distribution directory.\n\n* ``Theano/theano`` contains the package\n* ``Theano/theano`` has several submodules:\n \n  * ``gof`` + ``compile`` are the core\n  * ``scalar`` depends upon core\n  * ``tensor`` depends upon ``scalar``\n  * ``sparse`` depends upon ``tensor``\n  * ``sandbox`` can depend on everything else\n\n* ``Theano/examples`` are copies of the example found on the wiki\n* ``Theano/benchmark`` and ``Theano/examples`` are in the distribution, but not in\n  the Python package\n* ``Theano/bin`` contains executable scripts that are copied to the bin folder\n  when the Python package is installed\n* Tests are distributed and are part of the package, i.e. fall in\n  the appropriate submodules\n* ``Theano/doc`` contains files and scripts used to generate the documentation\n* ``Theano/html`` is where the documentation will be generated\n"}, "tdda": {"file_name": "tdda/tdda/README.md", "raw_text": "Test-Driven Data Analysis (Python TDDA library)\n===============================================\n\nWhat is it?\n-----------\n\nThe TDDA Python module provides command-line and Python API support for\nthe overall process of data analysis, through the following tools:\n\n - **Reference Testing**: extensions to `unittest` and `pytest` for\n   managing testing of data analysis pipelines, where the results are\n   typically much larger, and more complex, than single numerical\n   values.\n\n - **Constraints**: tools (and API) for discovery of constraints from data,\n   for validation of constraints on new data, and for anomaly detection.\n\n - **Finding Regular Expressions**: tools (and API) for automatically\n   inferring regular expressions from text data.\n\nDocumentation\n-------------\n\nhttp://tdda.readthedocs.io\n\nInstallation\n------------\n\nThe simplest way to install all of the TDDA Python modules is using *pip*:\n\n    pip install tdda\n\nThe full set of sources, including all examples, are downloadable from\nPyPi with:\n\n    pip download --no-binary :all: tdda\n\nThe sources are also publicly available from Github:\n\n    git clone git@github.com:tdda/tdda.git\n\nDocumentation is available at http://tdda.readthedocs.io.\n\nIf you clone the Github repo, use\n\n    python setup.py install\n\nafterwards to install the command-line tools (`tdda` and `rexpy`).\n\n\n*Reference Tests*\n-----------------\n\nThe `tdda.referencetest` library is used to support\nthe creation of *reference tests*, based on either unittest or pytest.\n\nThese are like other tests except:\n\n  1. They have special support for comparing strings to files\n     and files to files.\n  2. That support includes the ability to provide exclusion patterns\n     (for things like dates and versions that might be in the output).\n  3. When a string/file assertion fails, it spits out the command you\n     need to diff the output.\n  4. If there were exclusion patterns, it also writes modified versions\n     of both the actual and expected output and also prints the diff\n     command needed to compare those.\n  5. They have special support for handling CSV files.\n  6. It supports flags (-w and -W)  to rewrite the reference (expected)\n     results once you have confirmed that the new actuals are correct.\n\nFor more details from a source distribution or checkout, see the `README.md`\nfile and examples in the `referencetest` subdirectory.\n\n*Constraints*\n-------------\n\nThe `tdda.constraints` library is used to 'discover' constraints\nfrom a (Pandas) DataFrame, write them out as JSON, and to verify that\ndatasets meet the constraints in the constraints file.\n\nFor more details from a source distribution or checkout, see the `README.md`\nfile and examples in the `constraints` subdirectory.\n\n*Finding Regular Expressions*\n-----------------------------\n\nThe `tdda` repository also includes `rexpy`, a tool for automatically\ninferring regular expressions from a single field of data examples.\n\n*Resources*\n-----------\n\nResources on these topics include:\n\n  * TDDA Blog: http://www.tdda.info\n  * Quick Reference Guide (\"Cheatsheet\"): http://www.tdda.info/pdf/tdda-quickref.pdf\n  * Full documentation: http://tdda.readthedocs.io\n  * General Notes on Constraints and Assertions: http://www.tdda.info/constraints-and-assertions\n  * Notes on using the Pandas constraints library:\n    http://www.tdda.info/constraint-discovery-and-verification-for-pandas-dataframes\n  * PyCon UK Talk on TDDA:\n      - Video: https://www.youtube.com/watch?v=FIw_7aUuY50\n      - Slides and Rough Transcript:   http://www.tdda.info/slides-and-rough-transcript-of-tdda-talk-from-pycon-uk-2016\n\nAll examples, tests and code run under Python 2.7, Python 3.5 and Python 3.6.\n\n"}, "torch": {"file_name": "pytorch/pytorch/README.md", "raw_text": "![PyTorch Logo](https://github.com/pytorch/pytorch/blob/master/docs/source/_static/img/pytorch-logo-dark.png)\n\n--------------------------------------------------------------------------------\n\nPyTorch is a Python package that provides two high-level features:\n- Tensor computation (like NumPy) with strong GPU acceleration\n- Deep neural networks built on a tape-based autograd system\n\nYou can reuse your favorite Python packages such as NumPy, SciPy and Cython to extend PyTorch when needed.\n\n- [More about PyTorch](#more-about-pytorch)\n- [Installation](#installation)\n  - [Binaries](#binaries)\n  - [From Source](#from-source)\n  - [Docker Image](#docker-image)\n  - [Building the Documentation](#building-the-documentation)\n  - [Previous Versions](#previous-versions)\n- [Getting Started](#getting-started)\n- [Communication](#communication)\n- [Releases and Contributing](#releases-and-contributing)\n- [The Team](#the-team)\n\n| System | 3.6 | 3.7 | 3.8 |\n| :---: | :---: | :---: | :--: |\n| Linux CPU | [![Build Status](https://ci.pytorch.org/jenkins/job/pytorch-master/badge/icon)](https://ci.pytorch.org/jenkins/job/pytorch-master/) | [![Build Status](https://ci.pytorch.org/jenkins/job/pytorch-master/badge/icon)](https://ci.pytorch.org/jenkins/job/pytorch-master/) | <center>\u2014</center> |\n| Linux GPU | [![Build Status](https://ci.pytorch.org/jenkins/job/pytorch-master/badge/icon)](https://ci.pytorch.org/jenkins/job/pytorch-master/) | [![Build Status](https://ci.pytorch.org/jenkins/job/pytorch-master/badge/icon)](https://ci.pytorch.org/jenkins/job/pytorch-master/) | <center>\u2014</center> |\n| Windows CPU / GPU | <center>\u2014</center> | [![Build Status](https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-win-ws2016-cuda9-cudnn7-py3-trigger/badge/icon)](https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-win-ws2016-cuda9-cudnn7-py3-trigger/) |  <center>\u2014</center> |\n| Linux (ppc64le) CPU | <center>\u2014</center> | <center>\u2014</center> | [![Build Status](https://powerci.osuosl.org/job/pytorch-master-nightly-py3-linux-ppc64le/badge/icon)](https://powerci.osuosl.org/job/pytorch-master-nightly-py3-linux-ppc64le/) |\n| Linux (ppc64le) GPU | <center>\u2014</center> | <center>\u2014</center> | [![Build Status](https://powerci.osuosl.org/job/pytorch-linux-cuda92-cudnn7-py3-mpi-build-test-gpu/badge/icon)](https://powerci.osuosl.org/job/pytorch-linux-cuda92-cudnn7-py3-mpi-build-test-gpu/) |\n\nSee also the [ci.pytorch.org HUD](https://ezyang.github.io/pytorch-ci-hud/build/pytorch-master).\n\n\n## More About PyTorch\n\nAt a granular level, PyTorch is a library that consists of the following components:\n\n| Component | Description |\n| ---- | --- |\n| [**torch**](https://pytorch.org/docs/stable/torch.html) | a Tensor library like NumPy, with strong GPU support |\n| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | a neural networks library deeply integrated with autograd designed for maximum flexibility |\n| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |\n\nUsually PyTorch is used either as:\n\n- a replacement for NumPy to use the power of GPUs.\n- a deep learning research platform that provides maximum flexibility and speed.\n\nElaborating further:\n\n### A GPU-Ready Tensor Library\n\nIf you use NumPy, then you have used Tensors (a.k.a. ndarray).\n\n![Tensor illustration](./docs/source/_static/img/tensor_illustration.png)\n\nPyTorch provides Tensors that can live either on the CPU or the GPU, and accelerates the\ncomputation by a huge amount.\n\nWe provide a wide variety of tensor routines to accelerate and fit your scientific computation needs\nsuch as slicing, indexing, math operations, linear algebra, reductions.\nAnd they are fast!\n\n### Dynamic Neural Networks: Tape-Based Autograd\n\nPyTorch has a unique way of building neural networks: using and replaying a tape recorder.\n\nMost frameworks such as TensorFlow, Theano, Caffe and CNTK have a static view of the world.\nOne has to build a neural network, and reuse the same structure again and again.\nChanging the way the network behaves means that one has to start from scratch.\n\nWith PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to\nchange the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes\nfrom several research papers on this topic, as well as current and past work such as\n[torch-autograd](https://github.com/twitter/torch-autograd),\n[autograd](https://github.com/HIPS/autograd),\n[Chainer](https://chainer.org), etc.\n\nWhile this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.\nYou get the best of speed and flexibility for your crazy research.\n\n![Dynamic graph](https://github.com/pytorch/pytorch/blob/master/docs/source/_static/img/dynamic_graph.gif)\n\n### Python First\n\nPyTorch is not a Python binding into a monolithic C++ framework.\nIt is built to be deeply integrated into Python.\nYou can use it naturally like you would use [NumPy](https://www.numpy.org/) / [SciPy](https://www.scipy.org/) / [scikit-learn](https://scikit-learn.org) etc.\nYou can write your new neural network layers in Python itself, using your favorite libraries\nand use packages such as Cython and Numba.\nOur goal is to not reinvent the wheel where appropriate.\n\n### Imperative Experiences\n\nPyTorch is designed to be intuitive, linear in thought and easy to use.\nWhen you execute a line of code, it gets executed. There isn't an asynchronous view of the world.\nWhen you drop into a debugger, or receive error messages and stack traces, understanding them is straightforward.\nThe stack trace points to exactly where your code was defined.\nWe hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.\n\n### Fast and Lean\n\nPyTorch has minimal framework overhead. We integrate acceleration libraries\nsuch as [Intel MKL](https://software.intel.com/mkl) and NVIDIA (cuDNN, NCCL) to maximize speed.\nAt the core, its CPU and GPU Tensor and neural network backends\n(TH, THC, THNN, THCUNN) are mature and have been tested for years.\n\nHence, PyTorch is quite fast \u2013 whether you run small or large neural networks.\n\nThe memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.\nWe've written custom memory allocators for the GPU to make sure that\nyour deep learning models are maximally memory efficient.\nThis enables you to train bigger deep learning models than before.\n\n### Extensions Without Pain\n\nWriting new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward\nand with minimal abstractions.\n\nYou can write new neural network layers in Python using the torch API\n[or your favorite NumPy-based libraries such as SciPy](https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html).\n\nIf you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.\nNo wrapper code needs to be written. You can see [a tutorial here](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [an example here](https://github.com/pytorch/extension-cpp).\n\n\n## Installation\n\n### Binaries\nCommands to install from binaries via Conda or pip wheels are on our website:\n[https://pytorch.org](https://pytorch.org)\n\n\n#### NVIDIA Jetson platforms\n\nPython wheels for NVIDIA's Jetson Nano, Jetson TX2, and Jetson AGX Xavier are available via the following URLs:\n\n- Stable binaries:\n  - Python 3.6: https://nvidia.box.com/v/torch-stable-cp36-jetson-jp42\n- Rolling weekly binaries:\n  - Python 3.6: https://nvidia.box.com/v/torch-weekly-cp36-jetson-jp42\n\nThey require JetPack 4.2 and above, and @dusty-nv maintains them\n\n\n### From Source\n\nIf you are installing from source, you will need a C++14 compiler. Also, we highly recommend installing an [Anaconda](https://www.anaconda.com/distribution/#download-section) environment.\nYou will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.\n\nOnce you have [Anaconda](https://www.anaconda.com/distribution/#download-section) installed, here are the instructions.\n\nIf you want to compile with CUDA support, install\n- [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads) 9 or above\n- [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) v7 or above\n\nIf you want to disable CUDA support, export environment variable `USE_CUDA=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\nIf you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to [are available here](https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/)\n\n\n#### Install Dependencies\n\nCommon\n```\nconda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi\n```\n\nOn Linux\n```bash\n# Add LAPACK support for the GPU if needed\nconda install -c pytorch magma-cuda90 # or [magma-cuda92 | magma-cuda100 | magma-cuda101 ] depending on your cuda version\n```\n\n#### Get the PyTorch Source\n```bash\ngit clone --recursive https://github.com/pytorch/pytorch\ncd pytorch\n# if you are updating an existing checkout\ngit submodule sync\ngit submodule update --init --recursive\n```\n\n#### Install PyTorch\nOn Linux\n```bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\npython setup.py install\n```\n\nOn macOS\n```bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\nMACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install\n```\n\nEach CUDA version only supports one particular XCode version. The following combinations have been reported to work with PyTorch.\n\n| CUDA version | XCode version |\n| ------------ | ------------- |\n| 10.0         | XCode 9.4     |\n| 10.1         | XCode 10.1    |\n\n\nOn Windows\n\nAt least Visual Studio 2017 Update 3 (version 15.3.3 with the toolset 14.11) and [NVTX](https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm) are needed.\n\nIf the version of Visual Studio 2017 is higher than 15.4.5, installing of \"VC++ 2017 version 15.4 v14.11 toolset\" is strongly recommended.\n<br/> If the version of Visual Studio 2017 is lesser than 15.3.3, please update Visual Studio 2017 to the latest version along with installing \"VC++ 2017 version 15.4 v14.11 toolset\".\n<br/> There is no guarantee of the correct building with VC++ 2017 toolsets, others than version 15.4 v14.11.\n<br/> \"VC++ 2017 version 15.4 v14.11 toolset\" might be installed onto already installed Visual Studio 2017 by running its installation once again and checking the corresponding checkbox under \"Individual components\"/\"Compilers, build tools, and runtimes\".\n\nNVTX is a part of CUDA distributive, where it is called \"Nsight Compute\". To install it onto already installed CUDA run CUDA installation once again and check the corresponding checkbox.\nBe sure that CUDA with Nsight Compute is installed after Visual Studio 2017.\n\nCurrently VS 2017, VS 2019 and Ninja are supported as the generator of CMake. If `ninja.exe` is detected in `PATH`, then Ninja will be used as the default generator, otherwise it will use VS 2017.\n<br/> If Ninja is selected as the generator, the latest MSVC which is newer than VS 2015 (14.0) will get selected as the underlying toolchain if you have Python > 3.5, otherwise VS 2015 will be selected so you'll have to activate the environment. If you use CMake <= 3.14.2 and has VS 2019 installed, then even if you specify VS 2017 as the generator, VS 2019 will get selected as the generator.\n\nCUDA and MSVC have strong version dependencies, so even if you use VS 2017 / 2019, you will get build errors like `nvcc fatal : Host compiler targets unsupported OS`. For this kind of problem, please install the corresponding VS toolchain in the table below and then you can either specify the toolset during activation (recommended) or set `CUDAHOSTCXX` to override the cuda host compiler (not recommended if there are big version differences).\n\n| CUDA version | Newest supported VS version                             |\n| ------------ | ------------------------------------------------------- |\n| 9.0 / 9.1    | Visual Studio 2017 Update 4 (15.4) (`_MSC_VER` <= 1911) |\n| 9.2          | Visual Studio 2017 Update 5 (15.5) (`_MSC_VER` <= 1912) |\n| 10.0         | Visual Studio 2017 (15.X) (`_MSC_VER` < 1920)           |\n| 10.1         | Visual Studio 2019 (16.X) (`_MSC_VER` < 1930)           |\n\n```cmd\ncmd\n\n:: [Optional] If you want to build with VS 2019 generator, please change the value in the next line to `Visual Studio 16 2019`.\n:: Note: This value is useless if Ninja is detected. However, you can force that by using `set USE_NINJA=OFF`.\nset CMAKE_GENERATOR=Visual Studio 15 2017\n\n:: Read the content in the previous section carefully before you proceed.\n:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.\n:: \"Visual Studio 2017 Developer Command Prompt\" will be run automatically.\n:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.\nset CMAKE_GENERATOR_TOOLSET_VERSION=14.11\nset DISTUTILS_USE_SDK=1\nfor /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\" -version [15^,16^) -products * -latest -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\n\n:: [Optional] If you want to override the cuda host compiler\nset CUDAHOSTCXX=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64\\cl.exe\n\npython setup.py install\n\n```\n\n##### Adjust Build Options (Optional)\n\nYou can adjust the configuration of cmake variables optionally (without building first), by doing\nthe following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done\nwith such a step.\n\nOn Linux\n```bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\npython setup.py build --cmake-only\nccmake build  # or cmake-gui build\n```\n\nOn macOS\n```bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\nMACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only\nccmake build  # or cmake-gui build\n```\n\n### Docker Image\n\n#### Using pre-built images\n\nYou can also pull a pre-built docker image from Docker Hub and run with docker v19.03+\n\n```bash\ndocker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest\n```\n\nPlease note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.\nfor multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you\nshould increase shared memory size either with `--ipc=host` or `--shm-size` command line options to `nvidia-docker run`.\n\n#### Building the image yourself\n\n**NOTE:** Must be built with a docker version > 18.06\n\nThe `Dockerfile` is supplied to build images with cuda support and cudnn v7.\nYou can pass `PYTHON_VERSION=x.y` make variable to specify which Python version is to be used by Miniconda, or leave it\nunset to use the default.\n```bash\nmake -f docker.Makefile\n# images are tagged as docker.io/${your_docker_username}/pytorch\n```\n\n### Building the Documentation\n\nTo build documentation in various formats, you will need [Sphinx](http://www.sphinx-doc.org) and the\nreadthedocs theme.\n\n```\ncd docs/\npip install -r requirements.txt\n```\nYou can then build the documentation by running ``make <format>`` from the\n``docs/`` folder. Run ``make`` to get a list of all available output formats.\n\n### Previous Versions\n\nInstallation instructions and binaries for previous PyTorch versions may be found\non [our website](https://pytorch.org/previous-versions).\n\n\n## Getting Started\n\nThree pointers to get you started:\n- [Tutorials: get you started with understanding and using PyTorch](https://pytorch.org/tutorials/)\n- [Examples: easy to understand pytorch code across all domains](https://github.com/pytorch/examples)\n- [The API Reference](https://pytorch.org/docs/)\n\n## Communication\n* forums: discuss implementations, research, etc. https://discuss.pytorch.org\n* GitHub issues: bug reports, feature requests, install issues, RFCs, thoughts, etc.\n* Slack: The [PyTorch Slack](https://pytorch.slack.com/) hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration etc. If you are a beginner looking for help, the primary medium is [PyTorch Forums](https://discuss.pytorch.org). If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1\n* newsletter: no-noise, one-way email newsletter with important announcements about pytorch. You can sign-up here: https://eepurl.com/cbG0rv\n* for brand guidelines, please visit our website at [pytorch.org](https://pytorch.org/)\n\n## Releases and Contributing\n\nPyTorch has a 90 day release cycle (major releases). Please let us know if you encounter a bug by [filing an issue](https://github.com/pytorch/pytorch/issues).\n\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.\n\nIf you plan to contribute new features, utility functions or extensions to the core, please first open an issue and discuss the feature with us.\nSending a PR without discussion might end up resulting in a rejected PR, because we might be taking the core in a different direction than you might be aware of.\n\n## The Team\n\nPyTorch is a community driven project with several skillful engineers and researchers contributing to it.\n\nPyTorch is currently maintained by [Adam Paszke](https://apaszke.github.io/), [Sam Gross](https://github.com/colesbury), [Soumith Chintala](http://soumith.ch) and [Gregory Chanan](https://github.com/gchanan) with major contributions coming from hundreds of talented individuals in various forms and means.\nA non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Koepf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito.\n\nNote: this project is unrelated to [hughperkins/pytorch](https://github.com/hughperkins/pytorch) with the same name. Hugh is a valuable contributor in the Torch community and has helped with many things Torch and PyTorch.\n\n## License\n\nPyTorch is BSD-style licensed, as found in the LICENSE file.\n"}, "torch-geometric": {"file_name": "rusty1s/pytorch_geometric/README.md", "raw_text": "[pypi-image]: https://badge.fury.io/py/torch-geometric.svg\n[pypi-url]: https://pypi.python.org/pypi/torch-geometric\n[build-image]: https://travis-ci.org/rusty1s/pytorch_geometric.svg?branch=master\n[build-url]: https://travis-ci.org/rusty1s/pytorch_geometric\n[docs-image]: https://readthedocs.org/projects/pytorch-geometric/badge/?version=latest\n[docs-url]: https://pytorch-geometric.readthedocs.io/en/latest/?badge=latest\n[coverage-image]: https://codecov.io/gh/rusty1s/pytorch_geometric/branch/master/graph/badge.svg\n[coverage-url]: https://codecov.io/github/rusty1s/pytorch_geometric?branch=master\n[contributing-image]: https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat\n[contributing-url]: https://github.com/rusty1s/pytorch_geometric/blob/master/CONTRIBUTING.md\n\n<p align=\"center\">\n  <img width=\"40%\" src=\"https://raw.githubusercontent.com/rusty1s/pytorch_geometric/master/docs/source/_static/img/pyg_logo_text.svg?sanitize=true\" />\n</p>\n\n--------------------------------------------------------------------------------\n\n[![PyPI Version][pypi-image]][pypi-url]\n[![Build Status][build-image]][build-url]\n[![Docs Status][docs-image]][docs-url]\n[![Code Coverage][coverage-image]][coverage-url]\n[![Contributing][contributing-image]][contributing-url]\n\n**[Documentation](https://pytorch-geometric.readthedocs.io)** | **[Paper](https://arxiv.org/abs/1903.02428)** | **[External Resources](https://pytorch-geometric.readthedocs.io/en/latest/notes/resources.html)**\n\n*PyTorch Geometric* (PyG) is a geometric deep learning extension library for [PyTorch](https://pytorch.org/).\n\nIt consists of various methods for deep learning on graphs and other irregular structures, also known as *[geometric deep learning](http://geometricdeeplearning.com/)*, from a variety of published papers.\nIn addition, it consists of an easy-to-use mini-batch loader for many small and single giant graphs, multi gpu-support, a large number of common benchmark datasets (based on simple interfaces to create your own), and helpful transforms, both for learning on arbitrary graphs as well as on 3D meshes or point clouds.\n\n--------------------------------------------------------------------------------\n\nPyTorch Geometric makes implementing Graph Neural Networks a breeze (see [here](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html) for the accompanying tutorial).\nFor example, this is all it takes to implement the [edge convolutional layer](https://arxiv.org/abs/1801.07829):\n\n```python\nimport torch\nfrom torch.nn import Sequential as Seq, Linear as Lin, ReLU\nfrom torch_geometric.nn import MessagePassing\n\nclass EdgeConv(MessagePassing):\n    def __init__(self, F_in, F_out):\n        super(EdgeConv, self).__init__(aggr='max')  # \"Max\" aggregation.\n        self.mlp = Seq(Lin(2 * F_in, F_out), ReLU(), Lin(F_out, F_out))\n\n    def forward(self, x, edge_index):\n        # x has shape [N, F_in]\n        # edge_index has shape [2, E]\n        return self.propagate(edge_index, x=x)  # shape [N, F_out]\n\n    def message(self, x_i, x_j):\n        # x_i has shape [E, F_in]\n        # x_j has shape [E, F_in]\n        edge_features = torch.cat([x_i, x_j - x_i], dim=1)  # shape [E, 2 * F_in]\n        return self.mlp(edge_features)  # shape [E, F_out]\n```\n\nIn detail, the following methods are currently implemented:\n\n* **[SplineConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.SplineConv)** from Fey *et al.*: [SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels](https://arxiv.org/abs/1711.08920) (CVPR 2018)\n* **[GCNConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv)** from Kipf and Welling: [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907) (ICLR 2017)\n* **[ChebConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.ChebConv)** from Defferrard *et al.*: [Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://arxiv.org/abs/1606.09375) (NIPS 2016)\n* **[NNConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.NNConv)** from Gilmer *et al.*: [Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212) (ICML 2017)\n* **[CGConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.CGConv)** from Xie and Grossman: [Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.145301) (Physical Review Letters 120, 2018)\n* **[ECConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.ECConv)** from Simonovsky and Komodakis: [Edge-Conditioned Convolution on Graphs](https://arxiv.org/abs/1704.02901) (CVPR 2017)\n* **[GATConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GATConv)** from Veli\u010dkovi\u0107 *et al.*: [Graph Attention Networks](https://arxiv.org/abs/1710.10903) (ICLR 2018)\n* **[SAGEConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.SAGEConv)** from Hamilton *et al.*: [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216) (NIPS 2017)\n* **[GraphConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GraphConv)** from, *e.g.*, Morris *et al.*: [Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks](https://arxiv.org/abs/1810.02244) (AAAI 2019)\n* **[GatedGraphConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GatedGraphConv)** from Li *et al.*: [Gated Graph Sequence Neural Networks](https://arxiv.org/abs/1511.05493) (ICLR 2016)\n* **[GINConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GINConv)** from Xu *et al.*: [How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826) (ICLR 2019)\n* **[ARMAConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.ARMAConv)** from Bianchi *et al.*: [Graph Neural Networks with Convolutional ARMA Filters](https://arxiv.org/abs/1901.01343) (CoRR 2019)\n* **[SGConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.SGConv)** from Wu *et al.*: [Simplifying Graph Convolutional Networks](https://arxiv.org/abs/1902.07153) (CoRR 2019)\n* **[APPNP](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.APPNP)** from Klicpera *et al.*: [Predict then Propagate: Graph Neural Networks meet Personalized PageRank](https://arxiv.org/abs/1810.05997) (ICLR 2019)\n* **[AGNNConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.AGNNConv)** from Thekumparampil *et al.*: [Attention-based Graph Neural Network for Semi-Supervised Learning](https://arxiv.org/abs/1803.03735) (CoRR 2017)\n* **[TAGConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.TAGConv)** from Du *et al.*: [Topology Adaptive Graph Convolutional Networks](https://arxiv.org/abs/1710.10370) (CoRR 2017)\n* **[RGCNConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.RGCNConv)** from Schlichtkrull *et al.*: [Modeling Relational Data with Graph Convolutional Networks](https://arxiv.org/abs/1703.06103) (ESWC 2018)\n* **[SignedConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.SignedConv)** from Derr *et al.*: [Signed Graph Convolutional Network](https://arxiv.org/abs/1808.06354) (ICDM 2018)\n* **[DNAConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.DNAConv)** from Fey: [Just Jump: Dynamic Neighborhood Aggregation in Graph Neural Networks](https://arxiv.org/abs/1904.04849) (ICLR-W 2019)\n* **[EdgeConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.EdgeConv)** from Wang *et al.*: [Dynamic Graph CNN for Learning on Point Clouds](https://arxiv.org/abs/1801.07829) (CoRR, 2018)\n* **[PointConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.PointConv)** (including **[Iterative Farthest Point Sampling](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.pool.fps)**, dynamic graph generation based on **[nearest neighbor](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.pool.knn_graph)** or **[maximum distance](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.pool.radius_graph)**, and **[k-NN interpolation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.unpool.knn_interpolate)** for upsampling) from Qi *et al.*: [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593) (CVPR 2017) and [PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](https://arxiv.org/abs/1706.02413) (NIPS 2017)\n* **[XConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.XConv)** from Li *et al.*: [PointCNN: Convolution On X-Transformed Points](https://arxiv.org/abs/1801.07791) [(official implementation)](https://github.com/yangyanli/PointCNN) (NeurIPS 2018)\n* **[PPFConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.PPFConv)** from Deng *et al.*: [PPFNet: Global Context Aware Local Features for Robust 3D Point Matching](https://arxiv.org/abs/1802.02669) (CVPR 2018)\n* **[GMMConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GMMConv)** from Monti *et al.*: [Geometric Deep Learning on Graphs and Manifolds using Mixture Model CNNs](https://arxiv.org/abs/1611.08402) (CVPR 2017)\n* **[FeaStConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.FeaStConv)** from Verma *et al.*: [FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis](https://arxiv.org/abs/1706.05206) (CVPR 2018)\n* **[HypergraphConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.HypergraphConv)** from Bai *et al.*: [Hypergraph Convolution and Hypergraph Attention](https://arxiv.org/abs/1901.08150) (CoRR 2019)\n* **[GravNetConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GravNetConv)** from Qasim *et al.*: [Learning Representations of Irregular Particle-detector Geometry with Distance-weighted Graph Networks](https://arxiv.org/abs/1902.07987) (European Physics Journal C, 2019)\n* A **[MetaLayer](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.meta.MetaLayer)** for building any kind of graph network similar to the [TensorFlow Graph Nets library](https://github.com/deepmind/graph_nets) from Battaglia *et al.*: [Relational Inductive Biases, Deep Learning, and Graph Networks](https://arxiv.org/abs/1806.01261) (CoRR 2018)\n* **[GlobalAttention](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.glob.GlobalAttention)** from Li *et al.*: [Gated Graph Sequence Neural Networks](https://arxiv.org/abs/1511.05493) (ICLR 2016)\n* **[Set2Set](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.glob.Set2Set)** from Vinyals *et al.*: [Order Matters: Sequence to Sequence for Sets](https://arxiv.org/abs/1511.06391) (ICLR 2016)\n* **[Sort Pool](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.glob.global_sort_pool)** from Zhang *et al.*: [An End-to-End Deep Learning Architecture for Graph Classification](https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf) (AAAI 2018)\n* **[Dense Differentiable Pooling](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.dense.diff_pool.dense_diff_pool)** from Ying *et al.*: [Hierarchical Graph Representation Learning with Differentiable Pooling](https://arxiv.org/abs/1806.08804) (NeurIPS 2018)\n* **[Dense MinCUT Pooling](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.dense.mincut_pool.dense_mincut_pool)** from Bianchi *et al.*: [MinCUT Pooling in Graph Neural Networks](https://arxiv.org/abs/1907.00481) (CoRR 2019)\n* **[Graclus Pooling](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.pool.graclus)** from Dhillon *et al.*: [Weighted Graph Cuts without Eigenvectors: A Multilevel Approach](http://www.cs.utexas.edu/users/inderjit/public_papers/multilevel_pami.pdf) (PAMI 2007)\n* **[Voxel Grid Pooling](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.pool.voxel_grid)** from, *e.g.*, Simonovsky and Komodakis: [Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs](https://arxiv.org/abs/1704.02901) (CVPR 2017)\n* **[Top-K Pooling](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.pool.TopKPooling)** from Gao and Ji: [Graph U-Nets](https://arxiv.org/abs/1905.05178) (ICML 2019), Cangea *et al.*: [Towards Sparse Hierarchical Graph Classifiers](https://arxiv.org/abs/1811.01287) (NeurIPS-W 2018) and Knyazev *et al.*: [Understanding Attention and Generalization in Graph Neural Networks](https://arxiv.org/abs/1905.02850) (ICLR-W 2019)\n* **[SAG Pooling](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.pool.SAGPooling)** from Lee *et al.*: [Self-Attention Graph Pooling](https://arxiv.org/abs/1904.08082) (ICML 2019) and Knyazev *et al.*: [Understanding Attention and Generalization in Graph Neural Networks](https://arxiv.org/abs/1905.02850) (ICLR-W 2019)\n* **[Edge Pooling](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.pool.EdgePooling)** from Diehl *et al.*: [Towards Graph Pooling by Edge Contraction](https://graphreason.github.io/papers/17.pdf) (ICML-W 2019) and Diehl: [Edge Contraction Pooling for Graph Neural Networks](https://arxiv.org/abs/1905.10990) (CoRR 2019)\n* **[Local Degree Profile](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.transforms.LocalDegreeProfile)** from Cai and Wang: [A Simple yet Effective Baseline for Non-attribute Graph Classification](https://arxiv.org/abs/1811.03508) (CoRR 2018)\n* **[Jumping Knowledge](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.models.JumpingKnowledge)** from Xu *et al.*: [Representation Learning on Graphs with Jumping Knowledge Networks](https://arxiv.org/abs/1806.03536) (ICML 2018)\n* **[Node2Vec](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.models.Node2Vec)** from Grover and Leskovec: [node2vec: Scalable Feature Learning for Networks](https://arxiv.org/abs/1607.00653) (KDD 2016)\n* **[Deep Graph Infomax](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.models.DeepGraphInfomax)** from Veli\u010dkovi\u0107 *et al.*: [Deep Graph Infomax](https://arxiv.org/abs/1809.10341) (ICLR 2019)\n* All variants of **[Graph Auto-Encoders](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.models.GAE)** from Kipf and Welling: [Variational Graph Auto-Encoders](https://arxiv.org/abs/1611.07308) (NIPS-W 2016) and Pan *et al.*: [Adversarially Regularized Graph Autoencoder for Graph Embedding](https://arxiv.org/abs/1802.04407) (IJCAI 2018)\n* **[RENet](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.models.RENet)** from Jin *et al.*: [Recurrent Event Network for Reasoning over Temporal Knowledge Graphs](https://arxiv.org/abs/1904.05530) (ICLR-W 2019)\n* **[GraphUNet](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.models.GraphUNet)** from Gao and Ji: [Graph U-Nets](https://arxiv.org/abs/1905.05178) (ICML 2019)\n* **[NeighborSampler](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.NeighborSampler)** from Hamilton *et al.*: [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216) (NIPS 2017)\n* **[ClusterGCN](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.ClusterLoader)** from Chiang *et al.*: [Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks](https://arxiv.org/abs/1905.07953) (KDD 2019)\n* **[GraphSAINT](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.GraphSAINTSampler)** from Zeng *et al.*: [GraphSAINT: Graph Sampling Based Inductive Learning Method](https://arxiv.org/abs/1907.04931) (ICLR 2020)\n* **[GDC](https://pytorch-geometric.readthedocs.io/en/latest/modules/transforms.html#torch_geometric.transforms.GDC)** from Klicpera *et al.*: [Diffusion Improves Graph Learning](https://arxiv.org/abs/1911.05485) (NeurIPS 2019)\n* **[GraphSizeNorm](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.norm.GraphSizeNorm)** from Dwivedi *et al.*: [Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982) (CoRR 2020)\n* **[GNNExplainer](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.models.GNNExplainer)** from Ying *et al.*: [GNNExplainer: Generating Explanations for Graph Neural Networks](https://arxiv.org/abs/1903.03894) (NeurIPS 2019)\n* **[DropEdge](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.dropout_adj)** from Rong *et al.*: [DropEdge: Towards Deep Graph Convolutional Networks on Node Classification](https://openreview.net/forum?id=Hkx1qkrKPr) (ICLR 2020)\n\n--------------------------------------------------------------------------------\n\nHead over to our [documentation](https://pytorch-geometric.readthedocs.io) to find out more about installation, data handling, creation of datasets and a full list of implemented methods, transforms, and datasets.\nFor a quick start, check out our [examples](https://github.com/rusty1s/pytorch_geometric/tree/master/examples) in the `examples/` directory.\n\nIf you notice anything unexpected, please open an [issue](https://github.com/rusty1s/pytorch_geometric/issues) and let us know.\nIf you are missing a specific method, feel free to open a [feature request](https://github.com/rusty1s/pytorch_geometric/issues).\nWe are motivated to constantly make PyTorch Geometric even better.\n\n## Installation\n\nWe provide pip wheels for all major OS/PyTorch/CUDA combinations, see [here](https://pytorch-geometric.com/whl).\n\n|             | `cpu` | `cu92` | `cu100` | `cu101` |\n|-------------|-------|--------|---------|---------|\n| **Linux**   | \u2705    | \u2705     | \u2705      | \u2705      |\n| **Windows** | \u2705    | \u274c     | \u274c      | \u2705      |\n| **macOS**   | \u2705    |        |         |         |\n\nTo install the binaries, first ensure that PyTorch 1.4.0 is installed, *e.g.*:\n\n```\n$ python -c \"import torch; print(torch.__version__)\"\n>>> 1.4.0\n```\n\nThen run\n\n```sh\n$ pip install torch-scatter==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n$ pip install torch-sparse==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n$ pip install torch-cluster==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n$ pip install torch-spline-conv==latest+${CUDA} -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n$ pip install torch-geometric\n```\n\nwhere `${CUDA}` should be replaced by either `cpu`, `cu92`, `cu100` or `cu101` depending on your PyTorch installation.\n\n## Running examples\n\n```\n$ cd examples\n$ python gcn.py\n```\n\n## Cite\n\nPlease cite [our paper](https://arxiv.org/abs/1903.02428) (and the respective papers of the methods used) if you use this code in your own work:\n\n```\n@inproceedings{Fey/Lenssen/2019,\n  title={Fast Graph Representation Learning with {PyTorch Geometric}},\n  author={Fey, Matthias and Lenssen, Jan E.},\n  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},\n  year={2019},\n}\n```\n\nFeel free to [email us](mailto:matthias.fey@tu-dortmund.de) if you wish your work to be listed in the [external resources](https://pytorch-geometric.readthedocs.io/en/latest/notes/resources.html).\n\n## Running tests\n\n```\n$ python setup.py test\n```\n"}, "torchelastic": {"file_name": "pytorch/elastic/README.md", "raw_text": "[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](LICENSE)[![CircleCI](https://circleci.com/gh/pytorch/elastic.svg?style=svg&circle-token=9bea46e94adbe2f3e0fb2d4054b1b655f2e208c2)](https://circleci.com/gh/pytorch/elastic)\n\n# TorchElastic\n\nTorchElastic allows you to launch distributed PyTorch jobs in a\nfault-tolerant and elastic manner.\nFor the latest documentation, please refer to our\n[website](https://pytorch.org/elastic).\n\n\n## Requirements\ntorchelastic requires\n* python3 (3.6+)\n* torch\n* etcd\n\n## Installation\n```bash\npip install torchelastic\n```\n\n## Quickstart\n\n**Fault-tolerant** on `4` nodes, `8` trainers/node, total `4 * 8 = 32` trainers.\nRun the following on all nodes.\n```bash\npython -m torchelastic.distributed.launch\n            --nnodes=4\n            --nproc_per_node=8\n            --rdzv_id=JOB_ID\n            --rdzv_backend=etcd\n            --rdzv_endpoint=ETCD_HOST:ETCD_PORT\n            YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n```\n\n**Elastic on** `1 ~ 4` nodes, `8` trainers/node, total `8 ~ 32` trainers. Job\nstarts as soon as `1` node is healthy, you may add up to `4` nodes.\n```bash\npython -m torchelastic.distributed.launch\n            --nnodes=1:4\n            --nproc_per_node=8\n            --rdzv_id=JOB_ID\n            --rdzv_backend=etcd\n            --rdzv_endpoint=ETCD_HOST:ETCD_PORT\n            YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n\n```\n## Contributing\n\nWe welcome PRs. See the [CONTRIBUTING](CONTRIBUTING.md) file.\n\n## License\ntorchelastic is BSD licensed, as found in the [LICENSE](LICENSE) file.\n"}, "torchserve": {"file_name": "pytorch/serve/README.md", "raw_text": "# TorchServe\n\nTorchServe is a flexible and easy to use tool for serving PyTorch models.\n\n**For full documentation, see [Model Server for PyTorch Documentation](docs/README.md).**\n\n## Contents of this Document\n\n* [Install TorchServe](#install-torchserve)\n* [Serve a Model](#serve-a-model)\n* [Quick start with docker](#quick-start-with-docker)\n* [Contributing](#contributing)\n\n## Install TorchServe\n\nConda instructions are provided in more detail, but you may also use `pip` and `virtualenv` if that is your preference.\n**Note:** Java 11 is required. Instructions for installing Java 11 for Ubuntu or macOS are provided in the [Install with Conda](#install-with-conda) section.\n\n### Install with pip\nTo use `pip` to install TorchServe and the model archiver:\n\n```\npip install torch torchtext torchvision sentencepiece\npip install torchserve torch-model-archiver\n```\n\n### Install with Conda\n_Ubuntu_\n\n1. Install Java 11\n    ```bash\n    sudo apt-get install openjdk-11-jdk\n    ```\n1. Install Conda (https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html)\n1. Create an environment and install torchserve and torch-model-archiver\n    For CPU\n    ```bash\n    conda create --name torchserve torchserve torch-model-archiver pytorch torchtext torchvision -c pytorch -c powerai\n    ```\n    For GPU\n    ```bash\n    conda create --name torchserve torchserve torch-model-archiver pytorch torchtext torchvision cudatoolkit=10.1 -c pytorch -c powerai\n    ```\n1. Activate the environment\n    ```bash\n    source activate torchserve\n    ```\n\n_macOS_\n\n1. Install Java 11\n    ```bash\n    brew tap AdoptOpenJDK/openjdk\n    brew cask install adoptopenjdk11\n    ```\n1. Install Conda (https://docs.conda.io/projects/conda/en/latest/user-guide/install/macos.html)\n1. Create an environment and install torchserve and torch-model-archiver\n    ```bash\n    conda create --name torchserve torchserve torch-model-archiver pytorch torchtext torchvision -c pytorch -c powerai\n    ```\n1. Activate the environment\n    ```bash\n    source activate torchserve\n    ```\n\nNow you are ready to [package and serve models with TorchServe](#serve-a-model).\n\n### Install TorchServe for development\n\nIf you plan to develop with TorchServe and change some of the source code, you must install it from source code.\nFirst, clone the repo with:\n\n```bash\ngit clone https://github.com/pytorch/serve\ncd serve\n```\n\nThen make your changes executable with this command:\n\n```bash\npip install -e .\n```\n\n* To develop with torch-model-archiver:\n\n```bash\ncd serve/model-archiver\npip install -e .\n```\n\n* To upgrade TorchServe or model archiver from source code and make changes executable, run:\n\n```bash\npip install -U -e .\n```\n\nFor information about the model archiver, see [detailed documentation](model-archiver/README.md).\n\n## Serve a model\n\nThis section shows a simple example of serving a model with TorchServe. To complete this example, you must have already [installed TorchServe and the model archiver](#install-with-pip).\n\nTo run this example, clone the TorchServe repository and navigate to the root of the repository:\n\n```bash\ngit clone https://github.com/pytorch/serve.git\ncd serve\n```\n\nThen run the following steps from the root of the repository.\n\n### Store a Model\n\nTo serve a model with TorchServe, first archive the model as a MAR file. You can use the model archiver to package a model.\nYou can also create model stores to store your archived models.\n\n1. Create a directory to store your models.\n\n    ```bash\n    mkdir ~/model_store\n    cd ~/model_store\n    ```\n\n1. Download a trained model.\n\n    ```bash\n    wget https://download.pytorch.org/models/densenet161-8d451a50.pth\n    ```\n\n1. Archive the model by using the model archiver. The `extra-files` param uses fa file from the `TorchServe` repo, so update the path if necessary.\n\n    ```bash\n    torch-model-archiver --model-name densenet161 --version 1.0 --model-file ~/serve/examples/image_classifier/densenet_161/model.py --serialized-file ~/model_store/densenet161-8d451a50.pth --extra-files ~/serve/examples/image_classifier/index_to_name.json --handler image_classifier\n    ```\n\nFor more information about the model archiver, see [Torch Model archiver for TorchServe](../model-archiver/README.md)\n\n### Start TorchServe to serve the model\n\nAfter you archive and store the model, use the `torchserve` command to serve the model.\n\n```bash\ntorchserve --start --model-store model_store --models ~/model_store/densenet161=densenet161.mar\n```\n\nAfter you execute the `torchserve` command above, TorchServe runs on your host, listening for inference requests.\n\n**Note**: If you specify model(s) when you run TorchServe, it automatically scales backend workers to the number equal to available vCPUs (if you run on a CPU instance) or to the number of available GPUs (if you run on a GPU instance). In case of powerful hosts with a lot of compute resoures (vCPUs or GPUs). This start up and autoscaling process might take considerable time. If you want to minimize TorchServe start up time you avoid registering and scaling the model during start up time and move that to a later point by using corresponding [Management API](docs/management_api.md#register-a-model), which allows finer grain control of the resources that are allocated for any particular model).\n\n### Get predictions from a model\n\nTo test the model server, send a request to the server's `predictions` API.\n\nComplete the following steps:\n\n* Open a new terminal window (other than the one running TorchServe).\n* Use `curl` to download one of these [cute pictures of a kitten](https://www.google.com/search?q=cute+kitten&tbm=isch&hl=en&cr=&safe=images)\n  and use the  `-o` flag to name it `kitten.jpg` for you.\n* Use `curl` to send `POST` to the TorchServe `predict` endpoint with the kitten's image.\n\n![kitten](docs/images/kitten_small.jpg)\n\nThe following code completes all three steps:\n\n```bash\ncurl -O https://s3.amazonaws.com/model-server/inputs/kitten.jpg\ncurl -X POST http://127.0.0.1:8080/predictions/densenet161 -T kitten.jpg\n```\n\nThe predict endpoint returns a prediction response in JSON. It will look something like the following result:\n\n```json\n[\n  {\n    \"tiger_cat\": 0.46933549642562866\n  },\n  {\n    \"tabby\": 0.4633878469467163\n  },\n  {\n    \"Egyptian_cat\": 0.06456148624420166\n  },\n  {\n    \"lynx\": 0.0012828214094042778\n  },\n  {\n    \"plastic_bag\": 0.00023323034110944718\n  }\n]\n```\n\nYou will see this result in the response to your `curl` call to the predict endpoint, and in the server logs in the terminal window running TorchServe. It's also being [logged locally with metrics](docs/metrics.md).\n\nNow you've seen how easy it can be to serve a deep learning model with TorchServe! [Would you like to know more?](docs/server.md)\n\n### Stop the running TorchServe\n\nTo stop the currently running TorchServe instance, run the following command:\n\n```bash\ntorchserve --stop\n```\n\nYou see output specifying that TorchServe has stopped.\n\n## Quick Start with Docker\n\n### Prerequisites\n\n* docker - Refer to the [official docker installation guide](https://docs.docker.com/install/)\n* git    - Refer to the [official git set-up guide](https://help.github.com/en/github/getting-started-with-github/set-up-git)\n* TorchServe source code. Clone and enter the repo as follows:\n\n```bash\ngit clone https://github.com/pytorch/serve.git\ncd serve\n```\n\n### Build the TorchServe Docker image\n\nThe following are examples on how to use the `build_image.sh` script to build Docker images to support CPU or GPU inference.\n\nTo build the TorchServe image for a CPU device using the `master` branch, use the following command:\n\n```bash\n./build_image.sh\n```\n\nTo create a Docker image for a specific branch, use the following command:\n\n```bash\n./build_image.sh -b <branch_name>\n```\n\nTo create a Docker image for a GPU device, use the following command:\n\n```bash\n./build_image.sh --gpu\n```\n\nTo create a Docker image for a GPU device with a specific branch, use following command:\n\n```bash\n./build_image.sh -b <branch_name> --gpu\n```\n\nTo run your TorchServe Docker image and start TorchServe inside the container with a pre-registered `resnet-18` image classification model, use the following command:\n\n```bash\n./start.sh\n```\n\n## Learn More\n\n* [Full documentation on TorchServe](docs/README.md)\n* [Manage models API](docs/management_api.md)\n* [Inference API](docs/inference_api.md)\n* [Package models for use with TorchServe](model-archiver/README.md)\n\n## Contributing\n\nWe welcome all contributions!\n\nTo learn more about how to contribute, see the contributor guide [here](https://github.com/pytorch/serve/blob/master/CONTRIBUTING.md). \n\nTo file a bug or request a feature, please file a GitHub issue. For filing pull requests, please use the template [here](https://github.com/pytorch/serve/blob/master/pull_request_template.md). Cheers!\n"}, "vega": {"file_name": "vega/ipyvega/README.md", "raw_text": "# IPython Vega\n[![PyPI](https://img.shields.io/pypi/v/vega.svg)](https://pypi.python.org/pypi/vega)\n[![Build Status](https://travis-ci.org/vega/ipyvega.svg?branch=master)](https://travis-ci.org/vega/ipyvega)\n\nIPython/Jupyter notebook module for [Vega 5](https://github.com/vega/vega), and [Vega-Lite 4](https://github.com/vega/vega-lite). Notebooks with embedded visualizations can be viewed on [GitHub](https://github.com/vega/ipyvega/blob/master/notebooks/VegaLite.ipynb) and [nbviewer](https://nbviewer.jupyter.org/github/vega/ipyvega/blob/master/notebooks/VegaLite.ipynb). If you use JupyterLab (not the notebook), you don't need to install this extension since JupyterLab comes with built-in support for Vega and Vega-Lite.\n\nAvailable on [pypi](https://pypi.python.org/pypi/vega) and [Conda Forge](https://anaconda.org/conda-forge/vega) as `vega`.\n\n<img src=\"screenshot.png\" width=\"500\">\n\n## Install and run\n\n### Python Package Index\n\nTo install `vega` and its dependencies from the Python Package Index using\n`pip`, use the following commands:\n\n```sh\npip install jupyter pandas vega\npip install --upgrade notebook  # need jupyter_client >= 4.2 for sys-prefix below\njupyter nbextension install --sys-prefix --py vega  # not needed in notebook >= 5.3\n```\n\n### Conda Forge\n\nIf you use Conda, you probably already have the latest versions of the notebook and pandas installed. To install `vega` extension run:\n\n```sh\nconda install vega\n```\n\n## Usage\n\nOnce the package is installed, run\n```sh\njupyter notebook\n```\nto launch the Jupyter notebook server, and use `vega` within the notebook.\nSee the example notebooks for [Vega-Lite](https://github.com/vega/ipyvega/blob/master/notebooks/VegaLite.ipynb) and [Vega](https://github.com/vega/ipyvega/blob/master/notebooks/Vega.ipynb).\n\nTo run the notebooks yourself, you need to get the file [`cars.json`](https://raw.githubusercontent.com/vega/ipyvega/master/notebooks/cars.json).\n\n\n## Developers\n\nThis project uses [Poetry](https://python-poetry.org/). If you prefer a local virtual environment, run `poetry config virtualenvs.in-project true` first. Install requirements: `poetry install`.\n\nThen activate the virtual environment with `poetry shell`.\n\nSymlink files instead of copying files:\n\n```sh\njupyter nbextension install --py --symlink vega\n```\n\nRun kernel with `jupyter notebook`. Run the tests with `pytest vega` or `poetry run test`.\n\nTo rebuild the JavaScript continuously, run `yarn watch`.\n\n### How to make a release\n\n* Update the JavaScript dependendencies by changing `package.json` (e.g. with [ncu](https://www.npmjs.com/package/npm-check-updates)).\n* Run `yarn`.\n* Rebuild the JavaScript with `yarn build`.\n* Make sure that everything still works (launch notebook and try the examples).\n* Update the version number in `pyproject.toml` (with `poetry version [VERSION]`), `package.json`, and `__init__.py` and add a git tag.\n* `git push`.\n* Then run `poetry publish --build` to update https://pypi.python.org/pypi/vega.\n"}, "vincent": {"file_name": "wrobstory/vincent/README.md", "raw_text": "# Status\n\n#### 2016-06-18 Update\n\nIf you are interested in this library, I would direct you to the Altair project: https://github.com/altair-viz/altair It supports the latest version of vega, is fully-featured, has a great development team, and has been developed with the support of the Vega team at UW. \n\nThere will be no more updates, closed issues, or PR merges for the Vincent project. Thanks so much to everyone who tried it or used it along the way. \n\n#Vincent\n\n[![Travs-CI status](https://travis-ci.org/wrobstory/vincent.png)](https://travis-ci.org/wrobstory/vincent)\n\n![Vincent](http://farm9.staticflickr.com/8521/8644902478_0d1513db92_o.jpg)\n\n###A Python to Vega translator\n\nThe folks at Trifacta are making it easy to build visualizations on top of D3 with Vega. Vincent makes it easy to build Vega with Python.\n\nConcept\n-------\nThe data capabilities of Python. The visualization capabilities of JavaScript.\n\nVincent takes Python data structures and translates them into [Vega](https://github.com/trifacta/vega) visualization grammar. It allows for quick iteration of visualization designs via getters and setters on grammar elements, and outputs the final visualization to JSON.\n\nPerhaps most importantly, Vincent groks Pandas DataFrames and Series in an intuitive way.\n\nInstallation\n------------\n\n```$pip install vincent```\n\nWarning: requires Pandas, which isn't a simple pip install if you don't already have Numpy installed. If you want to go all-pip, I recommend ```$pip install numpy``` then ```$pip install pandas```. Or just use [Anaconda](http://www.continuum.io/downloads).\n\nDocs\n----\n\n[Here.](https://vincent.readthedocs.org/en/latest/)\n\nQuickstart\n---------------\n\nLet's start with some varying [data](https://vincent.readthedocs.org/en/latest/quickstart.html#data), and then show some different ways to visualize them with Vincent.\n\nStarting with a simple bar chart:\n\n```python\nimport vincent\nbar = vincent.Bar(multi_iter1['y1'])\nbar.axis_titles(x='Index', y='Value')\nbar.to_json('vega.json')\n```\n\n![bars](http://farm4.staticflickr.com/3720/9388500423_b3493bbba7_o.jpg)\n\nPlotting a number of lines:\n\n```python\nline = vincent.Line(multi_iter1, iter_idx='index')\nline.axis_titles(x='Index', y='Value')\nline.legend(title='Categories')\n```\n\n![lines](http://farm6.staticflickr.com/5543/9388500445_0bdf2a22e3_o.jpg)\n\nOr a real use case, plotting stock data:\n\n```python\nline = vincent.Line(price[['GOOG', 'AAPL']])\nline.axis_titles(x='Date', y='Price')\nline.legend(title='GOOG vs AAPL')\n```\n\n![stocks1](http://farm4.staticflickr.com/3774/9391272680_67e323de24_o.jpg)\n\nColor brewer scales are built-in. For example, plotting a scatter plot with the ```Set3``` colors:\n\n```python\nscatter = vincent.Scatter(multi_iter2, iter_idx='index')\nscatter.axis_titles(x='Index', y='Data Value')\nscatter.legend(title='Categories')\nscatter.colors(brew='Set3')\n```\n\n![scatter](http://farm6.staticflickr.com/5341/9391272876_724d5fca0d_o.jpg)\n\nArea charts:\n\n```python\narea = vincent.Area(list_data)\n```\n\n![area](http://farm3.staticflickr.com/2825/9388500487_b7c1a67771_o.jpg)\n\nStacked Area Charts from a DataFrame:\n```python\nstacked = vincent.StackedArea(df_1)\nstacked.axis_titles(x='Index', y='Value')\nstacked.legend(title='Categories')\nstacked.colors(brew='Spectral')\n```\n\n![areastack](http://farm4.staticflickr.com/3827/9388500389_88ca0f0e5f_o.jpg)\n\n```python\nstacked = vincent.StackedArea(price)\nstacked.axis_titles(x='Date', y='Price')\nstacked.legend(title='Tech Stocks')\n```\n![areastack2](http://farm8.staticflickr.com/7355/9388540267_823111c78d_o.jpg)\n\nStacked Bar Charts from a DataFrame:\n\n```python\nstack = vincent.StackedBar(df_2)\nstack.legend(title='Categories')\nstack.scales['x'].padding = 0.1\n```\n![barstack1](http://farm6.staticflickr.com/5528/9391272710_c92d21da11_o.jpg)\n\n```python\nstack = vincent.StackedBar(df_farm.T)\nstack.axis_titles(x='Total Produce', y='Farms')\nstack.legend(title='Produce Types')\nstack.colors(brew='Pastel1')\n```\n![barstack2](http://farm4.staticflickr.com/3784/9388530799_623084dbe0_o.jpg)\n\nGrouped Bars from a DataFrame:\n\n```python\ngroup = vincent.GroupedBar(df_2)\ngroup.legend(title='Categories')\ngroup.colors(brew='Spectral')\ngroup.width=750\n```\n![groupbar1](http://farm6.staticflickr.com/5507/9388500521_1ec446c0e9_o.jpg)\n\n```python\ngroup = vincent.GroupedBar(df_farm)\ngroup.axis_titles(x='Total Produce', y='Farms')\ngroup.legend(title='Produce Types')\ngroup.colors(brew='Set2')\n```\n![groupbar2](http://farm6.staticflickr.com/5518/9391272912_706055754a_o.jpg)\n\nPie charts:\n\n```python\nvis = vincent.Pie(farm_1)\nvis.legend('Farm 1 Fruit')\n```\n![pie](https://farm4.staticflickr.com/3684/11391908745_227ffba829.jpg)\n\nDonut charts:\n\n```\nvis = vincent.Pie(farm_1, inner_radius=200)\nvis.colors(brew=\"Set2\")\nvis.legend('Farm 1 Fruit')\n```\n![donut](http://farm6.staticflickr.com/5530/11391917226_598fbdf3e2.jpg)\n\nSimple maps can be built quickly (all data can be found in the [vincent_map_data](https://github.com/wrobstory/vincent_map_data) repo):\n\n```python\nworld_topo = r'world-countries.topo.json'\ngeo_data = [{'name': 'countries',\n             'url': world_topo,\n             'feature': 'world-countries'}]\n\nvis = vincent.Map(geo_data=geo_data, scale=200)\n```\n\n![simplemap](http://farm3.staticflickr.com/2852/10140081393_fa46545724_c.jpg)\n\nAlso with multiple map layers:\n\n```python\ngeo_data = [{'name': 'counties',\n             'url': county_topo,\n             'feature': 'us_counties.geo'},\n            {'name': 'states',\n             'url': state_topo,\n             'feature': 'us_states.geo'}]\n\nvis = vincent.Map(geo_data=geo_data, scale=1000, projection='albersUsa')\ndel vis.marks[1].properties.update\nvis.marks[0].properties.update.fill.value = '#084081'\nvis.marks[1].properties.enter.stroke.value = '#fff'\nvis.marks[0].properties.enter.stroke.value = '#7bccc4'\n\n```\n\n![multiplelayer](http://farm4.staticflickr.com/3797/10140037456_8256fbd32d_c.jpg)\n\nMaps can be bound with data to Pandas DataFrames for choropleth visualizations (see [here](https://vincent.readthedocs.org/en/latest/quickstart.html#data) for map data munging):\n\n```python\ngeo_data = [{'name': 'counties',\n             'url': county_topo,\n             'feature': 'us_counties.geo'}]\n\nvis = vincent.Map(data=merged, geo_data=geo_data, scale=1100, projection='albersUsa',\n          data_bind='Unemployment_rate_2011', data_key='FIPS',\n          map_key={'counties': 'properties.FIPS'})\nvis.marks[0].properties.enter.stroke_opacity = ValueRef(value=0.5)\nvis.to_json('vega.json')\n```\n\n![binding1](http://farm8.staticflickr.com/7414/10139958645_681c6dd006_c.jpg)\n\nIt can be rebound on the fly with new data and color brewer scales:\n\n```python\nvis.rebind(column='Median_Household_Income_2011', brew='YlGnBu')\n```\n\n![binding2](http://farm3.staticflickr.com/2833/10140081893_4f5fa762c5_c.jpg)\n\n\nFor more examples, including how to build these from scratch, see the [examples](https://github.com/wrobstory/vincent) directory, or the [docs](https://vincent.readthedocs.org/en/latest/index.html).\n\n\nBuilt from Scratch\n------------------\n\nTo see how the charts are being built with Vincent -> Vega grammar, see the ```charts.py``` module.\n\nBuilding the bar chart from scratch will provide a quick example of building with Vincent:\n\n```python\nimport pandas as pd\nfrom vincent import (Visualization, Scale, DataRef, Data, PropertySet,\n                     Axis, ValueRef, MarkRef, MarkProperties, Mark)\n\ndf = pd.DataFrame({'Data 1': [15, 29, 63, 28, 45, 73, 15, 62],\n                   'Data 2': [42, 27, 52, 18, 61, 19, 62, 33]})\n\n#Top level Visualization\nvis = Visualization(width=500, height=300)\nvis.padding = {'top': 10, 'left': 50, 'bottom': 50, 'right': 100}\n\n#Data. We're going to key Data 2 on Data 1\nvis.data.append(Data.from_pandas(df, columns=['Data 2'], key_on='Data 1', name='table'))\n\n#Scales\nvis.scales.append(Scale(name='x', type='ordinal', range='width',\n                        domain=DataRef(data='table', field=\"data.idx\")))\nvis.scales.append(Scale(name='y', range='height', nice=True,\n                        domain=DataRef(data='table', field=\"data.val\")))\n\n#Axes\nvis.axes.extend([Axis(type='x', scale='x'), Axis(type='y', scale='y')])\n\n#Marks\nenter_props = PropertySet(x=ValueRef(scale='x', field=\"data.idx\"),\n                                     y=ValueRef(scale='y', field=\"data.val\"),\n                                     width=ValueRef(scale='x', band=True, offset=-1),\n                                     y2=ValueRef(scale='y', value=0))\nupdate_props = PropertySet(fill=ValueRef(value='steelblue'))\nmark = Mark(type='rect', from_=MarkRef(data='table'),\n            properties=MarkProperties(enter=enter_props,\n            update=update_props))\n\nvis.marks.append(mark)\nvis.axis_titles(x='Data 1', y='Data 2')\nvis.to_json('vega.json')\n```\n![barscratch](http://farm3.staticflickr.com/2866/9341688818_c154660c3f_o.jpg)\n\nBecause the Vega elements are represented by Python classes, it can be difficult to get a good idea of what the Vega grammar looks like:\n```python\nIn [5]: vis.marks[0]\n<vincent.marks.Mark at 0x110d630d0>\n```\n\nHowever, at almost any point in the Vincent stack, you can call the ```grammar()``` method to output the Vega grammar as Python data structures:\n\n```python\n>>>vis.marks[0].grammar()\n{u'from': {u'data': u'table'},\n u'properties': {u'enter': {u'width': {u'band': True,\n    u'offset': -1,\n    u'scale': u'x'},\n   u'x': {u'field': u'data.idx', u'scale': u'x'},\n   u'y': {u'field': u'data.val', u'scale': u'y'},\n   u'y2': {u'scale': u'y', u'value': 0}},\n  u'update': {u'fill': {u'value': u'steelblue'}}},\n u'type': u'rect'}\n>>>vis.marks[0].properties.enter.x.grammar()\n{u'field': u'data.idx', u'scale': u'x'}\n```\n\nor you can simply output it to a string of JSON:\n```python\n>>>print(vis.marks[0].to_json())\n{\n  \"type\": \"rect\",\n  \"from\": {\n    \"data\": \"table\"\n  },\n  \"properties\": {\n    \"update\": {\n      \"fill\": {\n        \"value\": \"steelblue\"\n      }\n    },\n    \"enter\": {\n      \"y\": {\n        \"field\": \"data.val\",\n        \"scale\": \"y\"\n      },\n      \"width\": {\n        \"band\": true,\n        \"scale\": \"x\",\n        \"offset\": -1\n      },\n      \"y2\": {\n        \"scale\": \"y\",\n        \"value\": 0\n      },\n      \"x\": {\n        \"field\": \"data.idx\",\n        \"scale\": \"x\"\n      }\n    }\n  }\n}\n```\n\nVincent is built around classes and attributes that map 1:1 to Vega grammar, for easy getting, setting,\nand deleting of grammar elements:\n\n```python\n>>>vis.marks[0].properties.enter.grammar()\n{u'width': {u'band': True, u'offset': -1, u'scale': u'x'},\n u'x': {u'field': u'data.idx', u'scale': u'x'},\n u'y': {u'field': u'data.val', u'scale': u'y'},\n u'y2': {u'scale': u'y', u'value': 0}}\n >>> del vis.marks[0].properties.enter.width\n >>> vis.marks[0].properties.enter.y2.scale = 'y2'\n >>> vis.marks[0].properties.enter.grammar()\n{u'x': {u'field': u'data.idx', u'scale': u'x'},\n u'y': {u'field': u'data.val', u'scale': u'y'},\n u'y2': {u'scale': u'y2', u'value': 0}}\n```\n\nContributors\n------------\nHuge thanks to all who have contributed to Vincent development:\n\n * Rob Story (wrobstory)\n * Dan Miller (dnmiller)\n * Peter Lubell-Doughtie (pld)\n * Lx Yu (lxyu)\n * Damien Garaud (garaud)\n * Abraham Flaxman (aflaxman)\n * Mahdi Yusuf (myusuf3)\n * Richard Maisano (maisano)\n * Julian Berman (Julian)\n * Chris Rebert (cvrebert)\n * Wojciech Bederski (wuub)\n * Min RK (minrk)\n * Drazen Lucanin (kermit666)\n * tlukasiak\n\n\nDependencies\n------------\n\n * pandas\n * pkgtools\n\nTesting:\n\n * mock\n * nose\n\nPSA: you can use pieces of Vincent without Pandas, but its tricky. Besides, Pandas is awesome- try it!\n\n\n\n\n"}, "voila": {"file_name": "voila-dashboards/voila/README.md", "raw_text": "# ![voila](docs/source/voila-logo.svg)\n\n[![Documentation](http://readthedocs.org/projects/voila/badge/?version=latest)](https://voila.readthedocs.io/en/latest/?badge=latest)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/voila-dashboards/voila/stable?urlpath=voila%2Ftree%2Fnotebooks)\n[![Join the Gitter Chat](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/QuantStack/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nRendering of live Jupyter notebooks with interactive widgets.\n\n## Introduction\n\nVoil\u00e0 turns Jupyter notebooks into standalone web applications.\n\nUnlike the usual HTML-converted notebooks, each user connecting to the Voil\u00e0\ntornado application gets a dedicated Jupyter kernel which can execute the\ncallbacks to changes in Jupyter interactive widgets.\n\n- By default, Voil\u00e0 disallows execute requests from the front-end, preventing\n  execution of arbitrary code.\n- By default, Voil\u00e0 runs with the `strip_source` option, which strips out the\n  input cells from the rendered notebook.\n\n## Installation\n\nVoil\u00e0 can be installed with the conda package manager\n\n```\nconda install -c conda-forge voila\n```\n\nor from pypi\n\n```\npip install voila\n```\n\n### JupyterLab preview extension\n\nVoil\u00e0 provides a JupyterLab extension that displays a Voil\u00e0 preview of your Notebook in a side-pane:\n\n```\njupyter labextension install @jupyter-voila/jupyterlab-preview\n```\n\n## Usage\n\n### As a standalone tornado application\n\nTo render the `bqplot` example notebook as a standalone app, run\n`voila bqplot.ipynb`.\nTo serve a directory of jupyter notebooks, run `voila` with no argument.\n\nFor example, to render the example notebook `bqplot.ipynb` from this repository with Voil\u00e0, you can first update your current environment with the requirements of this notebook (in this case in a [conda environment](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) and render the notebook with\n\n```\nconda env update -f environment.yml\ncd notebooks/\nvoila bqplot.ipynb\n```\n\nFor more command line options (e.g., to specify an alternate port number),\nrun `voila --help`.\n\n### As a server extension to `notebook` or `jupyter_server`\n\nVoil\u00e0 can also be used as a Jupyter server extension, both with the\n[notebook](https://github.com/jupyter/notebook) server or with\n[jupyter_server](https://github.com/jupyter/jupyter_server).\n\nTo install the Jupyter server extension, run\n\n```\njupyter serverextension enable voila --sys-prefix\n```\n\nWhen running the Jupyter server, the Voil\u00e0 app is accessible from the base url\nsuffixed with `voila`.\n\n## Documentation\n\nTo get started with using Voil\u00e0, check out the full documentation:\n\nhttps://voila.readthedocs.io/\n\n## Examples\n\nThe following two examples show how a standalone Jupyter notebook can be turned into a separate app, from the command-line integration.\n\n**Rendering a notebook including interactive widgets and rich mime-type rendering**\n![Voila basics](voila-basics.gif)\n\n**Rendering a notebook making use of a custom widget library ([bqplot](https://github.com/bloomberg/bqplot))**\n\n![Voila bqplot](voila-bqplot.gif)\n\n**Showing the source code for a Voil\u00e0 notebook**\n\nThe sources of the Jupyter notebook can be displayed in a Voil\u00e0 app if option `strip_sources` is set to `False`.\n\n![Voila sources](voila-sources.gif)\n\n**Voil\u00e0 dashboards with other language kernels**\n\nVoil\u00e0 is built upon Jupyter standard formats and protocols, and is agnostic to the programming language of the notebook. In this example, we present an example of a Voil\u00e0 application powered by the C++ Jupyter kernel [xeus-cling](https://github.com/QuantStack/xeus-cling), and the [xleaflet](https://github.com/QuantStack/xleaflet) project.\n\n![Voila cling](voila-cling.gif)\n\n## The Voil\u00e0 Gallery\n\nThe [Voil\u00e0 Gallery](https://voila-gallery.org) is a collection of live dashboards and applications built with Voil\u00e0 and Jupyter widgets.\n\nMost of the examples rely on widget libraries such as ipywidgets, ipyleaflet, ipyvolume, bqplot and ipympl, and showcase how to build complex web applications entirely based on notebooks.\n\nNew examples can be added to the gallery by following the steps listed in the [voila-gallery/gallery](https://github.com/voila-gallery/gallery) repository.\n\n## Development\n\nSee [CONTRIBUTING.md](./CONTRIBUTING.md) to know how to contribute and set up a development environment.\n\n## Related projects\n\nVoil\u00e0 depends on [nbconvert](https://github.com/jupyter/nbconvert) and\n[jupyter_server](https://github.com/jupyter/jupyter_server/).\n\n## License\n\nWe use a shared copyright model that enables all contributors to maintain the\ncopyright on their contributions.\n\nThis software is licensed under the BSD-3-Clause license. See the\n[LICENSE](LICENSE) file for details.\n"}, "word2vec": {"file_name": "danielfrg/word2vec/README.md", "raw_text": "word2vec\n========\n\n[![travis-ci](https://api.travis-ci.org/danielfrg/word2vec.svg)](https://travis-ci.org/danielfrg/word2vec)\n\nPython interface to Google word2vec.\n\nTraining is done using the original C code, other functionality is pure Python with numpy.\n\n## Installation\n\n```\npip install word2vec\n```\n\nThe installation requires to compile the original C code:\n\n1. The requirements are `gcc` and `Cython` - Run `pip install Cython` prior to the installation.\n2. You can override the compilation flags if needed: `W2V_CFLAGS='-march=corei7' pip install word2vec`\n\n**Windows:** There is some support for this support based on this [win32 port](https://github.com/zhangyafeikimi/word2vec-win32). Use at your own risk.\n\n## Usage\n\nLook at this example:\n[word2vec](http://nbviewer.ipython.org/urls/raw.github.com/danielfrg/word2vec/master/examples/word2vec.ipynb)\n\nThe default functionality from word2vec is also available from the command line as:\n- word2vec\n- word2phrase\n- word2vec-distance\n- word2vec-word-analogy\n- word2vec-compute-accuracy\n\nExperimental functionality on doc2vec can be found in this other example:\n[doc2vec](http://nbviewer.ipython.org/urls/raw.github.com/danielfrg/word2vec/master/examples/doc2vec.ipynb)\n"}, "xgboost": {"file_name": "dmlc/xgboost/README.md", "raw_text": "<img src=https://raw.githubusercontent.com/dmlc/dmlc.github.io/master/img/logo-m/xgboost.png width=135/>  eXtreme Gradient Boosting\n===========\n[![Build Status](https://xgboost-ci.net/job/xgboost/job/master/badge/icon?style=plastic)](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/activity)\n[![Build Status](https://img.shields.io/travis/dmlc/xgboost.svg?label=build&logo=travis&branch=master)](https://travis-ci.org/dmlc/xgboost)\n[![Build Status](https://ci.appveyor.com/api/projects/status/5ypa8vaed6kpmli8?svg=true)](https://ci.appveyor.com/project/tqchen/xgboost)\n[![Documentation Status](https://readthedocs.org/projects/xgboost/badge/?version=latest)](https://xgboost.readthedocs.org)\n[![GitHub license](http://dmlc.github.io/img/apache2.svg)](./LICENSE)\n[![CRAN Status Badge](http://www.r-pkg.org/badges/version/xgboost)](http://cran.r-project.org/web/packages/xgboost)\n[![PyPI version](https://badge.fury.io/py/xgboost.svg)](https://pypi.python.org/pypi/xgboost/)\n[![Optuna](https://img.shields.io/badge/Optuna-integrated-blue)](https://optuna.org)\n\n[Community](https://xgboost.ai/community) |\n[Documentation](https://xgboost.readthedocs.org) |\n[Resources](demo/README.md) |\n[Contributors](CONTRIBUTORS.md) |\n[Release Notes](NEWS.md)\n\nXGBoost is an optimized distributed gradient boosting library designed to be highly ***efficient***, ***flexible*** and ***portable***.\nIt implements machine learning algorithms under the [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) framework.\nXGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.\nThe same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.\n\nLicense\n-------\n\u00a9 Contributors, 2019. Licensed under an [Apache-2](https://github.com/dmlc/xgboost/blob/master/LICENSE) license.\n\nContribute to XGBoost\n---------------------\nXGBoost has been developed and used by a group of active community members. Your help is very valuable to make the package better for everyone.\nCheckout the [Community Page](https://xgboost.ai/community).\n\nReference\n---------\n- Tianqi Chen and Carlos Guestrin. [XGBoost: A Scalable Tree Boosting System](http://arxiv.org/abs/1603.02754). In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016\n- XGBoost originates from research project at University of Washington.\n\nSponsors\n--------\nBecome a sponsor and get a logo here. See details at [Sponsoring the XGBoost Project](https://xgboost.ai/sponsors). The funds are used to defray the cost of continuous integration and testing infrastructure (https://xgboost-ci.net).\n\n## Open Source Collective sponsors\n[![Backers on Open Collective](https://opencollective.com/xgboost/backers/badge.svg)](#backers) [![Sponsors on Open Collective](https://opencollective.com/xgboost/sponsors/badge.svg)](#sponsors)\n\n### Sponsors\n[[Become a sponsor](https://opencollective.com/xgboost#sponsor)]\n\n<!--<a href=\"https://opencollective.com/xgboost/sponsor/0/website\" target=\"_blank\"><img src=\"https://opencollective.com/xgboost/sponsor/0/avatar.svg\"></a>-->\n<a href=\"https://www.nvidia.com/en-us/\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/xgboost-ai/xgboost-ai.github.io/master/images/sponsors/nvidia.jpg\" alt=\"NVIDIA\" width=\"72\" height=\"72\"></a>\n<a href=\"https://opencollective.com/xgboost/sponsor/1/website\" target=\"_blank\"><img src=\"https://opencollective.com/xgboost/sponsor/1/avatar.svg\"></a>\n<a href=\"https://opencollective.com/xgboost/sponsor/2/website\" target=\"_blank\"><img src=\"https://opencollective.com/xgboost/sponsor/2/avatar.svg\"></a>\n<a href=\"https://opencollective.com/xgboost/sponsor/3/website\" target=\"_blank\"><img src=\"https://opencollective.com/xgboost/sponsor/3/avatar.svg\"></a>\n<a href=\"https://opencollective.com/xgboost/sponsor/4/website\" target=\"_blank\"><img src=\"https://opencollective.com/xgboost/sponsor/4/avatar.svg\"></a>\n<a href=\"https://opencollective.com/xgboost/sponsor/5/website\" target=\"_blank\"><img src=\"https://opencollective.com/xgboost/sponsor/5/avatar.svg\"></a>\n<a href=\"https://opencollective.com/xgboost/sponsor/6/website\" target=\"_blank\"><img src=\"https://opencollective.com/xgboost/sponsor/6/avatar.svg\"></a>\n<a href=\"https://opencollective.com/xgboost/sponsor/7/website\" target=\"_blank\"><img src=\"https://opencollective.com/xgboost/sponsor/7/avatar.svg\"></a>\n<a href=\"https://opencollective.com/xgboost/sponsor/8/website\" target=\"_blank\"><img src=\"https://opencollective.com/xgboost/sponsor/8/avatar.svg\"></a>\n<a href=\"https://opencollective.com/xgboost/sponsor/9/website\" target=\"_blank\"><img src=\"https://opencollective.com/xgboost/sponsor/9/avatar.svg\"></a>\n\n### Backers\n[[Become a backer](https://opencollective.com/xgboost#backer)]\n\n<a href=\"https://opencollective.com/xgboost#backers\" target=\"_blank\"><img src=\"https://opencollective.com/xgboost/backers.svg?width=890\"></a>\n\n## Other sponsors\nThe sponsors in this list are donating cloud hours in lieu of cash donation.\n\n<a href=\"https://aws.amazon.com/\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/xgboost-ai/xgboost-ai.github.io/master/images/sponsors/aws.png\" alt=\"Amazon Web Services\" width=\"72\" height=\"72\"></a>\n"}, "xlrd": {"file_name": "python-excel/xlrd/README.md", "raw_text": "[![Build Status](https://travis-ci.org/python-excel/xlrd.svg?branch=master)](https://travis-ci.org/python-excel/xlrd)\n[![Coverage Status](https://coveralls.io/repos/github/python-excel/xlrd/badge.svg?branch=master)](https://coveralls.io/github/python-excel/xlrd?branch=master)\n[![Documentation Status](https://readthedocs.org/projects/xlrd/badge/?version=latest)](http://xlrd.readthedocs.io/en/latest/?badge=latest)\n[![PyPI version](https://badge.fury.io/py/xlrd.svg)](https://badge.fury.io/py/xlrd)\n\n### xlrd\n\nPLEASE NOTE: This library currently has no active maintainers. You are advised to use [OpenPyXL](https://openpyxl.readthedocs.io/en/stable/) instead. If you absolutely have to read .xls files, then\nxlrd will probably still work for you, but please do not submit issues complaining that this library\nwill not read your corrupted or non-standard file. Just because Excel or some other piece of software opens your\nfile does not mean it is a valid xls file.\n\nFor more background to this: https://groups.google.com/d/msg/python-excel/P6TjJgFVjMI/g8d0eWxTBQAJ\n\n**Purpose**: Provide a library for developers to use to extract data from Microsoft Excel (tm) spreadsheet files. It is not an end-user tool.\n\n**Author**: John Machin\n\n**Licence**: BSD-style (see licences.py)\n\n**Versions of Python supported**: 2.7, 3.4+.\n\n**Outside scope**: xlrd will safely and reliably ignore any of these if present in the file:\n\n*   Charts, Macros, Pictures, any other embedded object. WARNING: currently this includes embedded worksheets.\n*   VBA modules\n*   Formulas (results of formula calculations are extracted, of course).\n*   Comments\n*   Hyperlinks\n*   Autofilters, advanced filters, pivot tables, conditional formatting, data validation\n*   Handling password-protected (encrypted) files.\n\n**Quick start**:\n\n```python\nimport xlrd\nbook = xlrd.open_workbook(\"myfile.xls\")\nprint(\"The number of worksheets is {0}\".format(book.nsheets))\nprint(\"Worksheet name(s): {0}\".format(book.sheet_names()))\nsh = book.sheet_by_index(0)\nprint(\"{0} {1} {2}\".format(sh.name, sh.nrows, sh.ncols))\nprint(\"Cell D30 is {0}\".format(sh.cell_value(rowx=29, colx=3)))\nfor rx in range(sh.nrows):\n    print(sh.row(rx))\n```\n\n**Another quick start**: This will show the first, second and last rows of each sheet in each file:\n\n    python PYDIR/scripts/runxlrd.py 3rows *blah*.xls\n\n**Acknowledgements**:\n\n*   This package started life as a translation from C into Python of parts of a utility called \"xlreader\" developed by David Giffin. \"This product includes software developed by David Giffin <david@giffin.org>.\"\n*   OpenOffice.org has truly excellent documentation of the Microsoft Excel file formats and Compound Document file format, authored by Daniel Rentz. See http://sc.openoffice.org\n*   U+5F20 U+654F: over a decade of inspiration, support, and interesting decoding opportunities.\n*   Ksenia Marasanova: sample Macintosh and non-Latin1 files, alpha testing\n*   Backporting to Python 2.1 was partially funded by Journyx - provider of timesheet and project accounting solutions (http://journyx.com/).\n*   Provision of formatting information in version 0.6.1 was funded by Simplistix Ltd (http://www.simplistix.co.uk/)\n"}}